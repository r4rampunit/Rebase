pyproject.toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "gemini-liquidity-models"
version = "0.1.0"
description = "Liquidity Impact Calculation Model for ILM Analysis"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "Proprietary"}
dependencies = [
    "polars>=0.20.0",
    "openpyxl>=3.1.0",
    "xlsxwriter>=3.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "mypy>=1.5.0",
]

[tool.setuptools.packages.find]
where = ["src", "utils"]
include = ["gemini_liquidity_models*", "utils*"]
namespaces = false
utils/init.py
from .excel_utils import ExcelReader, ExcelWriter
from .data_utils import DataProcessor

__all__ = ["ExcelReader", "ExcelWriter", "DataProcessor"]
utils/excel_utils.py
import polars as pl
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font
from datetime import datetime
from typing import Dict, List, Tuple

class ExcelReader:
    
    @staticmethod
    def read_sheet(file_path: str, sheet_name: str, header_row: int = 2) -> Tuple[pl.DataFrame, List[str]]:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        headers = []
        date_columns = []
        date_indices = []
        
        header_row_data = list(ws[header_row])
        
        for idx, cell in enumerate(header_row_data):
            if cell.value:
                if isinstance(cell.value, datetime):
                    col_name = cell.value.strftime('%d-%b-%y')
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                elif isinstance(cell.value, str) and any(char.isdigit() for char in str(cell.value)):
                    try:
                        col_name = str(cell.value).strip()
                        date_columns.append(col_name)
                        date_indices.append(idx)
                        headers.append(col_name)
                    except:
                        col_name = str(cell.value).replace(' ', '_').replace('.', '')
                        headers.append(col_name)
                else:
                    col_name = str(cell.value).replace(' ', '_').replace('.', '')
                    headers.append(col_name)
        
        data = []
        for row in ws.iter_rows(min_row=header_row + 1, values_only=True):
            if row and row[0] is not None:
                row_dict = {}
                for col_idx in range(min(len(headers), len(row))):
                    header = headers[col_idx]
                    value = row[col_idx]
                    
                    if value is None:
                        value = 0.0 if col_idx in date_indices else ""
                    elif col_idx in date_indices:
                        if isinstance(value, (int, float)):
                            value = float(value)
                        else:
                            try:
                                value = float(str(value).replace(',', ''))
                            except:
                                value = 0.0
                    else:
                        value = str(value) if value is not None else ""
                    row_dict[header] = value
                data.append(row_dict)
        
        wb.close()
        
        if not data:
            return pl.DataFrame(), date_columns
            
        df = pl.DataFrame(data)
        
        for date_col in date_columns:
            if date_col in df.columns:
                df = df.with_columns(pl.col(date_col).cast(pl.Float64))
        
        return df, date_columns
    
    @staticmethod
    def read_assumptions(file_path: str, sheet_name: str) -> pl.DataFrame:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        data = []
        for row_idx, row in enumerate(ws.iter_rows(min_row=3, values_only=True)):
            if row[1] is not None:
                desc = str(row[1]).strip()
                us_ilm = row[2] if len(row) > 2 and row[2] else 0
                ilm = row[3] if len(row) > 3 and row[3] else 0
                
                if isinstance(us_ilm, str):
                    us_ilm = float(us_ilm.rstrip('%')) / 100.0 if '%' in us_ilm else 0.0
                else:
                    us_ilm = float(us_ilm) if us_ilm else 0.0
                
                if isinstance(ilm, str):
                    ilm = float(ilm.rstrip('%')) / 100.0 if '%' in ilm else 0.0
                else:
                    ilm = float(ilm) if ilm else 0.0
                
                data.append({
                    'Description': desc,
                    'US_ILM': us_ilm,
                    'ILM': ilm,
                    'Row_Index': row_idx
                })
        
        wb.close()
        return pl.DataFrame(data)

class ExcelWriter:
    
    @staticmethod
    def write_output(file_path: str, sheet_name: str, data_dict: Dict[str, pl.DataFrame], 
                     date_columns: List[str], bold_rows: List[str] = None):
        wb = load_workbook(file_path)
        
        if sheet_name in wb.sheetnames:
            del wb[sheet_name]
        
        ws = wb.create_sheet(sheet_name)
        
        for i, col in enumerate(date_columns):
            cell = ws.cell(row=2, column=i+3)
            cell.value = col
            cell.font = Font(bold=True)
        
        row_num = 3
        
        for section_name, section_df in data_dict.items():
            for category in section_df.columns:
                if category != 'Date':
                    ws.cell(row=row_num, column=2, value=category)
                    
                    for col_idx, date_col in enumerate(date_columns):
                        value = 0.0
                        try:
                            filtered = section_df.filter(pl.col('Date') == date_col)
                            if len(filtered) > 0:
                                value = filtered[category][0]
                        except:
                            value = 0.0
                        
                        ws.cell(row=row_num, column=col_idx+3, value=value)
                    
                    if bold_rows and category in bold_rows:
                        for c in range(2, len(date_columns)+3):
                            ws.cell(row=row_num, column=c).font = Font(bold=True)
                    
                    row_num += 1
            
            if section_name != list(data_dict.keys())[-1]:
                row_num += 1
        
        wb.save(file_path)
utils/data_utils.py
import polars as pl
from typing import List

class DataProcessor:
    
    @staticmethod
    def get_value_by_key(df: pl.DataFrame, key_column: str, key_value: str, value_column: str) -> float:
        key_value = key_value.strip()
        result = df.filter(pl.col(key_column).str.strip_chars() == key_value)
        if len(result) > 0:
            return float(result[value_column][0])
        return 0.0
    
    @staticmethod
    def get_value_by_row_index(df: pl.DataFrame, row_index: int, value_column: str) -> float:
        if 0 <= row_index < len(df):
            return float(df.row(row_index, named=True)[value_column])
        return 0.0
    
    @staticmethod
    def calculate_row_range_diff(df: pl.DataFrame, row_indices: List[int], 
                                 date_col: str, base_col: str) -> float:
        if not row_indices:
            return 0.0
        total_date = 0.0
        total_base = 0.0
        for idx in row_indices:
            if idx < len(df):
                row = df.row(idx, named=True)
                if date_col in row:
                    total_date += float(row[date_col] or 0)
                if base_col in row:
                    total_base += float(row[base_col] or 0)
        return total_date - total_base
    
    @staticmethod
    def calculate_sumifs(df: pl.DataFrame, date_col: str, base_col: str,
                        filter_column: str, filter_value: str) -> float:
        filtered = df.filter(pl.col(filter_column).str.strip_chars() == filter_value.strip())
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0
    
    @staticmethod
    def calculate_sumifs_multi(df: pl.DataFrame, date_col: str, base_col: str,
                              filter_column1: str, filter_value1: str,
                              filter_column2: str, filter_value2: str) -> float:
        filtered = df.filter(
            (pl.col(filter_column1).str.strip_chars() == filter_value1.strip()) &
            (pl.col(filter_column2).str.strip_chars() == filter_value2.strip())
        )
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0
src/gemini_liquidity_models/init.py
from .liquidity_impact_calculation import LiquidityImpactCalculation

__all__ = ["LiquidityImpactCalculation"]
src/gemini_liquidity_models/liquidity_impact_calculation/init.py
from .model import LiquidityImpactCalculation
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

__all__ = ["LiquidityImpactCalculation", "LiquidityCalculator", "CalculationConfig"]
src/gemini_liquidity_models/liquidity_impact_calculation/calculation_config.py
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class CalculationConfig:
    asset_row_mappings: Dict[str, List[int]]
    liability_row_mappings: Dict[str, List[int]]
    facility_row_mappings: Dict[str, List[int]]
    assumption_row_indices: Dict[str, int]
    
    @classmethod
    def get_default_config(cls):
        return cls(
            asset_row_mappings={
                'IWPB - Premier': list(range(5, 21)),  # Excel rows 6-21
                'IWPB - Private Banking': list(range(22, 25)),  # Excel rows 23-25
                'CIB Loans': list(range(26, 47)),  # Excel rows 27-47
                'UST - HTM': [54],  # Excel row 55
                'Level 1 - MBS - HTM': [53],  # Excel row 54
                'Level 1 Other - HTM': [55],  # Excel row 56
                'Level 2A - MBS - HTM': [57],  # Excel row 58
                'Level 2A - Other - HTM': [58],  # Excel row 59
                'UST - AFS': [63],  # Excel row 64
                'Level 1 - MBS - AFS': [62],  # Excel row 63
                'Level 1 Other - AFS': [64],  # Excel row 65
                'Level 2A- MBS - AFS': [66],  # Excel row 67
                'Level 2A- Other AFS': [67],  # Excel row 68
                'Liquid Equities': [86],  # Excel row 87
                'Illiquid Trading Assets': [90],  # Excel row 91
                'MSS - Loans': [94]  # Excel row 95
            },
            liability_row_mappings={
                'IWPB - Premier': [100],  # Excel row 101
                'PB - Personal': [108],  # Excel row 109
                'PB - Commercial - Financial': [113],  # Excel row 114
                'PB - Commercial - Non Financial': [118],  # Excel row 119
                'PB - Other': [123],  # Excel row 124
                'SME': [202],  # Excel row 203
                'Other (GPS)': [208],  # Excel row 209
                'Brokered - Committed': [213],  # Excel row 214
                'Brokered - Uncommitted': [214],  # Excel row 215
                'ISV': [215],  # Excel row 216
                'Innovation Banking': [218],  # Excel row 219
                'Other (CIB)': [221],  # Excel row 222
                'Structured CDs': [130],  # Excel row 131
                'Wholesale CDs': [139],  # Excel row 140
                'Equity': [225]  # Excel row 226
            },
            facility_row_mappings={
                'Mortgage commitments': [250],  # Excel row 251
                'Retail commitments': [253]  # Excel row 254
            },
            assumption_row_indices={
                'UST - HTM': 0,  # C3
                'Level 1 - MBS - HTM': 1,  # C4
                'Level 1 Other - HTM': 2,  # C5
                'Level 2A - MBS - HTM': 3,  # C6
                'Level 2A - Other - HTM': 4,  # C7
                'Illiquid': 5,  # C8
                'UST - AFS': 6,  # C9
                'Level 1 - MBS - AFS': 7,  # C10
                'Level 1 Other - AFS': 8,  # C11
                'Level 2A- MBS - AFS': 9,  # C12
                'Level 2A- Other AFS': 10,  # C13
                'Liquid Equities': 11,  # C14
                'IWPB - Premier': 13,  # C16
                'PB - Personal': 14,  # C17
                'PB - Commercial - Financial': 15,  # C18
                'PB - Commercial - Non Financial': 16,  # C19
                'PB - Other': 17,  # C20
                'Corp - Operational': 18,  # C21
                'Corp - Non Operational': 19,  # C22
                'NBFI - Operational': 20,  # C23
                'NBFI - Non Operational': 21,  # C24
                'Banks - Operational': 22,  # C25
                'Banks - Non Operational': 23,  # C26
                'SME': 24,  # C27
                'Other (GPS)': 25,  # C28
                'Brokered - Committed': 26,  # C29
                'Brokered - Uncommitted': 27,  # C30
                'ISV': 28,  # C31
                'Innovation Banking': 29,  # C32
                'Other (CIB)': 30,  # C33
                'Credit1': 32,  # C35
                'Liquidity1': 33,  # C36
                'Credit2': 34,  # C37
                'Liquidity2': 35,  # C38
                'Credit3': 36,  # C39
                'Liquidity3': 37,  # C40
                'Mortgage commitments': 38,  # C41
                'Retail commitments': 39  # C42
            }
        )
src/gemini_liquidity_models/liquidity_impact_calculation/calculation_engine.py
import polars as pl
from typing import Dict, List
from utils.data_utils import DataProcessor

class LiquidityCalculator:
    
    def __init__(self, config: 'CalculationConfig'):
        self.config = config
        self.processor = DataProcessor()
    
    def calculate_impacts(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                         date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        asset_impact = self._calculate_asset_impact(liquidity_df, assumptions_df, date_columns, base_date)
        liability_impact = self._calculate_liability_impact(liquidity_df, assumptions_df, date_columns, base_date)
        facility_impact = self._calculate_facility_impact(liquidity_df, assumptions_df, date_columns, base_date)
        us_ilm_summary = self._calculate_us_ilm_summary(asset_impact, liability_impact, facility_impact, date_columns)
        
        return {
            'asset': asset_impact,
            'liability': liability_impact,
            'facility': facility_impact,
            'summary': us_ilm_summary
        }
    
    def _get_assumption_by_index(self, assumptions_df: pl.DataFrame, key: str) -> float:
        row_index = self.config.assumption_row_indices.get(key, -1)
        if row_index >= 0:
            return self.processor.get_value_by_row_index(assumptions_df, row_index, 'US_ILM')
        return 0.0
    
    def _calculate_asset_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['IWPB - Premier'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['IWPB - Premier'], date_col, base_date
            )
            
            row_data['IWPB - Private Banking'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['IWPB - Private Banking'], date_col, base_date
            )
            
            row_data['CIB Loans'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['CIB Loans'], date_col, base_date
            )
            
            row_data['UST - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['UST - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'UST - HTM')
            
            row_data['Level 1 - MBS - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - MBS - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - MBS - HTM')
            
            row_data['Level 1 Other - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 Other - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 Other - HTM')
            
            row_data['Level 2A - MBS - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A - MBS - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A - MBS - HTM')
            
            row_data['Level 2A - Other - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A - Other - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A - Other - HTM')
            
            row_data['Illiquid'] = 0.0
            
            row_data['UST - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['UST - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'UST - AFS')
            
            row_data['Level 1 - MBS - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - MBS - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - MBS - AFS')
            
            row_data['Level 1 Other - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 Other - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 Other - AFS')
            
            row_data['Level 2A- MBS - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A- MBS - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquid Equities')
            
            row_data['Level 2A- Other AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A- Other AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A- Other AFS')
            
            row_data['Liquid Equities'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Liquid Equities'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquid Equities')
            
            row_data['Illiquid Trading Assets'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Illiquid Trading Assets'], date_col, base_date
            )
            
            row_data['MSS - Loans'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['MSS - Loans'], date_col, base_date
            )
            
            row_data['Liquid Trading Assets'] = 0.0
            
            asset_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Asset Impact'] = asset_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_liability_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['IWPB - Premier'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['IWPB - Premier'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'IWPB - Premier'))
            
            row_data['PB - Personal'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Personal'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Personal'))
            
            row_data['PB - Commercial - Financial'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Commercial - Financial'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Commercial - Financial'))
            
            row_data['PB - Commercial - Non Financial'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Commercial - Non Financial'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Commercial - Non Financial'))
            
            row_data['PB - Other'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Other'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Other'))
            
            row_data['Affiliate'] = 0.0
            row_data['Corp'] = 0.0
            
            row_data['Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Corp - Operational'))
            
            row_data['Non-Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Corp - Non Operational'))
            
            row_data['NBFI'] = 0.0
            
            row_data['Operational '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'NBFI - Operational'))
            
            row_data['Non-Operational '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'NBFI - Non Operational'))
            
            row_data['Banks'] = 0.0
            
            row_data['Operational  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Banks - Operational'))
            
            row_data['Non-Operational  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Banks - Non Operational'))
            
            row_data['SME'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['SME'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'SME'))
            
            row_data['Other (GPS)'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Other (GPS)'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Other (GPS)'))
            
            row_data['Brokered - Committed'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Brokered - Committed'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Brokered - Committed'))
            
            row_data['Brokered - Uncommitted'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Brokered - Uncommitted'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Brokered - Uncommitted'))
            
            row_data['ISV'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['ISV'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'ISV'))
            
            row_data['Innovation Banking'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Innovation Banking'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Innovation Banking'))
            
            row_data['Other (CIB)'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Other (CIB)'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Other (CIB)'))
            
            row_data['Structured CDs'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Structured CDs'], date_col, base_date
            )
            
            row_data['Wholesale CDs'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Wholesale CDs'], date_col, base_date
            )
            
            row_data['Equity'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Equity'], date_col, base_date
            )
            
            liability_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Liability Impact'] = liability_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_facility_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['Credit'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non-FI', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit1')
            
            row_data['Liquidity'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non-FI', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity1')
            
            row_data['Credit '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit2')
            
            row_data['Liquidity '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity2')
            
            row_data['Credit  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit3')
            
            row_data['Liquidity  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity3')
            
            row_data['Mortgage commitments'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.facility_row_mappings['Mortgage commitments'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Mortgage commitments')
            
            row_data['Retail commitments'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.facility_row_mappings['Retail commitments'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Retail commitments')
            
            facility_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Comitted Facility Impact'] = facility_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_us_ilm_summary(self, asset_df: pl.DataFrame, liability_df: pl.DataFrame,
                                  facility_df: pl.DataFrame, date_columns: List[str]) -> pl.DataFrame:
        results = []
        cumulative = 0.0
        
        for date_col in date_columns:
            asset_val = asset_df.filter(pl.col('Date') == date_col).select('Asset Impact')[0, 0]
            liability_val = liability_df.filter(pl.col('Date') == date_col).select('Liability Impact')[0, 0]
            facility_val = facility_df.filter(pl.col('Date') == date_col).select('Comitted Facility Impact')[0, 0]
            
            cumulative += asset_val + liability_val + facility_val
            results.append({'Date': date_col, 'US ILM December': cumulative})
        
        return pl.DataFrame(results)
src/gemini_liquidity_models/liquidity_impact_calculation/model.py
import polars as pl
from typing import Dict, List, Optional
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

class LiquidityImpactCalculation:
    
    def __init__(self, config: Optional[CalculationConfig] = None):
        self.config = config or CalculationConfig.get_default_config()
        self.calculator = LiquidityCalculator(self.config)
    
    def calculate(self, liquidity_data: pl.DataFrame, assumptions_data: pl.DataFrame,
                 date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        return self.calculator.calculate_impacts(
            liquidity_data, assumptions_data, date_columns, base_date
        )
    
    def get_formatted_output(self, calculation_results: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:
        return {
            'Assets': calculation_results['asset'],
            'Liabilities': calculation_results['liability'],
            'Committed Facilities': calculation_results['facility'],
            'US ILM Summary': calculation_results['summary']
        }
main.py
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from utils import ExcelReader, ExcelWriter

def main():
    loc_link = r"path/to/your/excel/file.xlsx"
    
    reader = ExcelReader()
    
    liquidity_data, date_columns = reader.read_sheet(
        file_path=loc_link,
        sheet_name="Liquidity Input",
        header_row=2
    )
    
    assumptions_data = reader.read_assumptions(
        file_path=loc_link,
        sheet_name="US ILM ILM Assumptions"
    )
    
    print(f"Loaded {len(liquidity_data)} rows from Liquidity Input")
    print(f"Found {len(date_columns)} date columns: {date_columns[:5]}...")
    print(f"Loaded {len(assumptions_data)} assumptions")
    
    model = LiquidityImpactCalculation()
    
    results = model.calculate(
        liquidity_data=liquidity_data,
        assumptions_data=assumptions_data,
        date_columns=date_columns,
        base_date='31-Dec-24'
    )
    
    formatted_results = model.get_formatted_output(results)
    
    writer = ExcelWriter()
    writer.write_output(
        file_path=loc_link,
        sheet_name="US ILM + ILM",
        data_dict=formatted_results,
        date_columns=date_columns,
        bold_rows=['Asset Impact', 'Liability Impact', 'Comitted Facility Impact', 'US ILM December']
    )
    
    print(f"\nCalculation completed. Results saved to sheet 'US ILM + ILM'")

if __name__ == "__main__":
    main()
