utils/init.py
from .excel_utils import ExcelReader, ExcelWriter
from .data_utils import DataProcessor

__all__ = ["ExcelReader", "ExcelWriter", "DataProcessor"]
utils/excel_utils.py
import polars as pl
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font
from datetime import datetime
from typing import Dict, List, Tuple

class ExcelReader:
    
    @staticmethod
    def read_sheet(file_path: str, sheet_name: str, header_row: int = 2) -> Tuple[pl.DataFrame, List[str]]:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        headers = []
        date_columns = []
        date_indices = []
        
        for idx, cell in enumerate(ws[header_row], start=0):
            if cell.value:
                if isinstance(cell.value, datetime):
                    col_name = cell.value.strftime('%d-%b-%y')
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                elif str(cell.value).strip() in ['31-Dec-24', '7-Jan-25', '14-Jan-25', '21-Jan-25', '28-Jan-25', 
                                                  '4-Feb-25', '11-Feb-25', '18-Feb-25', '25-Feb-25',
                                                  '4-Mar-25', '11-Mar-25', '18-Mar-25', '25-Mar-25', '31-Mar-25',
                                                  '30-Apr-25', '31-May-25', '30-Jun-25', '31-Jul-25', '31-Aug-25',
                                                  '30-Sep-25', '31-Oct-25', '30-Nov-25', '31-Dec-25']:
                    col_name = str(cell.value).strip()
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                else:
                    headers.append(str(cell.value).strip())
        
        data = []
        for row_idx, row in enumerate(ws.iter_rows(min_row=3, values_only=True)):
            if row and row[0] is not None:
                row_dict = {}
                for col_idx, value in enumerate(row[:len(headers)]):
                    if col_idx < len(headers):
                        header = headers[col_idx]
                        if col_idx in date_indices:
                            if value is None:
                                value = 0.0
                            elif isinstance(value, (int, float)):
                                value = float(value)
                            else:
                                try:
                                    value = float(str(value).replace(',', ''))
                                except:
                                    value = 0.0
                        else:
                            value = str(value).strip() if value is not None else ""
                        row_dict[header] = value
                data.append(row_dict)
        
        wb.close()
        
        df = pl.DataFrame(data) if data else pl.DataFrame()
        
        for date_col in date_columns:
            if date_col in df.columns:
                df = df.with_columns(pl.col(date_col).cast(pl.Float64))
        
        output_date_columns = [col for col in date_columns if col != '31-Dec-24']
        
        return df, output_date_columns
    
    @staticmethod
    def read_assumptions(file_path: str, sheet_name: str) -> pl.DataFrame:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        data = []
        for row_idx, row in enumerate(ws.iter_rows(min_row=3, values_only=True)):
            if row[1] is not None:
                desc = str(row[1]).strip()
                us_ilm = row[2] if len(row) > 2 and row[2] else 0
                
                if isinstance(us_ilm, str):
                    us_ilm = float(us_ilm.rstrip('%')) / 100.0 if '%' in us_ilm else 0.0
                else:
                    us_ilm = float(us_ilm) if us_ilm else 0.0
                
                data.append({'Description': desc, 'US_ILM': us_ilm, 'Row_Index': row_idx})
        
        wb.close()
        return pl.DataFrame(data)

class ExcelWriter:
    
    @staticmethod
    def write_output(file_path: str, sheet_name: str, data_dict: Dict[str, pl.DataFrame], 
                     date_columns: List[str], bold_rows: List[str] = None):
        wb = load_workbook(file_path)
        
        if sheet_name in wb.sheetnames:
            del wb[sheet_name]
        
        ws = wb.create_sheet(sheet_name)
        
        for i, col in enumerate(date_columns):
            cell = ws.cell(row=2, column=i+3)
            cell.value = col
            cell.font = Font(bold=True)
        
        row_num = 3
        
        for section_name, section_df in data_dict.items():
            for category in section_df.columns:
                if category != 'Date':
                    ws.cell(row=row_num, column=2, value=category)
                    
                    for col_idx, date_col in enumerate(date_columns):
                        value = 0.0
                        try:
                            filtered = section_df.filter(pl.col('Date') == date_col)
                            if len(filtered) > 0:
                                value = filtered[category][0]
                        except:
                            value = 0.0
                        
                        ws.cell(row=row_num, column=col_idx+3, value=value)
                    
                    if bold_rows and category in bold_rows:
                        for c in range(2, len(date_columns)+3):
                            ws.cell(row=row_num, column=c).font = Font(bold=True)
                    
                    row_num += 1
            
            if section_name != list(data_dict.keys())[-1]:
                row_num += 1
        
        wb.save(file_path)
utils/data_utils.py
import polars as pl
from typing import List

class DataProcessor:
    
    @staticmethod
    def get_value_by_row_index(df: pl.DataFrame, row_index: int, value_column: str) -> float:
        if 0 <= row_index < len(df):
            return float(df.row(row_index, named=True)[value_column])
        return 0.0
    
    @staticmethod
    def calculate_row_range_diff(df: pl.DataFrame, row_indices: List[int], 
                                 date_col: str, base_col: str) -> float:
        if not row_indices:
            return 0.0
        total_date = 0.0
        total_base = 0.0
        for idx in row_indices:
            if idx < len(df):
                row = df.row(idx, named=True)
                if date_col in row:
                    val = row[date_col]
                    total_date += float(val if val else 0)
                if base_col in row:
                    val = row[base_col]
                    total_base += float(val if val else 0)
        return total_date - total_base
    
    @staticmethod
    def calculate_sumifs(df: pl.DataFrame, date_col: str, base_col: str,
                        filter_column: str, filter_value: str) -> float:
        try:
            filter_value = str(filter_value).strip()
            filtered = df.filter(pl.col(filter_column).cast(pl.Utf8).str.strip_chars() == filter_value)
            if len(filtered) > 0:
                date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
                base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
                return float(date_sum or 0) - float(base_sum or 0)
            return 0.0
        except:
            return 0.0
    
    @staticmethod
    def calculate_sumifs_multi(df: pl.DataFrame, date_col: str, base_col: str,
                              filter_column1: str, filter_value1: str,
                              filter_column2: str, filter_value2: str) -> float:
        try:
            filter_value1 = str(filter_value1).strip()
            filter_value2 = str(filter_value2).strip()
            filtered = df.filter(
                (pl.col(filter_column1).cast(pl.Utf8).str.strip_chars() == filter_value1) &
                (pl.col(filter_column2).cast(pl.Utf8).str.strip_chars() == filter_value2)
            )
            if len(filtered) > 0:
                date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
                base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
                return float(date_sum or 0) - float(base_sum or 0)
            return 0.0
        except:
            return 0.0
src/gemini_liquidity_models/init.py
from .liquidity_impact_calculation import LiquidityImpactCalculation

__all__ = ["LiquidityImpactCalculation"]
src/gemini_liquidity_models/liquidity_impact_calculation/init.py
from .model import LiquidityImpactCalculation
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

__all__ = ["LiquidityImpactCalculation", "LiquidityCalculator", "CalculationConfig"]
src/gemini_liquidity_models/liquidity_impact_calculation/calculation_config.py
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class CalculationConfig:
    asset_row_mappings: Dict[str, List[int]]
    liability_row_mappings: Dict[str, List[int]]
    facility_row_mappings: Dict[str, List[int]]
    assumption_row_indices: Dict[str, int]
    
    @classmethod
    def get_default_config(cls):
        return cls(
            asset_row_mappings={
                'IWPB - Premier': list(range(3, 19)),
                'IWPB - Private Banking': list(range(19, 22)),
                'CIB Loans': list(range(23, 44)),
                'UST - HTM': [51],
                'Level 1 - MBS - HTM': [50],
                'Level 1 - Other - HTM': [52],
                'Level 2A - MBS - HTM': [54],
                'Level 2A - Other - HTM': [55],
                'UST - AFS': [60],
                'Level 1 - MBS - AFS': [59],
                'Level 1 - Other - AFS': [61],
                'Level 2A- MBS - AFS': [63],
                'Level 2A- Other - AFS': [64],
                'Liquid Equities': [83],
                'Illiquid Trading Assets': [87],
                'MSS - Loans': [91]
            },
            liability_row_mappings={
                'IWPB - Premier': [97],
                'PB - Personal': [105],
                'PB - Commercial - Financial': [110],
                'PB - Commercial - Non Financial': [115],
                'PB - Other': [120],
                'SME': [199],
                'Other (GPS)': [205],
                'Brokered - Committed': [210],
                'Brokered - Uncommitted': [211],
                'ISV': [212],
                'Innovation Banking': [215],
                'Other (CIB)': [218],
                'Structured CDs': [127],
                'Wholesale CDs': [136],
                'Equity': [222]
            },
            facility_row_mappings={
                'Mortgage commitments': [247],
                'Retail commitments': [250]
            },
            assumption_row_indices={
                'UST - HTM': 0,
                'Level 1 - MBS - HTM': 1,
                'Level 1 - Other - HTM': 2,
                'Level 2A - MBS - HTM': 3,
                'Level 2A - Other - HTM': 4,
                'Illiquid': 5,
                'UST - AFS': 6,
                'Level 1 - MBS - AFS': 7,
                'Level 1 - Other - AFS': 8,
                'Level 2A- MBS - AFS': 9,
                'Level 2A- Other - AFS': 10,
                'Liquid Equities': 11,
                'IWPB - Premier': 13,
                'PB - Personal': 14,
                'PB - Commercial - Financial': 15,
                'PB - Commercial - Non Financial': 16,
                'PB - Other': 17,
                'Corp - Operational': 18,
                'Corp - Non Operational': 19,
                'NBFI - Operational': 20,
                'NBFI - Non Operational': 21,
                'Banks - Operational': 22,
                'Banks - Non Operational': 23,
                'SME': 24,
                'Other (GPS)': 25,
                'Brokered - Committed': 26,
                'Brokered - Uncommitted': 27,
                'ISV': 28,
                'Innovation Banking': 29,
                'Other (CIB)': 30,
                'Credit1': 32,
                'Liquidity1': 33,
                'Credit2': 34,
                'Liquidity2': 35,
                'Credit3': 36,
                'Liquidity3': 37,
                'Mortgage commitments': 38,
                'Retail commitments': 39
            }
        )
src/gemini_liquidity_models/liquidity_impact_calculation/calculation_engine.py
import polars as pl
from typing import Dict, List
from utils.data_utils import DataProcessor

class LiquidityCalculator:
    
    def __init__(self, config: 'CalculationConfig'):
        self.config = config
        self.processor = DataProcessor()
    
    def calculate_impacts(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                         date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        asset_impact = self._calculate_asset_impact(liquidity_df, assumptions_df, date_columns, base_date)
        liability_impact = self._calculate_liability_impact(liquidity_df, assumptions_df, date_columns, base_date)
        facility_impact = self._calculate_facility_impact(liquidity_df, assumptions_df, date_columns, base_date)
        us_ilm_summary = self._calculate_us_ilm_summary(asset_impact, liability_impact, facility_impact, date_columns)
        
        return {
            'asset': asset_impact,
            'liability': liability_impact,
            'facility': facility_impact,
            'summary': us_ilm_summary
        }
    
    def _get_assumption_by_index(self, assumptions_df: pl.DataFrame, key: str) -> float:
        row_index = self.config.assumption_row_indices.get(key, -1)
        if row_index >= 0:
            return self.processor.get_value_by_row_index(assumptions_df, row_index, 'US_ILM')
        return 0.0
    
    def _calculate_asset_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['IWPB - Premier'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['IWPB - Premier'], date_col, base_date
            )
            
            row_data['IWPB - Private Banking'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['IWPB - Private Banking'], date_col, base_date
            )
            
            row_data['CIB Loans'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['CIB Loans'], date_col, base_date
            )
            
            row_data['UST - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['UST - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'UST - HTM')
            
            row_data['Level 1 - MBS - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - MBS - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - MBS - HTM')
            
            row_data['Level 1 - Other - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - Other - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - Other - HTM')
            
            row_data['Level 2A - MBS - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A - MBS - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A - MBS - HTM')
            
            row_data['Level 2A - Other - HTM'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A - Other - HTM'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A - Other - HTM')
            
            row_data['Illiquid'] = 0.0
            
            row_data['UST - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['UST - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'UST - AFS')
            
            row_data['Level 1 - MBS - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - MBS - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - MBS - AFS')
            
            row_data['Level 1 - Other - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 1 - Other - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 1 - Other - AFS')
            
            row_data['Level 2A- MBS - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A- MBS - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A- MBS - AFS')
            
            row_data['Level 2A- Other - AFS'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Level 2A- Other - AFS'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Level 2A- Other - AFS')
            
            row_data['Liquid Equities'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Liquid Equities'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquid Equities')
            
            row_data['Illiquid Trading Assets'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['Illiquid Trading Assets'], date_col, base_date
            )
            
            row_data['MSS - Loans'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.asset_row_mappings['MSS - Loans'], date_col, base_date
            )
            
            row_data['Liquid Trading Assets'] = 0.0
            
            asset_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Asset Impact'] = asset_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_liability_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['IWPB - Premier'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['IWPB - Premier'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'IWPB - Premier'))
            
            row_data['PB - Personal'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Personal'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Personal'))
            
            row_data['PB - Commercial - Financial'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Commercial - Financial'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Commercial - Financial'))
            
            row_data['PB - Commercial - Non Financial'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Commercial - Non Financial'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Commercial - Non Financial'))
            
            row_data['PB - Other'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['PB - Other'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'PB - Other'))
            
            row_data['Affiliate'] = 0.0
            row_data['Corp'] = 0.0
            
            row_data['Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level 3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Corp - Operational'))
            
            row_data['Non-Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level 3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Corp - Non Operational'))
            
            row_data['NBFI'] = 0.0
            
            row_data['Operational '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level 3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'NBFI - Operational'))
            
            row_data['Non-Operational '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level 3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'NBFI - Non Operational'))
            
            row_data['Banks'] = 0.0
            
            row_data['Operational  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level 3', 'Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Banks - Operational'))
            
            row_data['Non-Operational  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level 3', 'Non-Operational'
            ) * -(1 - self._get_assumption_by_index(assumptions_df, 'Banks - Non Operational'))
            
            row_data['SME'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['SME'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'SME'))
            
            row_data['Other (GPS)'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Other (GPS)'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Other (GPS)'))
            
            row_data['Brokered - Committed'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Brokered - Committed'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Brokered - Committed'))
            
            row_data['Brokered - Uncommitted'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Brokered - Uncommitted'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Brokered - Uncommitted'))
            
            row_data['ISV'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['ISV'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'ISV'))
            
            row_data['Innovation Banking'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Innovation Banking'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Innovation Banking'))
            
            row_data['Other (CIB)'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Other (CIB)'], date_col, base_date
            ) * (1 - self._get_assumption_by_index(assumptions_df, 'Other (CIB)'))
            
            row_data['Structured CDs'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Structured CDs'], date_col, base_date
            )
            
            row_data['Wholesale CDs'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Wholesale CDs'], date_col, base_date
            )
            
            row_data['Equity'] = -self.processor.calculate_row_range_diff(
                liquidity_df, self.config.liability_row_mappings['Equity'], date_col, base_date
            )
            
            liability_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Liability Impact'] = liability_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_facility_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            row_data['Credit'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'Non-FI', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit1')
            
            row_data['Liquidity'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'Non-FI', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity1')
            
            row_data['Credit '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'Banks', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit2')
            
            row_data['Liquidity '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'Banks', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity2')
            
            row_data['Credit  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'NBFI', 'Business', 'Credit'
            ) * -self._get_assumption_by_index(assumptions_df, 'Credit3')
            
            row_data['Liquidity  '] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance sheet', 'NBFI', 'Business', 'Liquidity'
            ) * -self._get_assumption_by_index(assumptions_df, 'Liquidity3')
            
            row_data['Mortgage commitments'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.facility_row_mappings['Mortgage commitments'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Mortgage commitments')
            
            row_data['Retail commitments'] = self.processor.calculate_row_range_diff(
                liquidity_df, self.config.facility_row_mappings['Retail commitments'], date_col, base_date
            ) * -self._get_assumption_by_index(assumptions_df, 'Retail commitments')
            
            facility_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Comitted Facility Impact'] = facility_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_us_ilm_summary(self, asset_df: pl.DataFrame, liability_df: pl.DataFrame,
                                  facility_df: pl.DataFrame, date_columns: List[str]) -> pl.DataFrame:
        results = []
        cumulative = 0.0
        
        for date_col in date_columns:
            asset_val = asset_df.filter(pl.col('Date') == date_col).select('Asset Impact')[0, 0]
            liability_val = liability_df.filter(pl.col('Date') == date_col).select('Liability Impact')[0, 0]
            facility_val = facility_df.filter(pl.col('Date') == date_col).select('Comitted Facility Impact')[0, 0]
            
            cumulative += asset_val + liability_val + facility_val
            results.append({'Date': date_col, 'US ILM December': cumulative})
        
        return pl.DataFrame(results)
src/gemini_liquidity_models/liquidity_impact_calculation/model.py
import polars as pl
from typing import Dict, List, Optional
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

class LiquidityImpactCalculation:
    
    def __init__(self, config: Optional[CalculationConfig] = None):
        self.config = config or CalculationConfig.get_default_config()
        self.calculator = LiquidityCalculator(self.config)
    
    def calculate(self, liquidity_data: pl.DataFrame, assumptions_data: pl.DataFrame,
                 date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        return self.calculator.calculate_impacts(
            liquidity_data, assumptions_data, date_columns, base_date
        )
    
    def get_formatted_output(self, calculation_results: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:
        return {
            'Assets': calculation_results['asset'],
            'Liabilities': calculation_results['liability'],
            'Committed Facilities': calculation_results['facility'],
            'US ILM Summary': calculation_results['summary']
        }
main.py
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from utils import ExcelReader, ExcelWriter

def main():
    loc_link = r"path/to/your/excel/file.xlsx"
    
    reader = ExcelReader()
    
    liquidity_data, date_columns = reader.read_sheet(
        file_path=loc_link,
        sheet_name="Liquidity Input",
        header_row=2
    )
    
    assumptions_data = reader.read_assumptions(
        file_path=loc_link,
        sheet_name="US ILM + ILM Assumptions"
    )
    
    print(f"Loaded {len(liquidity_data)} rows from Liquidity Input")
    print(f"Found {len(date_columns)} date columns")
    print(f"Loaded {len(assumptions_data)} assumptions")
    
    model = LiquidityImpactCalculation()
    
    results = model.calculate(
        liquidity_data=liquidity_data,
        assumptions_data=assumptions_data,
        date_columns=date_columns,
        base_date='31-Dec-24'
    )
    
    formatted_results = model.get_formatted_output(results)
    
    writer = ExcelWriter()
    writer.write_output(
        file_path=loc_link,
        sheet_name="US ILM + ILM",
        data_dict=formatted_results,
        date_columns=date_columns,
        bold_rows=['Asset Impact', 'Liability Impact', 'Comitted Facility Impact', 'US ILM December']
    )
    
    print(f"\nCalculation completed. Results saved to sheet 'US ILM + ILM'")

if __name__ == "__main__":
    main()
tests/init.py

tests/test_model.py
import pytest
import polars as pl
from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation

def test_model_initialization():
    model = LiquidityImpactCalculation()
    assert model is not None
    assert model.config is not None
    assert model.calculator is not None

def test_calculate():
    model = LiquidityImpactCalculation()
    
    test_liquidity_data = pl.DataFrame({
        'Balance sheet': ['Asset'] * 300,
        'Business': ['IWPB'] * 100 + ['CIB'] * 100 + ['MKTY'] * 100,
        'Level 1': ['Premier'] * 20 + ['Private Banking'] * 5 + ['CIB'] * 175 + ['MKTY'] * 100,
        'Level 2': ['Credit'] * 50 + ['Liquidity'] * 50 + ['Other'] * 200,
        'Level 3': ['Operational'] * 100 + ['Non-Operational'] * 100 + ['Other'] * 100,
        '31-Dec-24': [100.0] * 300,
        '7-Jan-25': [110.0] * 300,
    })
    
    test_assumptions = pl.DataFrame({
        'Description': ['UST - HTM', 'Level 1 - MBS - HTM'],
        'US_ILM': [0.0273, 0.018],
        'Row_Index': [0, 1]
    })
    
    results = model.calculate(
        liquidity_data=test_liquidity_data,
        assumptions_data=test_assumptions,
        date_columns=['7-Jan-25'],
        base_date='31-Dec-24'
    )
    
    assert 'asset' in results
    assert 'liability' in results
    assert 'facility' in results
    assert 'summary' in results
tests/test_data_utils.py
import pytest
import polars as pl
from utils.data_utils import DataProcessor

def test_calculate_row_range_diff():
    processor = DataProcessor()
    
    test_df = pl.DataFrame({
        'col1': ['a', 'b', 'c'],
        '31-Dec-24': [100.0, 200.0, 300.0],
        '7-Jan-25': [110.0, 220.0, 330.0]
    })
    
    diff = processor.calculate_row_range_diff(
        test_df,
        row_indices=[0, 1],
        date_col='7-Jan-25',
        base_col='31-Dec-24'
    )
    
    assert diff == 30.0

def test_get_value_by_row_index():
    processor = DataProcessor()
    
    test_df = pl.DataFrame({
        'Description': ['UST - HTM', 'Level 1 - MBS - HTM'],
        'US_ILM': [0.0273, 0.018]
    })
    
    value = processor.get_value_by_row_index(test_df, 0, 'US_ILM')
    assert value == 0.0273
    
    value = processor.get_value_by_row_index(test_df, 1, 'US_ILM')
    assert value == 0.018