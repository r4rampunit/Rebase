# utils/__init__.py
from .excel_utils import ExcelReader, ExcelWriter
from .data_utils import DataProcessor

__all__ = ["ExcelReader", "ExcelWriter", "DataProcessor"]

# utils/excel_utils.py
import polars as pl
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font
from datetime import datetime
from typing import Dict, List, Tuple

class ExcelReader:
    
    @staticmethod
    def read_sheet(file_path: str, sheet_name: str, header_row: int = 2) -> Tuple[pl.DataFrame, List[str]]:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        headers = []
        date_columns = []
        date_indices = []
        
        header_row_data = list(ws[header_row])
        
        for idx, cell in enumerate(header_row_data):
            if cell.value:
                if isinstance(cell.value, datetime):
                    col_name = cell.value.strftime('%d-%b-%y')
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                else:
                    col_name = str(cell.value).replace(' ', '_').replace('.', '')
                    headers.append(col_name)
        
        data = []
        for row in ws.iter_rows(min_row=header_row + 1, values_only=True):
            if row and row[0] is not None:
                row_dict = {}
                for col_idx in range(min(len(headers), len(row))):
                    header = headers[col_idx]
                    value = row[col_idx]
                    
                    if value is None:
                        value = 0.0 if col_idx in date_indices else ""
                    elif col_idx in date_indices:
                        if isinstance(value, (int, float)):
                            value = float(value)
                        else:
                            try:
                                value = float(str(value).replace(',', ''))
                            except:
                                value = 0.0
                    else:
                        value = str(value) if value is not None else ""
                    row_dict[header] = value
                data.append(row_dict)
        
        wb.close()
        
        if not data:
            return pl.DataFrame(), date_columns
            
        df = pl.DataFrame(data)
        
        for date_col in date_columns:
            if date_col in df.columns:
                df = df.with_columns(pl.col(date_col).cast(pl.Float64))
        
        return df, date_columns
    
    @staticmethod
    def read_assumptions(file_path: str, sheet_name: str) -> pl.DataFrame:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        data = []
        for row in ws.iter_rows(min_row=3, values_only=True):
            if len(row) > 1 and row[1] is not None:
                desc = str(row[1]).strip()
                us_ilm = row[2] if len(row) > 2 and row[2] is not None else 0
                ilm = row[3] if len(row) > 3 and row[3] is not None else 0
                
                if isinstance(us_ilm, str):
                    us_ilm = float(us_ilm.rstrip('%')) / 100.0 if '%' in us_ilm else float(us_ilm)
                else:
                    us_ilm = float(us_ilm) if us_ilm else 0.0
                
                if isinstance(ilm, str):
                    ilm = float(ilm.rstrip('%')) / 100.0 if '%' in ilm else float(ilm)
                else:
                    ilm = float(ilm) if ilm else 0.0
                
                data.append({
                    'Description': desc,
                    'US_ILM': us_ilm,
                    'ILM': ilm
                })
        
        wb.close()
        return pl.DataFrame(data)

class ExcelWriter:
    
    @staticmethod
    def write_output(file_path: str, sheet_name: str, data_dict: Dict[str, pl.DataFrame], 
                     date_columns: List[str], bold_rows: List[str] = None):
        wb = load_workbook(file_path)
        
        if sheet_name in wb.sheetnames:
            del wb[sheet_name]
        
        ws = wb.create_sheet(sheet_name)
        
        for i, col in enumerate(date_columns):
            cell = ws.cell(row=2, column=i+3)
            cell.value = col
            cell.font = Font(bold=True)
        
        row_num = 3
        
        for section_name, section_df in data_dict.items():
            for category in section_df.columns:
                if category != 'Date':
                    ws.cell(row=row_num, column=2, value=category)
                    
                    for col_idx, date_col in enumerate(date_columns):
                        value = 0.0
                        try:
                            filtered = section_df.filter(pl.col('Date') == date_col)
                            if len(filtered) > 0:
                                value = filtered[category][0]
                        except:
                            value = 0.0
                        
                        ws.cell(row=row_num, column=col_idx+3, value=value)
                    
                    if bold_rows and category in bold_rows:
                        for c in range(2, len(date_columns)+3):
                            ws.cell(row=row_num, column=c).font = Font(bold=True)
                    
                    row_num += 1
            
            if section_name != list(data_dict.keys())[-1]:
                row_num += 1
        
        wb.save(file_path)

# utils/data_utils.py
import polars as pl
from typing import List

class DataProcessor:
    
    @staticmethod
    def get_value_by_key(df: pl.DataFrame, key_column: str, key_value: str, value_column: str) -> float:
        key_value = key_value.strip()
        result = df.filter(pl.col(key_column).str.strip_chars() == key_value)
        if len(result) > 0:
            return float(result[value_column][0])
        return 0.0
    
    @staticmethod
    def calculate_row_range_diff(df: pl.DataFrame, row_indices: List[int], 
                                 date_col: str, base_col: str) -> float:
        if not row_indices:
            return 0.0
        total_date = 0.0
        total_base = 0.0
        for idx in row_indices:
            if idx < len(df):
                row = df.row(idx, named=True)
                if date_col in row:
                    total_date += float(row[date_col] or 0)
                if base_col in row:
                    total_base += float(row[base_col] or 0)
        return total_date - total_base
    
    @staticmethod
    def calculate_sumifs(df: pl.DataFrame, date_col: str, base_col: str,
                        filter_column: str, filter_value: str) -> float:
        filtered = df.filter(pl.col(filter_column).str.strip_chars() == filter_value.strip())
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0
    
    @staticmethod
    def calculate_sumifs_multi(df: pl.DataFrame, date_col: str, base_col: str,
                              filter_column1: str, filter_value1: str,
                              filter_column2: str, filter_value2: str) -> float:
        filtered = df.filter(
            (pl.col(filter_column1).str.strip_chars() == filter_value1.strip()) &
            (pl.col(filter_column2).str.strip_chars() == filter_value2.strip())
        )
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0

# src/gemini_liquidity_models/__init__.py
from .liquidity_impact_calculation import LiquidityImpactCalculation

__all__ = ["LiquidityImpactCalculation"]

# src/gemini_liquidity_models/liquidity_impact_calculation/__init__.py
from .model import LiquidityImpactCalculation
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

__all__ = ["LiquidityImpactCalculation", "LiquidityCalculator", "CalculationConfig"]

# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_config.py
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class CalculationConfig:
    asset_row_mappings: Dict[str, List[int]]
    liability_row_mappings: Dict[str, List[int]]
    facility_row_mappings: Dict[str, List[int]]
    assumption_mappings: Dict[str, str]
    
    @classmethod
    def get_default_config(cls):
        return cls(
            asset_row_mappings={
                'IWPB - Premier': list(range(3, 19)),
                'IWPB - Private Banking': list(range(19, 23)),
                'CIB Loans': list(range(23, 45)),
                'UST - HTM': [52],
                'Level 1 - MBS - HTM': [51],
                'Level 1 - Other - HTM': [53],
                'Level 2A - MBS - HTM': [55],
                'Level 2A - Other - HTM': [56],
                'UST - AFS': [61],
                'Level 1 - MBS - AFS': [60],
                'Level 1 - Other - AFS': [62],
                'Level 2A - MBS - AFS': [64],
                'Level 2A - Other - AFS': [65],
                'Liquid Equities': [84],
                'Illiquid Trading Assets': [88],
                'MSS - Loans': [92]
            },
            liability_row_mappings={
                'IWPB - Premier': [98],
                'PB - Personal': [106],
                'PB - Commercial - Financial': [111],
                'PB - Commercial - Non Financial': [116],
                'PB - Other': [121],
                'SME': [200],
                'Other (GPS)': [206],
                'Brokered - Committed': [211],
                'Brokered - Uncommitted': [212],
                'ISV': [213],
                'Innovation Banking': [216],
                'Other (CIB)': [219],
                'Structured CDs': [128],
                'Wholesale CDs': [137],
                'Equity': [223]
            },
            facility_row_mappings={
                'Mortgage commitments': [248],
                'Retail commitments': [251]
            },
            assumption_mappings={
                'UST - HTM': 'UST - HTM',
                'Level 1 - MBS - HTM': 'Level 1 - MBS - HTM',
                'Level 1 - Other - HTM': 'Level 1 - Other - HTM',
                'Level 2A - MBS - HTM': 'Level 2A - MBS - HTM',
                'Level 2A - Other - HTM': 'Level 2A - Other - HTM',
                'UST - AFS': 'UST - AFS',
                'Level 1 - MBS - AFS': 'Level 1 - MBS - AFS',
                'Level 1 - Other - AFS': 'Level 1 - Other - AFS',
                'Level 2A - MBS - AFS': 'Level 2A - MBS - AFS',
                'Level 2A - Other - AFS': 'Level 2A - Other - AFS',
                'Liquid Equities': 'Equities',
                'IWPB - Premier': 'IWPB - Premier',
                'PB - Personal': 'PB - Personal',
                'PB - Commercial - Financial': 'PB - Commercial - Financial',
                'PB - Commercial - Non Financial': 'PB - Commercial - Non Financial',
                'PB - Other': 'PB - Other',
                'SME': 'SME',
                'Other (GPS)': 'Other (GPS)',
                'Brokered - Committed': 'Brokered - Committed',
                'Brokered - Uncommitted': 'Brokered - Uncommitted',
                'ISV': 'ISV',
                'Innovation Banking': 'Innovation Banking',
                'Other (CIB)': 'Other (CIB)',
                'Corp - Operational': 'Corp - Operational',
                'Corp - Non Operational': 'Corp - Non Operational',
                'NBFI - Operational': 'NBFI - Operational',
                'NBFI - Non Operational': 'NBFI - Non Operational',
                'Banks - Operational': 'Banks - Operational',
                'Banks - Non Operational': 'Banks - Non Operational',
                'Credit': 'Credit',
                'Liquidity': 'Liquidity',
                'Non - FI Credit': 'Non - FI Credit',
                'Non - FI Liquidity': 'Non - FI Liquidity',
                'Banks Credit': 'Banks Credit',
                'Banks Liquidity': 'Banks Liquidity',
                'NBFI Credit': 'NBFI Credit',
                'NBFI Liquidity': 'NBFI Liquidity',
                'Mortgage commitments': 'Mortgage commitments',
                'Retail commitments': 'Retail commitments'
            }
        )

# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_engine.py
import polars as pl
from typing import Dict, List
from utils.data_utils import DataProcessor

class LiquidityCalculator:
    
    def __init__(self, config: 'CalculationConfig'):
        self.config = config
        self.processor = DataProcessor()
    
    def calculate_impacts(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                         date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        asset_impact = self._calculate_asset_impact(liquidity_df, assumptions_df, date_columns, base_date)
        liability_impact = self._calculate_liability_impact(liquidity_df, assumptions_df, date_columns, base_date)
        facility_impact = self._calculate_facility_impact(liquidity_df, assumptions_df, date_columns, base_date)
        us_ilm_summary = self._calculate_us_ilm_summary(asset_impact, liability_impact, facility_impact, date_columns)
        
        return {
            'asset': asset_impact,
            'liability': liability_impact,
            'facility': facility_impact,
            'summary': us_ilm_summary
        }
    
    def _get_assumption(self, assumptions_df: pl.DataFrame, key: str) -> float:
        mapped_key = self.config.assumption_mappings.get(key, key)
        return self.processor.get_value_by_key(assumptions_df, 'Description', mapped_key, 'US_ILM')
    
    def _calculate_asset_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.asset_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['IWPB - Premier', 'IWPB - Private Banking', 'CIB Loans', 
                                'Illiquid Trading Assets', 'MSS - Loans']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = diff * -assumption_val
            
            row_data['Illiquid'] = 0.0
            row_data['Liquid Trading Assets'] = 0.0
            
            asset_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Asset Impact'] = asset_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_liability_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.liability_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['Structured CDs', 'Wholesale CDs', 'Equity', 
                                'Brokered - Uncommitted', 'ISV', 'Innovation Banking', 'Other (CIB)']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = -diff * (1 - assumption_val)
            
            row_data['Affiliate'] = 0.0
            row_data['Corp'] = 0.0
            
            row_data['Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp - Operational'))
            
            row_data['Non-Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp - Non Operational'))
            
            row_data['NBFI'] = 0.0
            
            row_data['Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI - Operational'))
            
            row_data['Non-Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI - Non Operational'))
            
            row_data['Banks'] = 0.0
            
            row_data['Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks - Operational'))
            
            row_data['Non-Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks - Non Operational'))
            
            liability_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Liability Impact'] = liability_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_facility_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            non_fi_credit = self._get_assumption(assumptions_df, 'Non - FI Credit')
            non_fi_liq = self._get_assumption(assumptions_df, 'Non - FI Liquidity')
            banks_credit = self._get_assumption(assumptions_df, 'Banks Credit')
            banks_liq = self._get_assumption(assumptions_df, 'Banks Liquidity')
            nbfi_credit = self._get_assumption(assumptions_df, 'NBFI Credit')
            nbfi_liq = self._get_assumption(assumptions_df, 'NBFI Liquidity')
            
            row_data['Credit'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non - FI', 'Level_2', 'Credit'
            ) * -non_fi_credit
            
            row_data['Liquidity'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non - FI', 'Level_2', 'Liquidity'
            ) * -non_fi_liq
            
            row_data['Credit_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Level_2', 'Credit'
            ) * -banks_credit
            
            row_data['Liquidity_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Level_2', 'Liquidity'
            ) * -banks_liq
            
            row_data['Credit_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Level_2', 'Credit'
            ) * -nbfi_credit
            
            row_data['Liquidity_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Level_2', 'Liquidity'
            ) * -nbfi_liq
            
            for item_name, row_indices in self.config.facility_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                assumption_val = self._get_assumption(assumptions_df, item_name)
                row_data[item_name] = diff * -assumption_val
            
            facility_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Comitted Facility Impact'] = facility_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_us_ilm_summary(self, asset_df: pl.DataFrame, liability_df: pl.DataFrame,
                                  facility_df: pl.DataFrame, date_columns: List[str]) -> pl.DataFrame:
        results = []
        cumulative = 0.0
        
        for date_col in date_columns:
            asset_val = asset_df.filter(pl.col('Date') == date_col).select('Asset Impact')[0, 0]
            liability_val = liability_df.filter(pl.col('Date') == date_col).select('Liability Impact')[0, 0]
            facility_val = facility_df.filter(pl.col('Date') == date_col).select('Comitted Facility Impact')[0, 0]
            
            cumulative += asset_val + liability_val + facility_val
            results.append({'Date': date_col, 'US ILM December': cumulative})
        
        return pl.DataFrame(results)

# src/gemini_liquidity_models/liquidity_impact_calculation/model.py
import polars as pl
from typing import Dict, List, Optional
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

class LiquidityImpactCalculation:
    
    def __init__(self, config: Optional[CalculationConfig] = None):
        self.config = config or CalculationConfig.get_default_config()
        self.calculator = LiquidityCalculator(self.config)
    
    def calculate(self, liquidity_data: pl.DataFrame, assumptions_data: pl.DataFrame,
                 date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        return self.calculator.calculate_impacts(
            liquidity_data, assumptions_data, date_columns, base_date
        )
    
    def get_formatted_output(self, calculation_results: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:
        return {
            'Assets': calculation_results['asset'],
            'Liabilities': calculation_results['liability'],
            'Committed Facilities': calculation_results['facility'],
            'US ILM Summary': calculation_results['summary']
        }

# main.py
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from utils import ExcelReader, ExcelWriter

def main():
    loc_link = r"path/to/your/excel/file.xlsx"
    
    reader = ExcelReader()
    
    liquidity_data, date_columns = reader.read_sheet(
        file_path=loc_link,
        sheet_name="Liquidity Input",
        header_row=2
    )
    
    assumptions_data = reader.read_assumptions(
        file_path=loc_link,
        sheet_name="US ILM ILM Assumptions"
    )
    
    print(f"Loaded {len(liquidity_data)} rows from Liquidity Input")
    print(f"Found {len(date_columns)} date columns")
    print(f"Loaded {len(assumptions_data)} assumptions")
    
    model = LiquidityImpactCalculation()
    
    results = model.calculate(
        liquidity_data=liquidity_data,
        assumptions_data=assumptions_data,
        date_columns=date_columns,
        base_date='31-Dec-24'
    )
    
    formatted_results = model.get_formatted_output(results)
    
    writer = ExcelWriter()
    writer.write_output(
        file_path=loc_link,
        sheet_name="US ILM + ILM",
        data_dict=formatted_results,
        date_columns=date_columns,
        bold_rows=['Asset Impact', 'Liability Impact', 'Comitted Facility Impact', 'US ILM December']
    )
    
    print(f"\nCalculation completed. Results saved to sheet 'US ILM + ILM'")

if __name__ == "__main__":
    main()















# utils/__init__.py
from .excel_utils import ExcelReader, ExcelWriter
from .data_utils import DataProcessor

__all__ = ["ExcelReader", "ExcelWriter", "DataProcessor"]

# utils/excel_utils.py
import polars as pl
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font
from datetime import datetime
from typing import Dict, List, Tuple

class ExcelReader:
    
    @staticmethod
    def read_sheet(file_path: str, sheet_name: str, header_row: int = 2) -> Tuple[pl.DataFrame, List[str]]:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        headers = []
        date_columns = []
        date_indices = []
        
        header_row_data = list(ws[header_row])
        
        for idx, cell in enumerate(header_row_data):
            if cell.value:
                if isinstance(cell.value, datetime):
                    col_name = cell.value.strftime('%d-%b-%y')
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                else:
                    col_name = str(cell.value).replace(' ', '_').replace('.', '')
                    headers.append(col_name)
        
        data = []
        for row in ws.iter_rows(min_row=header_row + 1, values_only=True):
            if row and row[0] is not None:
                row_dict = {}
                for col_idx in range(min(len(headers), len(row))):
                    header = headers[col_idx]
                    value = row[col_idx]
                    
                    if value is None:
                        value = 0.0 if col_idx in date_indices else ""
                    elif col_idx in date_indices:
                        if isinstance(value, (int, float)):
                            value = float(value)
                        else:
                            try:
                                value = float(str(value).replace(',', ''))
                            except:
                                value = 0.0
                    else:
                        value = str(value) if value is not None else ""
                    row_dict[header] = value
                data.append(row_dict)
        
        wb.close()
        
        if not data:
            return pl.DataFrame(), date_columns
            
        df = pl.DataFrame(data)
        
        for date_col in date_columns:
            if date_col in df.columns:
                df = df.with_columns(pl.col(date_col).cast(pl.Float64))
        
        return df, date_columns
    
    @staticmethod
    def read_assumptions(file_path: str, sheet_name: str) -> pl.DataFrame:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        data = []
        for row in ws.iter_rows(min_row=3, values_only=True):
            if len(row) > 1 and row[1] is not None:
                desc = str(row[1]).strip()
                us_ilm = row[2] if len(row) > 2 and row[2] is not None else 0
                ilm = row[3] if len(row) > 3 and row[3] is not None else 0
                
                if isinstance(us_ilm, str):
                    us_ilm = float(us_ilm.rstrip('%')) / 100.0 if '%' in us_ilm else float(us_ilm)
                else:
                    us_ilm = float(us_ilm) if us_ilm else 0.0
                
                if isinstance(ilm, str):
                    ilm = float(ilm.rstrip('%')) / 100.0 if '%' in ilm else float(ilm)
                else:
                    ilm = float(ilm) if ilm else 0.0
                
                data.append({
                    'Description': desc,
                    'US_ILM': us_ilm,
                    'ILM': ilm
                })
        
        wb.close()
        return pl.DataFrame(data)

class ExcelWriter:
    
    @staticmethod
    def write_output(file_path: str, sheet_name: str, data_dict: Dict[str, pl.DataFrame], 
                     date_columns: List[str], bold_rows: List[str] = None):
        wb = load_workbook(file_path)
        
        if sheet_name in wb.sheetnames:
            del wb[sheet_name]
        
        ws = wb.create_sheet(sheet_name)
        
        for i, col in enumerate(date_columns):
            cell = ws.cell(row=2, column=i+3)
            cell.value = col
            cell.font = Font(bold=True)
        
        row_num = 3
        
        for section_name, section_df in data_dict.items():
            for category in section_df.columns:
                if category != 'Date':
                    ws.cell(row=row_num, column=2, value=category)
                    
                    for col_idx, date_col in enumerate(date_columns):
                        value = 0.0
                        try:
                            filtered = section_df.filter(pl.col('Date') == date_col)
                            if len(filtered) > 0:
                                value = filtered[category][0]
                        except:
                            value = 0.0
                        
                        ws.cell(row=row_num, column=col_idx+3, value=value)
                    
                    if bold_rows and category in bold_rows:
                        for c in range(2, len(date_columns)+3):
                            ws.cell(row=row_num, column=c).font = Font(bold=True)
                    
                    row_num += 1
            
            if section_name != list(data_dict.keys())[-1]:
                row_num += 1
        
        wb.save(file_path)

# utils/data_utils.py
import polars as pl
from typing import List

class DataProcessor:
    
    @staticmethod
    def get_value_by_key(df: pl.DataFrame, key_column: str, key_value: str, value_column: str) -> float:
        key_value = key_value.strip()
        result = df.filter(pl.col(key_column).str.strip_chars() == key_value)
        if len(result) > 0:
            return float(result[value_column][0])
        return 0.0
    
    @staticmethod
    def calculate_row_range_diff(df: pl.DataFrame, row_indices: List[int], 
                                 date_col: str, base_col: str) -> float:
        if not row_indices:
            return 0.0
        total_date = 0.0
        total_base = 0.0
        for idx in row_indices:
            if idx < len(df):
                row = df.row(idx, named=True)
                if date_col in row:
                    total_date += float(row[date_col] or 0)
                if base_col in row:
                    total_base += float(row[base_col] or 0)
        return total_date - total_base
    
    @staticmethod
    def calculate_sumifs(df: pl.DataFrame, date_col: str, base_col: str,
                        filter_column: str, filter_value: str) -> float:
        filtered = df.filter(pl.col(filter_column).str.strip_chars() == filter_value.strip())
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0
    
    @staticmethod
    def calculate_sumifs_multi(df: pl.DataFrame, date_col: str, base_col: str,
                              filter_column1: str, filter_value1: str,
                              filter_column2: str, filter_value2: str) -> float:
        filtered = df.filter(
            (pl.col(filter_column1).str.strip_chars() == filter_value1.strip()) &
            (pl.col(filter_column2).str.strip_chars() == filter_value2.strip())
        )
        if len(filtered) > 0:
            date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
            base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
            return float(date_sum or 0) - float(base_sum or 0)
        return 0.0

# src/gemini_liquidity_models/__init__.py
from .liquidity_impact_calculation import LiquidityImpactCalculation

__all__ = ["LiquidityImpactCalculation"]

# src/gemini_liquidity_models/liquidity_impact_calculation/__init__.py
from .model import LiquidityImpactCalculation
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

__all__ = ["LiquidityImpactCalculation", "LiquidityCalculator", "CalculationConfig"]

# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_config.py
from dataclasses import dataclass
from typing import Dict, List

@dataclass
class CalculationConfig:
    asset_row_mappings: Dict[str, List[int]]
    liability_row_mappings: Dict[str, List[int]]
    facility_row_mappings: Dict[str, List[int]]
    assumption_mappings: Dict[str, str]
    
    @classmethod
    def get_default_config(cls):
        return cls(
            asset_row_mappings={
                'IWPB - Premier': list(range(3, 19)),
                'IWPB - Private Banking': list(range(19, 23)),
                'CIB Loans': list(range(23, 45)),
                'UST - HTM': [52],
                'Level 1 - MBS - HTM': [51],
                'Level 1 - Other - HTM': [53],
                'Level 2A - MBS - HTM': [55],
                'Level 2A - Other - HTM': [56],
                'UST - AFS': [61],
                'Level 1 - MBS - AFS': [60],
                'Level 1 - Other - AFS': [62],
                'Level 2A - MBS - AFS': [64],
                'Level 2A - Other - AFS': [65],
                'Liquid Equities': [84],
                'Illiquid Trading Assets': [88],
                'MSS - Loans': [92]
            },
            liability_row_mappings={
                'IWPB - Premier': [98],
                'PB - Personal': [106],
                'PB - Commercial - Financial': [111],
                'PB - Commercial - Non Financial': [116],
                'PB - Other': [121],
                'SME': [200],
                'Other (GPS)': [206],
                'Brokered - Committed': [211],
                'Brokered - Uncommitted': [212],
                'ISV': [213],
                'Innovation Banking': [216],
                'Other (CIB)': [219],
                'Structured CDs': [128],
                'Wholesale CDs': [137],
                'Equity': [223]
            },
            facility_row_mappings={
                'Mortgage commitments': [248],
                'Retail commitments': [251]
            },
            assumption_mappings={
                'UST - HTM': 'UST - HTM',
                'Level 1 - MBS - HTM': 'Level 1 - MBS - HTM',
                'Level 1 - Other - HTM': 'Level 1 - Other - HTM',
                'Level 2A - MBS - HTM': 'Level 2A - MBS - HTM',
                'Level 2A - Other - HTM': 'Level 2A - Other - HTM',
                'UST - AFS': 'UST - AFS',
                'Level 1 - MBS - AFS': 'Level 1 - MBS - AFS',
                'Level 1 - Other - AFS': 'Level 1 - Other - AFS',
                'Level 2A - MBS - AFS': 'Level 2A - MBS - AFS',
                'Level 2A - Other - AFS': 'Level 2A - Other - AFS',
                'Liquid Equities': 'Equities',
                'IWPB - Premier': 'IWPB - Premier',
                'PB - Personal': 'PB - Personal',
                'PB - Commercial - Financial': 'PB - Commercial - Financial',
                'PB - Commercial - Non Financial': 'PB - Commercial - Non Financial',
                'PB - Other': 'PB - Other',
                'SME': 'SME',
                'Other (GPS)': 'Other (GPS)',
                'Brokered - Committed': 'Brokered - Committed',
                'Brokered - Uncommitted': 'Brokered - Uncommitted',
                'ISV': 'ISV',
                'Innovation Banking': 'Innovation Banking',
                'Other (CIB)': 'Other (CIB)',
                'Corp - Operational': 'Corp - Operational',
                'Corp - Non Operational': 'Corp - Non Operational',
                'NBFI - Operational': 'NBFI - Operational',
                'NBFI - Non Operational': 'NBFI - Non Operational',
                'Banks - Operational': 'Banks - Operational',
                'Banks - Non Operational': 'Banks - Non Operational',
                'Credit': 'Credit',
                'Liquidity': 'Liquidity',
                'Non - FI Credit': 'Non - FI Credit',
                'Non - FI Liquidity': 'Non - FI Liquidity',
                'Banks Credit': 'Banks Credit',
                'Banks Liquidity': 'Banks Liquidity',
                'NBFI Credit': 'NBFI Credit',
                'NBFI Liquidity': 'NBFI Liquidity',
                'Mortgage commitments': 'Mortgage commitments',
                'Retail commitments': 'Retail commitments'
            }
        )

# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_engine.py
import polars as pl
from typing import Dict, List
from utils.data_utils import DataProcessor

class LiquidityCalculator:
    
    def __init__(self, config: 'CalculationConfig'):
        self.config = config
        self.processor = DataProcessor()
    
    def calculate_impacts(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                         date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        asset_impact = self._calculate_asset_impact(liquidity_df, assumptions_df, date_columns, base_date)
        liability_impact = self._calculate_liability_impact(liquidity_df, assumptions_df, date_columns, base_date)
        facility_impact = self._calculate_facility_impact(liquidity_df, assumptions_df, date_columns, base_date)
        us_ilm_summary = self._calculate_us_ilm_summary(asset_impact, liability_impact, facility_impact, date_columns)
        
        return {
            'asset': asset_impact,
            'liability': liability_impact,
            'facility': facility_impact,
            'summary': us_ilm_summary
        }
    
    def _get_assumption(self, assumptions_df: pl.DataFrame, key: str) -> float:
        mapped_key = self.config.assumption_mappings.get(key, key)
        return self.processor.get_value_by_key(assumptions_df, 'Description', mapped_key, 'US_ILM')
    
    def _calculate_asset_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.asset_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['IWPB - Premier', 'IWPB - Private Banking', 'CIB Loans', 
                                'Illiquid Trading Assets', 'MSS - Loans']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = diff * -assumption_val
            
            row_data['Illiquid'] = 0.0
            row_data['Liquid Trading Assets'] = 0.0
            
            asset_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Asset Impact'] = asset_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_liability_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.liability_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['Structured CDs', 'Wholesale CDs', 'Equity', 
                                'Brokered - Uncommitted', 'ISV', 'Innovation Banking', 'Other (CIB)']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = -diff * (1 - assumption_val)
            
            row_data['Affiliate'] = 0.0
            row_data['Corp'] = 0.0
            
            row_data['Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp - Operational'))
            
            row_data['Non-Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp - Non Operational'))
            
            row_data['NBFI'] = 0.0
            
            row_data['Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI - Operational'))
            
            row_data['Non-Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI - Non Operational'))
            
            row_data['Banks'] = 0.0
            
            row_data['Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks - Operational'))
            
            row_data['Non-Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks - Non Operational'))
            
            liability_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Liability Impact'] = liability_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_facility_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            non_fi_credit = self._get_assumption(assumptions_df, 'Non - FI Credit')
            non_fi_liq = self._get_assumption(assumptions_df, 'Non - FI Liquidity')
            banks_credit = self._get_assumption(assumptions_df, 'Banks Credit')
            banks_liq = self._get_assumption(assumptions_df, 'Banks Liquidity')
            nbfi_credit = self._get_assumption(assumptions_df, 'NBFI Credit')
            nbfi_liq = self._get_assumption(assumptions_df, 'NBFI Liquidity')
            
            row_data['Credit'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non - FI', 'Level_2', 'Credit'
            ) * -non_fi_credit
            
            row_data['Liquidity'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Non - FI', 'Level_2', 'Liquidity'
            ) * -non_fi_liq
            
            row_data['Credit_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Level_2', 'Credit'
            ) * -banks_credit
            
            row_data['Liquidity_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'Banks', 'Level_2', 'Liquidity'
            ) * -banks_liq
            
            row_data['Credit_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Level_2', 'Credit'
            ) * -nbfi_credit
            
            row_data['Liquidity_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Balance_sheet', 'NBFI', 'Level_2', 'Liquidity'
            ) * -nbfi_liq
            
            for item_name, row_indices in self.config.facility_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                assumption_val = self._get_assumption(assumptions_df, item_name)
                row_data[item_name] = diff * -assumption_val
            
            facility_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Comitted Facility Impact'] = facility_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_us_ilm_summary(self, asset_df: pl.DataFrame, liability_df: pl.DataFrame,
                                  facility_df: pl.DataFrame, date_columns: List[str]) -> pl.DataFrame:
        results = []
        cumulative = 0.0
        
        for date_col in date_columns:
            asset_val = asset_df.filter(pl.col('Date') == date_col).select('Asset Impact')[0, 0]
            liability_val = liability_df.filter(pl.col('Date') == date_col).select('Liability Impact')[0, 0]
            facility_val = facility_df.filter(pl.col('Date') == date_col).select('Comitted Facility Impact')[0, 0]
            
            cumulative += asset_val + liability_val + facility_val
            results.append({'Date': date_col, 'US ILM December': cumulative})
        
        return pl.DataFrame(results)

# src/gemini_liquidity_models/liquidity_impact_calculation/model.py
import polars as pl
from typing import Dict, List, Optional
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

class LiquidityImpactCalculation:
    
    def __init__(self, config: Optional[CalculationConfig] = None):
        self.config = config or CalculationConfig.get_default_config()
        self.calculator = LiquidityCalculator(self.config)
    
    def calculate(self, liquidity_data: pl.DataFrame, assumptions_data: pl.DataFrame,
                 date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        return self.calculator.calculate_impacts(
            liquidity_data, assumptions_data, date_columns, base_date
        )
    
    def get_formatted_output(self, calculation_results: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:
        return {
            'Assets': calculation_results['asset'],
            'Liabilities': calculation_results['liability'],
            'Committed Facilities': calculation_results['facility'],
            'US ILM Summary': calculation_results['summary']
        }

# main.py
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from utils import ExcelReader, ExcelWriter

def main():
    loc_link = r"path/to/your/excel/file.xlsx"
    
    reader = ExcelReader()
    
    liquidity_data, date_columns = reader.read_sheet(
        file_path=loc_link,
        sheet_name="Liquidity Input",
        header_row=2
    )
    
    assumptions_data = reader.read_assumptions(
        file_path=loc_link,
        sheet_name="US ILM ILM Assumptions"
    )
    
    print(f"Loaded {len(liquidity_data)} rows from Liquidity Input")
    print(f"Found {len(date_columns)} date columns")
    print(f"Loaded {len(assumptions_data)} assumptions")
    
    model = LiquidityImpactCalculation()
    
    results = model.calculate(
        liquidity_data=liquidity_data,
        assumptions_data=assumptions_data,
        date_columns=date_columns,
        base_date='31-Dec-24'
    )
    
    formatted_results = model.get_formatted_output(results)
    
    writer = ExcelWriter()
    writer.write_output(
        file_path=loc_link,
        sheet_name="US ILM + ILM",
        data_dict=formatted_results,
        date_columns=date_columns,
        bold_rows=['Asset Impact', 'Liability Impact', 'Comitted Facility Impact', 'US ILM December']
    )
    
    print(f"\nCalculation completed. Results saved to sheet 'US ILM + ILM'")

if __name__ == "__main__":
    main()

# tests/conftest.py
import pytest
import polars as pl
from typing import List
from src.gemini_liquidity_models.liquidity_impact_calculation.calculation_config import CalculationConfig

@pytest.fixture
def sample_liquidity_data():
    data = {
        'Balance_sheet': ['Asset'] * 100 + ['Non - FI'] * 50 + ['Banks'] * 50 + ['NBFI'] * 50,
        'Business': ['IWPB'] * 100 + ['Corp'] * 50 + ['Banks'] * 50 + ['NBFI'] * 50,
        'Level_1': ['Premier'] * 100 + [''] * 150,
        'Level_2': [''] * 100 + ['Credit'] * 75 + ['Liquidity'] * 75,
        'Level_3': [''] * 100 + ['Operational'] * 75 + ['Non-Operational'] * 75,
        '31-Dec-24': [100.0] * 250,
        '07-Jan-25': [110.0] * 250,
        '14-Jan-25': [120.0] * 250,
    }
    return pl.DataFrame(data)

@pytest.fixture
def sample_assumptions_data():
    data = {
        'Description': [
            'UST - HTM', 'Level 1 - MBS - HTM', 'Level 1 - Other - HTM',
            'Level 2A - MBS - HTM', 'Level 2A - Other - HTM', 'Illiquid',
            'UST - AFS', 'Level 1 - MBS - AFS', 'Level 1 - Other - AFS',
            'Level 2A - MBS - AFS', 'Level 2A - Other - AFS', 'Equities',
            'IWPB - Premier', 'PB - Personal', 'PB - Commercial - Financial',
            'PB - Commercial - Non Financial', 'PB - Other',
            'Corp - Operational', 'Corp - Non Operational',
            'NBFI - Operational', 'NBFI - Non Operational',
            'Banks - Operational', 'Banks - Non Operational',
            'SME', 'Other (GPS)', 'Brokered - Committed', 'Brokered - Uncommitted',
            'ISV', 'Innovation Banking', 'Other (CIB)',
            'Non - FI Credit', 'Non - FI Liquidity',
            'Banks Credit', 'Banks Liquidity',
            'NBFI Credit', 'NBFI Liquidity',
            'Mortgage commitments', 'Retail commitments'
        ],
        'US_ILM': [
            0.0273, 0.018, 0.05, 0.014, 0.05, 1.0,
            0.0009, 0.0071, 0.002, 0.0055, 0.01, 0.15,
            0.125, 0.10, 0.325, 1.0, 0.10,
            0.175, 0.318, 0.25, 1.0, 0.25, 1.0,
            0.20, 0.40, 0.20, 0.50, 0.30, 0.60, 0.30,
            0.10, 0.30, 0.50, 0.50, 0.40, 1.00,
            0.10, 0.10
        ],
        'ILM': [0.0] * 38
    }
    return pl.DataFrame(data)

@pytest.fixture
def date_columns():
    return ['07-Jan-25', '14-Jan-25']

@pytest.fixture
def default_config():
    return CalculationConfig.get_default_config()

# tests/test_data_utils.py
import pytest
import polars as pl
from utils.data_utils import DataProcessor

class TestDataProcessor:
    
    def test_get_value_by_key(self, sample_assumptions_data):
        processor = DataProcessor()
        
        value = processor.get_value_by_key(
            sample_assumptions_data, 'Description', 'UST - HTM', 'US_ILM'
        )
        assert value == 0.0273
        
        value = processor.get_value_by_key(
            sample_assumptions_data, 'Description', 'NonExistent', 'US_ILM'
        )
        assert value == 0.0
    
    def test_calculate_row_range_diff(self, sample_liquidity_data):
        processor = DataProcessor()
        
        diff = processor.calculate_row_range_diff(
            sample_liquidity_data,
            row_indices=[0, 1, 2],
            date_col='07-Jan-25',
            base_col='31-Dec-24'
        )
        assert diff == 30.0
        
        diff = processor.calculate_row_range_diff(
            sample_liquidity_data,
            row_indices=[],
            date_col='07-Jan-25',
            base_col='31-Dec-24'
        )
        assert diff == 0.0
    
    def test_calculate_sumifs(self, sample_liquidity_data):
        processor = DataProcessor()
        
        diff = processor.calculate_sumifs(
            sample_liquidity_data,
            date_col='07-Jan-25',
            base_col='31-Dec-24',
            filter_column='Level_3',
            filter_value='Operational'
        )
        assert diff == 750.0
    
    def test_calculate_sumifs_multi(self, sample_liquidity_data):
        processor = DataProcessor()
        
        diff = processor.calculate_sumifs_multi(
            sample_liquidity_data,
            date_col='07-Jan-25',
            base_col='31-Dec-24',
            filter_column1='Balance_sheet',
            filter_value1='Non - FI',
            filter_column2='Level_2',
            filter_value2='Credit'
        )
        assert diff > 0

# tests/test_calculation_config.py
import pytest
from src.gemini_liquidity_models.liquidity_impact_calculation.calculation_config import CalculationConfig

class TestCalculationConfig:
    
    def test_default_config_creation(self):
        config = CalculationConfig.get_default_config()
        
        assert isinstance(config.asset_row_mappings, dict)
        assert isinstance(config.liability_row_mappings, dict)
        assert isinstance(config.facility_row_mappings, dict)
        assert isinstance(config.assumption_mappings, dict)
        
        assert 'IWPB - Premier' in config.asset_row_mappings
        assert config.asset_row_mappings['IWPB - Premier'] == list(range(3, 19))
        
        assert 'IWPB - Premier' in config.liability_row_mappings
        assert config.liability_row_mappings['IWPB - Premier'] == [98]
        
        assert 'Mortgage commitments' in config.facility_row_mappings
        assert config.facility_row_mappings['Mortgage commitments'] == [248]
        
        assert 'UST - HTM' in config.assumption_mappings
        assert config.assumption_mappings['UST - HTM'] == 'UST - HTM'
        
        assert 'Non - FI Credit' in config.assumption_mappings
        assert 'NBFI Liquidity' in config.assumption_mappings
    
    def test_custom_config_creation(self):
        custom_config = CalculationConfig(
            asset_row_mappings={'Test Asset': [1, 2, 3]},
            liability_row_mappings={'Test Liability': [4, 5]},
            facility_row_mappings={'Test Facility': [6]},
            assumption_mappings={'Test': 'Test Mapping'}
        )
        
        assert custom_config.asset_row_mappings['Test Asset'] == [1, 2, 3]
        assert custom_config.liability_row_mappings['Test Liability'] == [4, 5]
        assert custom_config.facility_row_mappings['Test Facility'] == [6]
        assert custom_config.assumption_mappings['Test'] == 'Test Mapping'

# tests/test_calculation_engine.py
import pytest
import polars as pl
from src.gemini_liquidity_models.liquidity_impact_calculation.calculation_engine import LiquidityCalculator
from src.gemini_liquidity_models.liquidity_impact_calculation.calculation_config import CalculationConfig

class TestLiquidityCalculator:
    
    def test_initialization(self, default_config):
        calculator = LiquidityCalculator(default_config)
        assert calculator.config == default_config
        assert calculator.processor is not None
    
    def test_calculate_impacts(self, sample_liquidity_data, sample_assumptions_data, 
                              date_columns, default_config):
        calculator = LiquidityCalculator(default_config)
        
        results = calculator.calculate_impacts(
            sample_liquidity_data,
            sample_assumptions_data,
            date_columns,
            '31-Dec-24'
        )
        
        assert 'asset' in results
        assert 'liability' in results
        assert 'facility' in results
        assert 'summary' in results
        
        assert isinstance(results['asset'], pl.DataFrame)
        assert isinstance(results['liability'], pl.DataFrame)
        assert isinstance(results['facility'], pl.DataFrame)
        assert isinstance(results['summary'], pl.DataFrame)
        
        assert len(results['asset']) == len(date_columns)
        assert len(results['liability']) == len(date_columns)
        assert len(results['facility']) == len(date_columns)
        assert len(results['summary']) == len(date_columns)
        
        assert 'Asset Impact' in results['asset'].columns
        assert 'Liability Impact' in results['liability'].columns
        assert 'Comitted Facility Impact' in results['facility'].columns
        assert 'US ILM December' in results['summary'].columns
    
    def test_get_assumption(self, sample_assumptions_data, default_config):
        calculator = LiquidityCalculator(default_config)
        
        value = calculator._get_assumption(sample_assumptions_data, 'UST - HTM')
        assert value == 0.0273
        
        value = calculator._get_assumption(sample_assumptions_data, 'IWPB - Premier')
        assert value == 0.125
        
        value = calculator._get_assumption(sample_assumptions_data, 'Non - FI Credit')
        assert value == 0.10
    
    def test_calculate_asset_impact(self, sample_liquidity_data, sample_assumptions_data,
                                   date_columns, default_config):
        calculator = LiquidityCalculator(default_config)
        
        asset_impact = calculator._calculate_asset_impact(
            sample_liquidity_data,
            sample_assumptions_data,
            date_columns,
            '31-Dec-24'
        )
        
        assert isinstance(asset_impact, pl.DataFrame)
        assert len(asset_impact) == len(date_columns)
        assert 'Date' in asset_impact.columns
        assert 'Asset Impact' in asset_impact.columns
        
        for col in ['IWPB - Premier', 'UST - HTM', 'Illiquid', 'Liquid Trading Assets']:
            assert col in asset_impact.columns
    
    def test_calculate_liability_impact(self, sample_liquidity_data, sample_assumptions_data,
                                       date_columns, default_config):
        calculator = LiquidityCalculator(default_config)
        
        liability_impact = calculator._calculate_liability_impact(
            sample_liquidity_data,
            sample_assumptions_data,
            date_columns,
            '31-Dec-24'
        )
        
        assert isinstance(liability_impact, pl.DataFrame)
        assert len(liability_impact) == len(date_columns)
        assert 'Date' in liability_impact.columns
        assert 'Liability Impact' in liability_impact.columns
        
        for col in ['IWPB - Premier', 'Affiliate', 'Corp', 'Operational', 'Non-Operational']:
            assert col in liability_impact.columns
    
    def test_calculate_facility_impact(self, sample_liquidity_data, sample_assumptions_data,
                                      date_columns, default_config):
        calculator = LiquidityCalculator(default_config)
        
        facility_impact = calculator._calculate_facility_impact(
            sample_liquidity_data,
            sample_assumptions_data,
            date_columns,
            '31-Dec-24'
        )
        
        assert isinstance(facility_impact, pl.DataFrame)
        assert len(facility_impact) == len(date_columns)
        assert 'Date' in facility_impact.columns
        assert 'Comitted Facility Impact' in facility_impact.columns
        
        for col in ['Credit', 'Liquidity', 'Credit_Banks', 'Liquidity_Banks', 
                    'Credit_NBFI', 'Liquidity_NBFI']:
            assert col in facility_impact.columns
    
    def test_calculate_us_ilm_summary(self, date_columns, default_config):
        calculator = LiquidityCalculator(default_config)
        
        asset_data = {
            'Date': date_columns,
            'Asset Impact': [100.0, 110.0]
        }
        liability_data = {
            'Date': date_columns,
            'Liability Impact': [50.0, 55.0]
        }
        facility_data = {
            'Date': date_columns,
            'Comitted Facility Impact': [10.0, 11.0]
        }
        
        asset_df = pl.DataFrame(asset_data)
        liability_df = pl.DataFrame(liability_data)
        facility_df = pl.DataFrame(facility_data)
        
        summary = calculator._calculate_us_ilm_summary(
            asset_df, liability_df, facility_df, date_columns
        )
        
        assert isinstance(summary, pl.DataFrame)
        assert len(summary) == len(date_columns)
        assert 'Date' in summary.columns
        assert 'US ILM December' in summary.columns
        
        expected_cumulative = [160.0, 336.0]
        actual_values = summary['US ILM December'].to_list()
        assert actual_values == expected_cumulative

# tests/test_model.py
import pytest
import polars as pl
from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from src.gemini_liquidity_models.liquidity_impact_calculation.calculation_config import CalculationConfig

class TestLiquidityImpactCalculation:
    
    def test_initialization_with_default_config(self):
        model = LiquidityImpactCalculation()
        
        assert model.config is not None
        assert isinstance(model.config, CalculationConfig)
        assert model.calculator is not None
    
    def test_initialization_with_custom_config(self):
        custom_config = CalculationConfig(
            asset_row_mappings={'Test': [1, 2]},
            liability_row_mappings={'Test': [3, 4]},
            facility_row_mappings={'Test': [5]},
            assumption_mappings={'Test': 'Test'}
        )
        
        model = LiquidityImpactCalculation(config=custom_config)
        assert model.config == custom_config
    
    def test_calculate(self, sample_liquidity_data, sample_assumptions_data, date_columns):
        model = LiquidityImpactCalculation()
        
        results = model.calculate(
            liquidity_data=sample_liquidity_data,
            assumptions_data=sample_assumptions_data,
            date_columns=date_columns,
            base_date='31-Dec-24'
        )
        
        assert isinstance(results, dict)
        assert 'asset' in results
        assert 'liability' in results
        assert 'facility' in results
        assert 'summary' in results
        
        for key in results:
            assert isinstance(results[key], pl.DataFrame)
    
    def test_get_formatted_output(self, sample_liquidity_data, sample_assumptions_data, date_columns):
        model = LiquidityImpactCalculation()
        
        results = model.calculate(
            liquidity_data=sample_liquidity_data,
            assumptions_data=sample_assumptions_data,
            date_columns=date_columns,
            base_date='31-Dec-24'
        )
        
        formatted = model.get_formatted_output(results)
        
        assert isinstance(formatted, dict)
        assert 'Assets' in formatted
        assert 'Liabilities' in formatted
        assert 'Committed Facilities' in formatted
        assert 'US ILM Summary' in formatted
        
        assert formatted['Assets'].equals(results['asset'])
        assert formatted['Liabilities'].equals(results['liability'])
        assert formatted['Committed Facilities'].equals(results['facility'])
        assert formatted['US ILM Summary'].equals(results['summary'])

# tests/test_excel_utils.py
import pytest
import polars as pl
import tempfile
import os
from openpyxl import Workbook, load_workbook
from datetime import datetime
from utils.excel_utils import ExcelReader, ExcelWriter

class TestExcelReader:
    
    def test_read_sheet(self):
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp:
            wb = Workbook()
            ws = wb.active
            ws.title = "Test Sheet"
            
            ws['A2'] = 'Balance_sheet'
            ws['B2'] = 'Business'
            ws['C2'] = datetime(2025, 1, 7)
            ws['D2'] = datetime(2025, 1, 14)
            
            ws['A3'] = 'Asset'
            ws['B3'] = 'IWPB'
            ws['C3'] = 100.0
            ws['D3'] = 110.0
            
            ws['A4'] = 'Asset'
            ws['B4'] = 'CIB'
            ws['C4'] = 200.0
            ws['D4'] = 220.0
            
            wb.save(tmp.name)
            
            reader = ExcelReader()
            df, date_columns = reader.read_sheet(tmp.name, 'Test Sheet', header_row=2)
            
            assert isinstance(df, pl.DataFrame)
            assert len(df) == 2
            assert len(date_columns) == 2
            assert '07-Jan-25' in date_columns
            assert '14-Jan-25' in date_columns
            
            assert df['Balance_sheet'].to_list() == ['Asset', 'Asset']
            assert df['Business'].to_list() == ['IWPB', 'CIB']
            
            os.unlink(tmp.name)
    
    def test_read_assumptions(self):
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp:
            wb = Workbook()
            ws = wb.active
            ws.title = "Assumptions"
            
            ws['B2'] = 'Description'
            ws['C2'] = 'US_ILM'
            ws['D2'] = 'ILM'
            
            ws['B3'] = 'UST - HTM'
            ws['C3'] = '2.73%'
            ws['D3'] = None
            
            ws['B4'] = 'Level 1 - MBS - HTM'
            ws['C4'] = 0.018
            ws['D4'] = 0.0
            
            wb.save(tmp.name)
            
            reader = ExcelReader()
            df = reader.read_assumptions(tmp.name, 'Assumptions')
            
            assert isinstance(df, pl.DataFrame)
            assert len(df) == 2
            assert df['Description'].to_list() == ['UST - HTM', 'Level 1 - MBS - HTM']
            assert abs(df['US_ILM'][0] - 0.0273) < 0.0001
            assert df['US_ILM'][1] == 0.018
            
            os.unlink(tmp.name)

class TestExcelWriter:
    
    def test_write_output(self):
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp:
            wb = Workbook()
            wb.save(tmp.name)
            
            data_dict = {
                'Test Section': pl.DataFrame({
                    'Date': ['07-Jan-25', '14-Jan-25'],
                    'Item1': [100.0, 110.0],
                    'Item2': [200.0, 220.0],
                    'Total': [300.0, 330.0]
                })
            }
            
            date_columns = ['07-Jan-25', '14-Jan-25']
            bold_rows = ['Total']
            
            writer = ExcelWriter()
            writer.write_output(tmp.name, 'Output Sheet', data_dict, date_columns, bold_rows)
            
            wb = load_workbook(tmp.name)
            assert 'Output Sheet' in wb.sheetnames
            
            ws = wb['Output Sheet']
            assert ws['C2'].value == '07-Jan-25'
            assert ws['D2'].value == '14-Jan-25'
            assert ws['B3'].value == 'Item1'
            assert ws['B4'].value == 'Item2'
            assert ws['B5'].value == 'Total'
            
            os.unlink(tmp.name)

# tests/test_integration.py
import pytest
import polars as pl
from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation

class TestIntegration:
    
    def test_full_workflow(self, sample_liquidity_data, sample_assumptions_data, date_columns):
        model = LiquidityImpactCalculation()
        
        results = model.calculate(
            liquidity_data=sample_liquidity_data,
            assumptions_data=sample_assumptions_data,
            date_columns=date_columns,
            base_date='31-Dec-24'
        )
        
        formatted_output = model.get_formatted_output(results)
        
        assert len(formatted_output) == 4
        
        for section_name, section_df in formatted_output.items():
            assert isinstance(section_df, pl.DataFrame)
            assert len(section_df) > 0
            assert 'Date' in section_df.columns
        
        assert 'Asset Impact' in formatted_output['Assets'].columns
        assert 'Liability Impact' in formatted_output['Liabilities'].columns
        assert 'Comitted Facility Impact' in formatted_output['Committed Facilities'].columns
        assert 'US ILM December' in formatted_output['US ILM Summary'].columns
    
    def test_empty_data_handling(self):
        model = LiquidityImpactCalculation()
        
        empty_liquidity = pl.DataFrame({
            'Balance_sheet': [],
            'Business': [],
            'Level_1': [],
            'Level_2': [],
            'Level_3': [],
            '31-Dec-24': [],
            '07-Jan-25': []
        })
        
        empty_assumptions = pl.DataFrame({
            'Description': [],
            'US_ILM': [],
            'ILM': []
        })
        
        results = model.calculate(
            liquidity_data=empty_liquidity,
            assumptions_data=empty_assumptions,
            date_columns=['07-Jan-25'],
            base_date='31-Dec-24'
        )
        
        assert all(len(df) == 1 for df in results.values())
    
    def test_single_date_column(self, sample_liquidity_data, sample_assumptions_data):
        model = LiquidityImpactCalculation()
        
        results = model.calculate(
            liquidity_data=sample_liquidity_data,
            assumptions_data=sample_assumptions_data,
            date_columns=['07-Jan-25'],
            base_date='31-Dec-24'
        )
        
        assert all(len(df) == 1 for df in results.values())
        
        for key in ['asset', 'liability', 'facility', 'summary']:
            assert results[key]['Date'][0] == '07-Jan-25'
    
    def test_cumulative_summary(self, sample_liquidity_data, sample_assumptions_data, date_columns):
        model = LiquidityImpactCalculation()
        
        results = model.calculate(
            liquidity_data=sample_liquidity_data,
            assumptions_data=sample_assumptions_data,
            date_columns=date_columns,
            base_date='31-Dec-24'
        )
        
        summary = results['summary']
        
        for i in range(1, len(summary)):
            prev_val = summary['US ILM December'][i-1]
            curr_val = summary['US ILM December'][i]
            assert curr_val >= prev_val or curr_val <= prev_val

















