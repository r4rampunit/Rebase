Process 1: Ingestion
Purpose: Initial data intake into the system

Raw CORA data is collected and prepared for processing
This involves loading documents, files, or structured data from various sources
Data validation and initial formatting occurs here
Sets up the pipeline for subsequent processing steps

Process 2: Generate Embeddings
Purpose: Convert textual data into numerical vector representations

Text documents are processed through an embedding model (likely a transformer-based model)
Each piece of text is converted into high-dimensional vectors that capture semantic meaning
These embeddings enable mathematical similarity comparisons between different pieces of content
Chunking strategies may be applied to break large documents into manageable segments

Process 3: Save Embeddings
Purpose: Persist vector representations in a specialized database

Generated embeddings are stored in a Vector Database (Vector DB)
The database is optimized for similarity search operations
Metadata and original text references are typically stored alongside embeddings
Indexing structures are created to enable fast retrieval

Process 4: Query
Purpose: User initiates a search or question

User submits a natural language query through the interface
The query represents what information the user wants to retrieve
This triggers the retrieval process in the RAG system

Process 5: Search Relevant Information
Purpose: Find semantically similar content using vector similarity

User query is converted into an embedding using the same model from Process 2
Vector similarity search is performed against the stored embeddings
Top-k most relevant document chunks are retrieved based on similarity scores
Metadata and context from relevant documents are extracted

Process 6: Generated Prompt using Query + Top K Context
Purpose: Construct an enhanced prompt for the language model

Original user query is combined with the retrieved relevant context
A structured prompt is created that includes:

The user's original question
Relevant background information from retrieved documents
Instructions for how to use the context


This enriched prompt provides the LLM with domain-specific knowledge

Process 7: Return Formatted Response
Purpose: Generate final answer using retrieved context

ALICE (the LLM) processes the enhanced prompt
Generates a response that combines the retrieved information with its training knowledge
Response is formatted according to specified guidelines
May include citations or references to source documents

Process 8: Return Human-like Response
Purpose: Present final answer to the user

The generated response is delivered back to the user
Response appears natural and conversational while being grounded in the retrieved documents
User receives an answer that is both contextually relevant and factually supported by the CORA data







{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf4llm\n",
    "\n",
    "def read_pdf_and_chunk(folder_path, max_char_limit=1000, overlap_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Read PDF files from a folder and chunk them into smaller pieces.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing PDF files.\n",
    "    max_char_limit (int): Maximum number of characters per chunk.\n",
    "    overlap_percentage (float): Percentage of overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing chunked text and metadata.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for pdf_file in os.listdir(folder_path):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, pdf_file)\n",
    "            try:\n",
    "                # Extract markdown chunks from the PDF\n",
    "                page_data = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "\n",
    "                previous_chunk = None\n",
    "\n",
    "                for page_number, content in enumerate(page_data):\n",
    "                    text = content['text']\n",
    "                    metadata = {\n",
    "                        'page_number': page_number + 1,\n",
    "                        'toc_items': content.get('toc_items', []),\n",
    "                        'pdf_name': pdf_file\n",
    "                    }\n",
    "\n",
    "                    # Assign topic based on headers in the text\n",
    "                    metadata['topic'] = extract_topic(text)\n",
    "\n",
    "                    # Perform chunking based on headers and markdown formatting\n",
    "                    chunks = chunk_text(text, metadata, max_char_limit, overlap_percentage)\n",
    "                    \n",
    "                    # Handle header continuation across pages\n",
    "                    if page_number > 0 and is_header(content['text'].splitlines()[0]):\n",
    "                        if previous_chunk:\n",
    "                            # Merge previous chunk with the first chunk of the current page\n",
    "                            previous_chunk['text'] += \"\\n\" + chunks[0]['text']\n",
    "                            all_chunks[-1] = previous_chunk  # Update last chunk\n",
    "                            chunks.pop(0)  # Remove first chunk to avoid duplication\n",
    "\n",
    "                    all_chunks.extend(chunks)\n",
    "                    previous_chunk = chunks[-1] if chunks else None\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_file}: {e}\")\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "def extract_topic(text):\n",
    "    \"\"\"\n",
    "    Extract topic from headers in the text or fallback to first line or entire text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to extract the topic from.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted topic.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    # Check for markdown headers (ATX-style)\n",
    "    for line in lines:\n",
    "        if is_header(line):\n",
    "            return line.strip()  # Return the first header found as the topic\n",
    "\n",
    "    # Fallback to first line if no headers found\n",
    "    if lines and len(lines) > 0:\n",
    "        return lines[0].strip()  # Use the first line as the topic\n",
    "\n",
    "    return 'Unknown Topic'  # Default if no content is present\n",
    "\n",
    "def chunk_text(text, metadata, max_char_limit, overlap_percentage):\n",
    "    \"\"\"\n",
    "    Chunk the text based on headers and markdown formatting while respecting max character limits.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    metadata (dict): Metadata associated with the text.\n",
    "    max_char_limit (int): Maximum number of characters per chunk.\n",
    "    overlap_percentage (float): Percentage of overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing chunked text and metadata.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if the line is a header\n",
    "        if is_header(line):\n",
    "            # If there's already content in the current chunk, save it before starting a new one\n",
    "            if current_chunk:\n",
    "                chunk_text = '\\n'.join(current_chunk)\n",
    "                if len(chunk_text) > max_char_limit:\n",
    "                    # Break the chunk into smaller ones if it exceeds the limit\n",
    "                    sub_chunks = break_large_chunk(chunk_text, metadata, max_char_limit, overlap_percentage)\n",
    "                    chunks.extend(sub_chunks)\n",
    "                else:\n",
    "                    chunks.append({'text': chunk_text, **metadata})\n",
    "                current_chunk = []  # Reset current chunk\n",
    "\n",
    "        # Add the line to the current chunk\n",
    "        current_chunk.append(line)\n",
    "\n",
    "    # Add any remaining lines as the last chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = '\\n'.join(current_chunk)\n",
    "        if len(chunk_text) > max_char_limit:\n",
    "            # Break the last chunk into smaller ones if it exceeds the limit\n",
    "            sub_chunks = break_large_chunk(chunk_text, metadata, max_char_limit, overlap_percentage)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append({'text': chunk_text, **metadata})\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def break_large_chunk(chunk_text, metadata, max_char_limit, overlap_percentage):\n",
    "    \"\"\"\n",
    "    Break a large chunk into smaller ones with overlap while maintaining the topic.\n",
    "\n",
    "    Args:\n",
    "    chunk_text (str): The text to be broken into smaller chunks.\n",
    "    metadata (dict): Metadata associated with the text.\n",
    "    max_char_limit (int): Maximum number of characters per chunk.\n",
    "    overlap_percentage (float): Percentage of overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing chunked text and metadata.\n",
    "    \"\"\"\n",
    "    lines = chunk_text.splitlines()\n",
    "    sub_chunks = []\n",
    "    current_chunk = []\n",
    "    chunk_size = int(max_char_limit * (1 - overlap_percentage))\n",
    "\n",
    "    for line in lines:\n",
    "        # Add the line to the current sub-chunk\n",
    "        current_chunk.append(line)\n",
    "\n",
    "        # If the current sub-chunk exceeds the chunk size, save it and start a new one\n",
    "        if len('\\n'.join(current_chunk)) > chunk_size:\n",
    "            sub_chunks.append({'text': '\\n'.join(current_chunk), **metadata})\n",
    "            current_chunk = []\n",
    "\n",
    "    # Add any remaining lines as the last sub-chunk\n",
    "    if current_chunk:\n",
    "        sub_chunks.append({'text': '\\n'.join(current_chunk), **metadata})\n",
    "\n",
    "    return sub_chunks\n",
    "\n",
    "def is_header(line):\n",
    "    \"\"\"\n",
    "    Determine if a line is a header based on its formatting.\n",
    "\n",
    "    Args:\n",
    "    line (str): The line to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the line is a header, False otherwise.\n",
    "    \"\"\"\n",
    "    return line.strip() != \"\" and (line.isupper() or line.startswith(\"#\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embeddings of chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "def create_azure_openai_embeddings(chunks, openai_api_key: str, azure_endpoint: str, model: str):\n",
    "    \"\"\"\n",
    "    Created the vector embedding of the chunks.\n",
    "your -\n",
    "    Args:\n",
    "    chunks (list(str)): PDF chunks \n",
    "    openai_api_key (str): The OpenAI API key\n",
    "    azure_endpoint (str): The Azure OpenAI base url\n",
    "    model (str): model name\n",
    "\n",
    "    Retrurns:\n",
    "    embeddings (instance/function): Gives a function to create embeddings\n",
    "    embedded_texts (list(int)): List of Text embedding of the chunk passed\n",
    "    \"\"\"\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_key=\"your-api-key-key\",\n",
    "        azure_endpoint=\"your-endpoint\",\n",
    "        model=\"text-embedding-ada-002\",  # Specify the model directly,\n",
    "        chunk_size=5\n",
    "    )\n",
    "\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embedded_texts = embeddings.embed_documents(texts)\n",
    "\n",
    "    return embeddings, embedded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_openai_embeddings_instance(openai_api_key: str, azure_endpoint: str, model: str):\n",
    "    \"\"\"\n",
    "    Creates Azure OpenAI Embeddings Instance\n",
    "\n",
    "    Args: \n",
    "    openai_api_key (str): The OpenAI API key\n",
    "    azure_endpoint (str): The Azure OpenAI base url\n",
    "    model (str): model name\n",
    "\n",
    "    Retrurns:\n",
    "    embeddings (instance/function): Gives a function to create embeddings\n",
    "    \"\"\"\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_key=\"your-api-key-here\",\n",
    "        azure_endpoint=\"your-azure-endpoint-here\",\n",
    "        model=\"text-embedding-ada-002\",  # Specify the model directly,\n",
    "        chunk_size=5\n",
    "    )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anand\\rsystems_projects\\seapeak\\seapeak_env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Anand\\rsystems_projects\\seapeak\\seapeak_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[77.3481, 71.5315]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"msmarco-distilbert-dot-v5\")\n",
    "\n",
    "query_embedding = model.encode(\"How big is London\")\n",
    "passage_embedding = model.encode([\n",
    "    \"London has 9,787,426 inhabitants at the 2011 census\",\n",
    "    \"London is known for its financial district\",\n",
    "])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))\n",
    "\n",
    "# def create_masmarco_embeddings(chunks):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def documents_and_embedding_prep(chunks, embedded_texts, index_name):\n",
    "    \"\"\"\n",
    "    Cretes Documents and Vector Embeddings.\n",
    "\n",
    "    Args:\n",
    "    chunks (list):\n",
    "    embedded_texts (list(float)):\n",
    "    index_name (str): The Pinecone Index name.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    vectors = []\n",
    "    idx= 0\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embedded_texts)):\n",
    "        idx+= 1\n",
    "\n",
    "        if 'text' not in chunk or not chunk['text']:\n",
    "            print((f\"Skipping Chunk {idx} because it has no text.\"))\n",
    "            continue \n",
    "        \n",
    "        chunk_content = f\"----- source: {chunk['pdf_name']}, page_number: {chunk['page_number']}, topic: {chunk['topic']} -----\\n{chunk['text']}\\n-----\"\n",
    "        document = Document(\n",
    "            page_content=chunk_content,\n",
    "            metadata={\n",
    "                \"chunk-id\": idx,\n",
    "                \"source\": chunk[\"pdf_name\"],\n",
    "                \"page\": chunk[\"page_number\"],\n",
    "                \"index\": index_name,\n",
    "                \"topic\": chunk[\"topic\"],\n",
    "                \"text\": chunk_content\n",
    "            }\n",
    "        )\n",
    "        documents.append(document)\n",
    "        vectors.append((str(i), embedding, {\n",
    "                \"chunk-id\": idx,\n",
    "                \"source\": chunk[\"pdf_name\"],\n",
    "                \"page\": chunk[\"page_number\"],\n",
    "                \"index\": index_name,\n",
    "                \"topic\": chunk[\"topic\"],\n",
    "                \"text\": chunk_content\n",
    "            }))\n",
    "    \n",
    "    return documents, vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "def initialize_pinecone(api_key: str, \n",
    "                        # environment: str\n",
    "                        ) -> Pinecone:\n",
    "    \"\"\"\n",
    "    Initialize the Pinecone client.\n",
    "\n",
    "    Args:\n",
    "    api_key (str): The Pinecone API key.\n",
    "    environment (str): The Pinecone environment.\n",
    "\n",
    "    Returns:\n",
    "    Pinecone: An initialized Pinecone client.\n",
    "    \"\"\"\n",
    "    os.environ[\"PINECONE_API_KEY\"] = api_key\n",
    "    # os.environ[\"PINECONE_ENVIRONMENT\"] = environment\n",
    "    return Pinecone(api_key=api_key)\n",
    "\n",
    "def create_index(\n",
    "    pc: Pinecone,\n",
    "    index_name: str,\n",
    "    dimension: int = 1536,\n",
    "    metric: str = 'cosine',\n",
    "    serverless_spec= ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        ) # type: ignore\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Create a new Pinecone index if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "    pc (Pinecone): An initialized Pinecone client.\n",
    "    index_name (str): The name of the index to create.\n",
    "    dimension (int): The dimension of the vectors to be stored in the index.\n",
    "    metric (str): The distance metric to use for similarity search. Default is 'cosine'.\n",
    "    serverless_spec (ServerlessSpec): Specification for serverless deployment. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the index was created or already exists, False otherwise.\n",
    "    \"\"\"\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        try:\n",
    "            if serverless_spec:\n",
    "                pc.create_index(\n",
    "                    name=index_name,\n",
    "                    dimension=dimension,\n",
    "                    metric=metric,\n",
    "                    spec=serverless_spec\n",
    "                )\n",
    "            else:\n",
    "                pc.create_index(\n",
    "                    name=index_name,\n",
    "                    dimension=dimension,\n",
    "                    metric=metric\n",
    "                )\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating index '{index_name}': {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "        return True\n",
    "\n",
    "def upsert_vectors(\n",
    "    index: Any,\n",
    "    vectors: List[Dict[str, Any]],\n",
    "    batch_size: int = 100\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Upsert vectors into the Pinecone index in batches.\n",
    "\n",
    "    Args:\n",
    "    index (Any): The Pinecone index object.\n",
    "    vectors (List[Dict[str, Any]]): A list of dictionaries containing vector data.\n",
    "                                    Each dictionary should have 'id', 'values', and 'metadata' keys.\n",
    "    batch_size (int): The number of vectors to upsert in each batch. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all vectors were upserted successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for i in range(0, len(vectors), batch_size):\n",
    "            batch = vectors[i:i + batch_size]\n",
    "            index.upsert(vectors=batch)\n",
    "            print(f\"Upserted batch {i // batch_size + 1} with {len(batch)} vectors.\")\n",
    "        print(f\"Successfully upserted {len(vectors)} vectors to the index.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vectors: {e}\")\n",
    "        return False\n",
    "\n",
    "def query_index(\n",
    "    index: Any,\n",
    "    query_vector: List[float],\n",
    "    top_k: int = 10,\n",
    "    include_metadata: bool = True\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Query the Pinecone index with a vector.\n",
    "\n",
    "    Args:\n",
    "    index (Any): The Pinecone index object.\n",
    "    query_vector (List[float]): The query vector.\n",
    "    top_k (int): The number of most similar vectors to return. Default is 10.\n",
    "    include_metadata (bool): Whether to include metadata in the results. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict[str, Any]]: A list of dictionaries containing the query results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = index.query(\n",
    "            vector=query_vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=include_metadata\n",
    "        )\n",
    "        return results['matches']\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying index: {e}\")\n",
    "        return []\n",
    "\n",
    "def delete_vectors(index: Any, ids: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Delete vectors from the Pinecone index by their IDs.\n",
    "\n",
    "    Args:\n",
    "    index (Any): The Pinecone index object.\n",
    "    ids (List[str]): A list of vector IDs to delete.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the vectors were deleted successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index.delete(ids=ids)\n",
    "        print(f\"Successfully deleted {len(ids)} vectors from the index.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting vectors: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_index_stats(index: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get statistics about the Pinecone index.\n",
    "\n",
    "    Args:\n",
    "    index (Any): The Pinecone index object.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing index statistics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return index.describe_index_stats()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting index stats: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Chaining Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def rag_chaining(Question: str, index_name: str, embeddings, retriever_type: str, BASE_URL: str, version: str, API_KEY: str):\n",
    "    # Set up OpenAI credentials\n",
    "    openai.api_type = \"azure\"\n",
    "    openai.api_base = BASE_URL\n",
    "    openai.api_version = version\n",
    "    openai.api_key = API_KEY\n",
    "    \n",
    "    # Initialize LangChain's Pinecone vector store\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "    \n",
    "    # Create a retriever from the vector store that uses similarity search with a score threshold.\n",
    "    retriever_similarity = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 15})\n",
    "    retriever_sst= vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 15, \"score_threshold\": 0.8})\n",
    "    retriever_mmr= vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 15, \"score_threshold\": 0.8})\n",
    "    \n",
    "    if (retriever_type== \"similarity\"):\n",
    "        retriever= retriever_similarity\n",
    "    elif (retriever_type== \"sst\"):\n",
    "        retriever= retriever_sst\n",
    "    elif (retriever_type== \"mmr\"):\n",
    "        retriever= retriever_mmr\n",
    "    else:\n",
    "        print(\"Error set retriver type as 'similarity' or 'sst' or 'mmr' only.\")\n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize memory for conversational context.\n",
    "    memory = []\n",
    "    \n",
    "    # Define the Azure OpenAI model for generating responses\n",
    "    model = AzureChatOpenAI(deployment_name=\"gpt-35-turbo-16k\",api_key= API_KEY, api_version= version, azure_endpoint= BASE_URL ,temperature=0.5)\n",
    "    \n",
    "    # Define standalone prompt (for condensing questions)\n",
    "    standalone_prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a helpful assistant. Please condense this question: {question}\"\n",
    "    )\n",
    "    \n",
    "    # Define answer prompt (for generating answers)\n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Based on the following context, answer the question:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    def qna(query, retriever=retriever, memory= memory):\n",
    "        # global memory  # Use the global memory variable to maintain conversation history.\n",
    "        \n",
    "        # Create a ConversationalRetrievalChain that retrieves documents based on the query and generates a response.\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            condense_question_prompt=standalone_prompt,\n",
    "            llm=model,\n",
    "            retriever=retriever,\n",
    "            chain_type=\"stuff\",\n",
    "            combine_docs_chain_kwargs={\"prompt\": answer_prompt},\n",
    "            return_source_documents=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # print(chain)\n",
    "        # Invoke the chain with the recent chat history and the current question.\n",
    "        result = chain.invoke({\"chat_history\": memory[-3:], \"question\": query})\n",
    "        # print(f\"Result: {result}\")\n",
    "        \n",
    "        retrived_docs= result['source_documents']\n",
    "        # if retrived_docs:\n",
    "        #     for doc in retrived_docs:\n",
    "        #         print(f\"Document:  {doc.page_content}\")\n",
    "        #         print(f\"Metadata: {doc.metadata}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    result_qna= qna(Question, retriever, memory= memory)\n",
    "    # print(f\"{retriever_type}\\t {result_qna}\\n\")\n",
    "\n",
    "    return result_qna \n",
    " \n",
    "# Example query to generate a summary.\n",
    "# Q = \"What are different Stages of Listening?\"\n",
    "# result_qna = qna(Q)\n",
    " \n",
    "# print(f\"similarity {result_qna}\\n\")\n",
    "# print(f\"sst {qna(Q, retriever_sst)}\\n\")\n",
    "# print(f\"mmr {qna(Q, retriever_mmr)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data_Corpous\\0000814375-24-000012 (1).pdf...\n",
      "[                                        ] (0/4=========="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b[==========                              ] (1/4=========[====================                    ] (2/4=========[==============================          ] (3/4=========[========================================] (4/4]\n",
      "Processing Data_Corpous\\0000814375-24-000012.pdf...\n",
      "[                                        ] (0/4=========[==========                              ] (1/4=========[====================                    ] (2/4=========[==============================          ] (3/4=========[========================================] (4/4]\n",
      "Processing Data_Corpous\\0001193125-23-210508.pdf...\n",
      "[                                        ] (0/4=========[==========                              ] (1/4=========[====================                    ] (2/4=========[==============================          ] (3/4=========[========================================] (4/4]\n",
      "Processing Data_Corpous\\0001193125-23-241208.pdf...\n",
      "[                                        ] (0/68[                                        ] ( 1/68[=                                       ] ( 2/68[=                                       ] ( 3/6[==                                      ] ( 4/6[==                                      ] ( 5/68[===                                     ] ( 6/6[====                                    ] ( 7/6[====                                    ] ( 8/68[=====                                   ] ( 9/68[=====                                   ] (10/6[======                                  ] (11/68[=======                                 ] (12/68[=======                                 ] (13/6[========                                ] (14/6[========                                ] (15/68[=========                               ] (16/6[==========                              ] (17/6[==========                              ] (18/68[===========                             ] (19/[===========                             ] (20/6[============                            ] (21/6[============                            ] (22/68[=============                           ] (23/6[==============                          ] (24/6[==============                          ] (25/68[===============                         ] (26/[===============                         ] (27/6[================                        ] (28/68[=================                       ] (29/68[=================                       ] (30/6[==================                      ] (31/6[==================                      ] (32/68[===================                     ] (33/6[====================                    ] (34/6[====================                    ] (35/68[=====================                   ] (36/[=====================                   ] (37/6[======================                  ] (38/6[======================                  ] (39/68[=======================                 ] (40/6[========================                ] (41/6[========================                ] (42/68[=========================               ] (43/[=========================               ] (44/6[==========================              ] (45/68[===========================             ] (46/68[===========================             ] (47/6[============================            ] (48/6[============================            ] (49/68[=============================           ] (50/6[==============================          ] (51/6[==============================          ] (52/68[===============================         ] (53/68[===============================         ] (54/6[================================        ] (55/6[================================        ] (56/68[=================================       ] (57/6[==================================      ] (58/6[==================================      ] (59/68[===================================     ] (60/[===================================     ] (61/6[====================================    ] (62/68[=====================================   ] (63/[=====================================   ] (64/6[======================================  ] (65/6[======================================  ] (66/68[======================================= ] (67/6[========================================] (68/68]\n",
      "Processing Data_Corpous\\0001193125-23-246915.pdf...\n",
      "[                                        ] (0/5=======[========                                ] (1/5=======[================                        ] (2/5=======[========================                ] (3/5=======[================================        ] (4/5=======[========================================] (5/5]\n",
      "Processing Data_Corpous\\0001193125-23-247065.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
      "Processing Data_Corpous\\0001193125-23-247204 (1).pdf...\n",
      "[                                        ] (0/8====[=====                                   ] (1/====[==========                              ] (2/8====[===============                         ] (3/====[====================                    ] (4/8====[=========================               ] (5/====[==============================          ] (6/8====[===================================     ] (7/====[========================================] (8/8]\n",
      "Processing Data_Corpous\\0001193125-23-247204.pdf...\n",
      "[                                        ] (0/8====[=====                                   ] (1/====[==========                              ] (2/8====[===============                         ] (3/====[====================                    ] (4/8====[=========================               ] (5/====[==============================          ] (6/8====[===================================     ] (7/====[========================================] (8/8]\n",
      "Processing Data_Corpous\\0001193125-23-248965.pdf...\n",
      "[                                        ] (0/5=======[========                                ] (1/5=======[================                        ] (2/5=======[========================                ] (3/5=======[================================        ] (4/5=======[========================================] (5/5]\n",
      "Processing Data_Corpous\\0001193125-23-252236.pdf...\n",
      "[                                        ] (0/9===[====                                    ] (1/9===[========                                ] (2/9====[=============                           ] (3/===[=================                       ] (4/====[======================                  ] (5/9===[==========================              ] (6/9====[===============================         ] (7/===[===================================     ] (8/====[========================================] (9/9]\n",
      "Processing Data_Corpous\\0001193125-23-264697.pdf...\n",
      "[                                        ] (0/5=======[========                                ] (1/5=======[================                        ] (2/5=======[========================                ] (3/5=======[================================        ] (4/5=======[========================================] (5/5]\n",
      "Processing Data_Corpous\\0001193125-23-275459.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2[==                                      ] ( 2/28=[====                                    ] ( 3/28[=====                                   ] ( 4/2=[=======                                 ] ( 5/2[========                                ] ( 6/28=[==========                              ] ( 7/28[===========                             ] ( 8/2[============                            ] ( 9/28=[==============                          ] (10/28[===============                         ] (11/2=[=================                       ] (12/2[==================                      ] (13/28=[====================                    ] (14/28[=====================                   ] (15/2[======================                  ] (16/28=[========================                ] (17/28[=========================               ] (18/2=[===========================             ] (19/2[============================            ] (20/28=[==============================          ] (21/28[===============================         ] (22/2[================================        ] (23/28=[==================================      ] (24/28[===================================     ] (25/2=[=====================================   ] (26/2[======================================  ] (27/28=[========================================] (28/28]\n",
      "Processing Data_Corpous\\0001193125-23-275963.pdf...\n",
      "[                                        ] (0/30[                                        ] (  1/30[                                        ] (  2/30[                                        ] (  3/30[                                        ] (  4/30[                                        ] (  5/30[                                        ] (  6/30[                                        ] (  7/306[=                                       ] (  8/3[=                                       ] (  9/306[=                                       ] ( 10/306[=                                       ] ( 11/306[=                                       ] ( 12/306[=                                       ] ( 13/306[=                                       ] ( 14/306[=                                       ] ( 15/30[==                                      ] ( 16/30[==                                      ] ( 17/30[==                                      ] ( 18/30[==                                      ] ( 19/30[==                                      ] ( 20/30[==                                      ] ( 21/30[==                                      ] ( 22/306[===                                     ] ( 23/3[===                                     ] ( 24/3[===                                     ] ( 25/3[===                                     ] ( 26/306[===                                     ] ( 27/3[===                                     ] ( 28/3[===                                     ] ( 29/306[===                                     ] ( 30/30[====                                    ] ( 31/30[====                                    ] ( 32/30[====                                    ] ( 33/30[====                                    ] ( 34/30[====                                    ] ( 35/30[====                                    ] ( 36/30[====                                    ] ( 37/30[====                                    ] ( 38/306[=====                                   ] ( 39/306[=====                                   ] ( 40/306[=====                                   ] ( 41/3[=====                                   ] ( 42/3[=====                                   ] ( 43/3[=====                                   ] ( 44/3[=====                                   ] ( 45/30[======                                  ] ( 46/30[======                                  ] ( 47/30[======                                  ] ( 48/30[======                                  ] ( 49/30[======                                  ] ( 50/30[======                                  ] ( 51/30[======                                  ] ( 52/30[======                                  ] ( 53/306[=======                                 ] ( 54/3[=======                                 ] ( 55/3[=======                                 ] ( 56/306[=======                                 ] ( 57/3[=======                                 ] ( 58/3[=======                                 ] ( 59/306[=======                                 ] ( 60/306[=======                                 ] ( 61/30[========                                ] ( 62/30[========                                ] ( 63/30[========                                ] ( 64/30[========                                ] ( 65/30[========                                ] ( 66/30[========                                ] ( 67/30[========                                ] ( 68/306[=========                               ] ( 69/306[=========                               ] ( 70/306[=========                               ] ( 71/306[=========                               ] ( 72/3[=========                               ] ( 73/3[=========                               ] ( 74/3[=========                               ] ( 75/3[=========                               ] ( 76/30[==========                              ] ( 77/30[==========                              ] ( 78/30[==========                              ] ( 79/30[==========                              ] ( 80/30[==========                              ] ( 81/30[==========                              ] ( 82/30[==========                              ] ( 83/30[==========                              ] ( 84/306[===========                             ] ( 85/3[===========                             ] ( 86/306[===========                             ] ( 87/306[===========                             ] ( 88/306[===========                             ] ( 89/3[===========                             ] ( 90/3[===========                             ] ( 91/30[============                            ] ( 92/30[============                            ] ( 93/30[============                            ] ( 94/30[============                            ] ( 95/30[============                            ] ( 96/30[============                            ] ( 97/30[============                            ] ( 98/30[============                            ] ( 99/306[=============                           ] (100/3[=============                           ] (101/306[=============                           ] (102/3[=============                           ] (103/3[=============                           ] (104/3[=============                           ] (105/306[=============                           ] (106/306[=============                           ] (107/30[==============                          ] (108/30[==============                          ] (109/30[==============                          ] (110/30[==============                          ] (111/30[==============                          ] (112/30[==============                          ] (113/30[==============                          ] (114/306[===============                         ] (115/3[===============                         ] (116/3[===============                         ] (117/306[===============                         ] (118/3[===============                         ] (119/3[===============                         ] (120/306[===============                         ] (121/3[===============                         ] (122/30[================                        ] (123/30[================                        ] (124/30[================                        ] (125/30[================                        ] (126/30[================                        ] (127/30[================                        ] (128/30[================                        ] (129/30[================                        ] (130/306[=================                       ] (131/3[=================                       ] (132/3[=================                       ] (133/306[=================                       ] (134/3[=================                       ] (135/3[=================                       ] (136/3[=================                       ] (137/30[==================                      ] (138/30[==================                      ] (139/30[==================                      ] (140/30[==================                      ] (141/30[==================                      ] (142/30[==================                      ] (143/30[==================                      ] (144/30[==================                      ] (145/306[===================                     ] (146/306[===================                     ] (147/306[===================                     ] (148/3[===================                     ] (149/306[===================                     ] (150/3[===================                     ] (151/3[===================                     ] (152/30[====================                    ] (153/30[====================                    ] (154/30[====================                    ] (155/30[====================                    ] (156/30[====================                    ] (157/30[====================                    ] (158/30[====================                    ] (159/30[====================                    ] (160/306[=====================                   ] (161/3[=====================                   ] (162/306[=====================                   ] (163/306[=====================                   ] (164/306[=====================                   ] (165/306[=====================                   ] (166/306[=====================                   ] (167/3[=====================                   ] (168/30[======================                  ] (169/30[======================                  ] (170/30[======================                  ] (171/30[======================                  ] (172/30[======================                  ] (173/30[======================                  ] (174/30[======================                  ] (175/306[=======================                 ] (176/306[=======================                 ] (177/306[=======================                 ] (178/306[=======================                 ] (179/3[=======================                 ] (180/306[=======================                 ] (181/306[=======================                 ] (182/306[=======================                 ] (183/30[========================                ] (184/30[========================                ] (185/30[========================                ] (186/30[========================                ] (187/30[========================                ] (188/30[========================                ] (189/30[========================                ] (190/30[========================                ] (191/306[=========================               ] (192/3[=========================               ] (193/306[=========================               ] (194/306[=========================               ] (195/3[=========================               ] (196/306[=========================               ] (197/3[=========================               ] (198/30[==========================              ] (199/30[==========================              ] (200/30[==========================              ] (201/30[==========================              ] (202/30[==========================              ] (203/30[==========================              ] (204/30[==========================              ] (205/30[==========================              ] (206/306[===========================             ] (207/3[===========================             ] (208/3[===========================             ] (209/306[===========================             ] (210/3[===========================             ] (211/3[===========================             ] (212/306[===========================             ] (213/3[===========================             ] (214/30[============================            ] (215/30[============================            ] (216/30[============================            ] (217/30[============================            ] (218/30[============================            ] (219/30[============================            ] (220/30[============================            ] (221/306[=============================           ] (222/306[=============================           ] (223/306[=============================           ] (224/3[=============================           ] (225/3[=============================           ] (226/3[=============================           ] (227/3[=============================           ] (228/306[=============================           ] (229/30[==============================          ] (230/30[==============================          ] (231/30[==============================          ] (232/30[==============================          ] (233/30[==============================          ] (234/30[==============================          ] (235/30[==============================          ] (236/30[==============================          ] (237/306[===============================         ] (238/306[===============================         ] (239/306[===============================         ] (240/306[===============================         ] (241/306[===============================         ] (242/306[===============================         ] (243/3[===============================         ] (244/30[================================        ] (245/30[================================        ] (246/30[================================        ] (247/30[================================        ] (248/30[================================        ] (249/30[================================        ] (250/30[================================        ] (251/30[================================        ] (252/306[=================================       ] (253/3[=================================       ] (254/306[=================================       ] (255/3[=================================       ] (256/306[=================================       ] (257/306[=================================       ] (258/3[=================================       ] (259/306[=================================       ] (260/30[==================================      ] (261/30[==================================      ] (262/30[==================================      ] (263/30[==================================      ] (264/30[==================================      ] (265/30[==================================      ] (266/30[==================================      ] (267/306[===================================     ] (268/3[===================================     ] (269/3[===================================     ] (270/3[===================================     ] (271/306[===================================     ] (272/3[===================================     ] (273/306[===================================     ] (274/3[===================================     ] (275/30[====================================    ] (276/30[====================================    ] (277/30[====================================    ] (278/30[====================================    ] (279/30[====================================    ] (280/30[====================================    ] (281/30[====================================    ] (282/30[====================================    ] (283/306[=====================================   ] (284/306[=====================================   ] (285/3[=====================================   ] (286/3[=====================================   ] (287/3[=====================================   ] (288/3[=====================================   ] (289/3[=====================================   ] (290/30[======================================  ] (291/30[======================================  ] (292/30[======================================  ] (293/30[======================================  ] (294/30[======================================  ] (295/30[======================================  ] (296/30[======================================  ] (297/30[======================================  ] (298/306[======================================= ] (299/3[======================================= ] (300/306[======================================= ] (301/306[======================================= ] (302/3[======================================= ] (303/3[======================================= ] (304/306[======================================= ] (305/30[========================================] (306/306]\n",