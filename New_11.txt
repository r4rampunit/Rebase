import pandas as pd
import numpy as np
import sqlite3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import nltk

nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
    if not isinstance(text, str):
        return ''
    
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
    
    return ' '.join(tokens)

def generate_column_name(description):
    preprocessed_desc = preprocess_text(description)
    
    words = preprocessed_desc.split()
    
    if len(words) == 0:
        return 'unknown_column'
    
    if len(words) <= 3:
        return '_'.join(words)
    
    vectorizer = TfidfVectorizer(stop_words='english', max_features=5)
    tfidf_matrix = vectorizer.fit_transform([preprocessed_desc])
    
    feature_names = vectorizer.get_feature_names_out()
    tfidf_scores = tfidf_matrix.toarray()[0]
    
    top_words = [feature_names[idx] for idx in tfidf_scores.argsort()[-3:][::-1]]
    
    column_name = '_'.join(top_words)
    column_name = re.sub(r'[^a-z0-9_]', '', column_name)
    
    return column_name

def rename_columns_in_database(input_db_path, output_db_path, mapping_excel_path):
    mapping_df = pd.read_excel(mapping_excel_path)
    
    conn_input = sqlite3.connect(input_db_path)
    conn_output = sqlite3.connect(output_db_path)
    
    table_names = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn_input)['name']
    
    for table_name in table_names:
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn_input)
        
        column_mapping = {}
        for column in df.columns:
            match = mapping_df[mapping_df['MDRM'] == column]
            
            if not match.empty:
                description = match['Line Item Description'].values[0]
                new_column_name = generate_column_name(description)
                column_mapping[column] = new_column_name
            else:
                column_mapping[column] = column
        
        df.rename(columns=column_mapping, inplace=True)
        df.to_sql(table_name, conn_output, if_exists='replace', index=False)
    
    conn_input.close()
    conn_output.close()

rename_columns_in_database(
    input_db_path=self_path, 
    output_db_path='mirror_layer.sqlite', 
    mapping_excel_path='your_mapping_excel.xlsx'
)



import pandas as pd
import sqlite3
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
   if not isinstance(text, str):
       return ''
   
   text = text.lower()
   text = re.sub(r'[^a-zA-Z\s]', '', text)
   
   tokens = word_tokenize(text)
   stop_words = set(stopwords.words('english'))
   tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
   
   return ' '.join(tokens)

def generate_column_name(description):
   preprocessed_desc = preprocess_text(description)
   
   words = preprocessed_desc.split()
   
   if len(words) == 0:
       return 'unknown_column'
   
   if len(words) <= 3:
       return '_'.join(words)
   
   vectorizer = TfidfVectorizer(stop_words='english', max_features=5)
   tfidf_matrix = vectorizer.fit_transform([preprocessed_desc])
   
   feature_names = vectorizer.get_feature_names_out()
   tfidf_scores = tfidf_matrix.toarray()[0]
   
   top_words = [feature_names[idx] for idx in tfidf_scores.argsort()[-3:][::-1]]
   
   column_name = '_'.join(top_words)
   column_name = re.sub(r'[^a-z0-9_]', '', column_name)
   
   return column_name

def create_metadata_table(input_db_path, mapping_excel_path, metadata_db_path):
   mapping_df = pd.read_excel(mapping_excel_path)
   
   conn_input = sqlite3.connect(input_db_path)
   conn_metadata = sqlite3.connect(metadata_db_path)
   
   table_names = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn_input)['name']
   
   metadata_records = []
   
   for table_name in table_names:
       df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn_input)
       
       for column in df.columns:
           match = mapping_df[mapping_df['MDRM'] == column]
           
           if not match.empty:
               description = match['Line Item Description'].values[0]
               business_name = generate_column_name(description)
               
               metadata_record = {
                   'Encrypted_Name': column,
                   'Business_Name': business_name,
                   'Description': description,
                   'Include_in_Mirror_Layer': 'Yes'  
               }
               metadata_records.append(metadata_record)
           else:
               metadata_record = {
                   'Encrypted_Name': column,
                   'Business_Name': column,
                   'Description': 'No description available',
                   'Include_in_Mirror_Layer': 'No'
               }
               metadata_records.append(metadata_record)
   
   metadata_df = pd.DataFrame(metadata_records)
   
   metadata_df.to_sql('Metadata', conn_metadata, if_exists='replace', index=False)
   
   conn_input.close()
   conn_metadata.close()

def extract_mirror_layer_columns(input_db_path, metadata_db_path, output_db_path):
   conn_input = sqlite3.connect(input_db_path)
   conn_metadata = sqlite3.connect(metadata_db_path)
   conn_output = sqlite3.connect(output_db_path)
   
   metadata_df = pd.read_sql_query("SELECT * FROM Metadata WHERE Include_in_Mirror_Layer = 'Yes'", conn_metadata)
   
   table_names = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn_input)['name']
   
   for table_name in table_names:
       df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn_input)
       
       columns_to_include = metadata_df['Encrypted_Name'].tolist()
       columns_mapping = dict(zip(metadata_df['Encrypted_Name'], metadata_df['Business_Name']))
       
       filtered_df = df[columns_to_include]
       filtered_df.rename(columns=columns_mapping, inplace=True)
       
       filtered_df.to_sql(table_name, conn_output, if_exists='replace', index=False)
   
   conn_input.close()
   conn_metadata.close()
   conn_output.close()

create_metadata_table(
   input_db_path=self_path, 
   mapping_excel_path='your_mapping_excel.xlsx', 
   metadata_db_path='metadata.sqlite'
)

extract_mirror_layer_columns(
   input_db_path=self_path,
   metadata_db_path='metadata.sqlite',
   output_db_path='mirror_layer.sqlite'
)