# ====================================================================================
# FILE: Utils/data_import_helpers.py
# ====================================================================================
import polars as pl
from pathlib import Path
from typing import Optional

class DataImportHelper:
    def __init__(self, input_dir: str = "Input"):
        self.input_dir = Path(input_dir)
    
    def import_excel(self, filename: str, sheet_name: Optional[str] = None) -> pl.DataFrame:
        """Import Excel file and auto-convert date columns"""
        file_path = self.input_dir / filename
        df = pl.read_excel(file_path, sheet_name=sheet_name)
        
        # Auto-detect and convert date columns from string format
        for col in df.columns:
            if col.lower() in ['date', 'f_date']:
                if df[col].dtype == pl.Utf8:
                    try:
                        # Try M/D/YYYY format first
                        df = df.with_columns([
                            pl.col(col).str.to_date("%m/%d/%Y").alias(col)
                        ])
                    except:
                        try:
                            # Try YYYY-MM-DD format
                            df = df.with_columns([
                                pl.col(col).str.to_date("%Y-%m-%d").alias(col)
                            ])
                        except:
                            pass
        
        return df
    
    def export_csv(self, data: pl.DataFrame, filename: str, output_dir: str = "Output") -> None:
        output_path = Path(output_dir) / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        data.write_csv(output_path)
    
    def export_excel(self, data: pl.DataFrame, filename: str, sheet_name: str = "Sheet1", output_dir: str = "Output") -> None:
        output_path = Path(output_dir) / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        data.write_excel(output_path, worksheet=sheet_name)


# ====================================================================================
# FILE: Utils/glm_model_architecture.py
# ====================================================================================
import polars as pl
import numpy as np
from sklearn.linear_model import LinearRegression
from typing import List
import pandas as pd

class HPIGLMModel:
    """GLM Model for HPI Projection matching SAS proc glmselect logic"""
    
    def __init__(self, target_col: str, categorical_cols: List[str], continuous_cols: List[str]):
        self.model = LinearRegression(fit_intercept=False)  # noint in SAS
        self.target_col = target_col
        self.categorical_cols = categorical_cols
        self.continuous_cols = continuous_cols
        self.is_fitted = False
        self.fitted_categories = {}
    
    def prepare_features(self, data: pl.DataFrame) -> np.ndarray:
        """Prepare features: categorical (one-hot) + continuous"""
        features = []
        
        # Handle categorical variables (like SAS class statement)
        for cat_col in self.categorical_cols:
            if not self.is_fitted:
                unique_vals = data[cat_col].unique().to_list()
                self.fitted_categories[cat_col] = unique_vals
            
            cat_data = data.select(cat_col).to_pandas()
            encoded = pd.get_dummies(cat_data, drop_first=True, dtype=float)
            features.append(encoded.values)
        
        # Handle continuous variables
        for cont_col in self.continuous_cols:
            cont_data = data.select(cont_col).to_numpy().reshape(-1, 1)
            features.append(cont_data)
        
        if features:
            return np.hstack(features)
        else:
            return np.array([]).reshape(len(data), 0)
    
    def prepare_target(self, data: pl.DataFrame) -> np.ndarray:
        return data.select(self.target_col).to_numpy().ravel()
    
    def fit(self, train_data: pl.DataFrame):
        """Fit the GLM model (equivalent to SAS proc glmselect)"""
        X = self.prepare_features(train_data)
        y = self.prepare_target(train_data)
        
        if X.shape[0] == 0:
            raise ValueError("Training data is empty after removing nulls")
        
        self.model.fit(X, y)
        self.is_fitted = True
        return self
    
    def predict(self, data: pl.DataFrame) -> np.ndarray:
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        X = self.prepare_features(data)
        return self.model.predict(X)
    
    def score_data(self, data: pl.DataFrame, prediction_col: str = "Pred") -> pl.DataFrame:
        """Score data (equivalent to SAS score statement)"""
        predictions = self.predict(data)
        return data.with_columns(pl.Series(name=prediction_col, values=predictions))


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/_constants.py
# ====================================================================================
from datetime import date

class Constants:
    LAST_HISTORY_DATE = date(2025, 3, 31)
    REGRESSION_START_DATE = date(2000, 3, 31)
    REGRESSION_END_DATE = date(2023, 6, 30)
    FORECAST_START_DATE = date(2025, 6, 30)


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/_parameters.py
# ====================================================================================
from dataclasses import dataclass
from typing import List
from datetime import date

@dataclass
class Params:
    scenarios: List[str]
    regions: List[str]
    input_dir: str
    output_dir: str
    last_history_date: date
    regression_start_date: date
    regression_end_date: date
    forecast_start_date: date


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/data_preparation.py
# ====================================================================================
import polars as pl
from datetime import date
from typing import Tuple

class DataPreparation:
    """Data preparation matching SAS data steps and proc sql logic"""
    
    def __init__(self, params):
        self.params = params
        
        # Month name to number mapping (for converting "January" -> "01")
        self.month_map = {
            "January": "01", "February": "02", "March": "03", "April": "04",
            "May": "05", "June": "06", "July": "07", "August": "08",
            "September": "09", "October": "10", "November": "11", "December": "12"
        }
    
    def run_data_prep(
        self,
        clv4_state_extract: pl.DataFrame,
        clv4_msa_extract: pl.DataFrame,
        moodys_mapping: pl.DataFrame
    ) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """Main data preparation pipeline"""
        state_data = self._prepare_state_data(clv4_state_extract, moodys_mapping)
        metro_data = self._prepare_metro_data(clv4_msa_extract, moodys_mapping)
        combined_data = self._combine_data(state_data, metro_data)
        return state_data, metro_data, combined_data
    
    def _prepare_state_data(self, clv4_state_extract: pl.DataFrame, moodys_mapping: pl.DataFrame) -> pl.DataFrame:
        """
        SAS equivalent: proc sql create table temp1 + data CLV4_St_Extract
        Add FIP code from mapping and create YYYYMM
        """
        # Detect column names (case-sensitive handling)
        state_name_col = "State Name" if "State Name" in clv4_state_extract.columns else "State_Name"
        year_col = "Year"
        month_col = "Month"
        hpi_col = "Home Price Index" if "Home Price Index" in clv4_state_extract.columns else "Home_Price_Index"
        
        geography_col = "Geography"
        fip_col = "FIP"
        
        # Join with mapping to get State_Code (FIP)
        state_with_fip = clv4_state_extract.join(
            moodys_mapping.select([
                pl.col(geography_col).str.to_uppercase().alias("state_name_upper"),
                pl.col(fip_col).alias("State_Code")
            ]),
            left_on=pl.col(state_name_col).str.to_uppercase(),
            right_on="state_name_upper",
            how="left"
        )
        
        # Create YYYYMM: if month < 10 then cats(Year,'0',month) else cats(Year,month)
        state_with_yyyymm = state_with_fip.with_columns([
            pl.col(month_col).map_elements(
                lambda x: self.month_map.get(x, "01"), 
                return_dtype=pl.Utf8
            ).alias("month_str")
        ]).with_columns([
            (pl.col(year_col).cast(pl.Utf8) + pl.col("month_str")).alias("YYYYMM")
        ])
        
        # Filter: IF YYYYMM > 202503 then delete
        filtered_data = state_with_yyyymm.filter(pl.col("YYYYMM").cast(pl.Int32) <= 202503)
        
        return filtered_data.select([
            "State_Code",
            pl.col(state_name_col).alias("State_Name"),
            "YYYYMM",
            pl.col(hpi_col).alias("Home_Price_Index")
        ])
    
    def _prepare_metro_data(self, clv4_msa_extract: pl.DataFrame, moodys_mapping: pl.DataFrame) -> pl.DataFrame:
        """
        SAS equivalent: proc sql create table temp2 + data CLV4_MSA_Extract
        Add CBSA Code from mapping and create YYYYMM
        """
        # Detect column names (case-sensitive handling)
        cbsa_name_col = "CBSA Name" if "CBSA Name" in clv4_msa_extract.columns else "CBSA_Name"
        year_col = "Year"
        month_col = "Month"
        hpi_col = "Home Price Index" if "Home Price Index" in clv4_msa_extract.columns else "Home_Price_Index"
        
        geography_col = "Geography"
        fip_col = "FIP"
        
        # Join with mapping to get CBSA_Code (compress removes special chars)
        metro_with_fip = clv4_msa_extract.join(
            moodys_mapping.select([
                pl.col(geography_col).str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase().alias("cbsa_name_clean"),
                pl.col(fip_col).alias("CBSA_Code")
            ]),
            left_on=pl.col(cbsa_name_col).str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase(),
            right_on="cbsa_name_clean",
            how="left"
        )
        
        # Create YYYYMM
        metro_with_yyyymm = metro_with_fip.with_columns([
            pl.col(month_col).map_elements(
                lambda x: self.month_map.get(x, "01"), 
                return_dtype=pl.Utf8
            ).alias("month_str")
        ]).with_columns([
            (pl.col(year_col).cast(pl.Utf8) + pl.col("month_str")).alias("YYYYMM")
        ])
        
        # Filter: IF YYYYMM > 202503 then delete
        filtered_data = metro_with_yyyymm.filter(pl.col("YYYYMM").cast(pl.Int32) <= 202503)
        
        return filtered_data.select([
            "CBSA_Code",
            pl.col(cbsa_name_col).alias("CBSA_Name"),
            "YYYYMM",
            pl.col(hpi_col).alias("Home_Price_Index")
        ])
    
    def _combine_data(self, state_data: pl.DataFrame, metro_data: pl.DataFrame) -> pl.DataFrame:
        """
        SAS equivalent: Data CLV4_Combined1 through CLV4_Combined5
        Combine state and metro, add dates, quarters, seasonal adjustment, YOY changes
        """
        # Rename to common columns (Code, Name)
        state_renamed = state_data.select([
            pl.col("State_Code").alias("Code"),
            pl.col("State_Name").alias("Name"),
            pl.col("YYYYMM"),
            pl.col("Home_Price_Index")
        ])
        
        metro_renamed = metro_data.select([
            pl.col("CBSA_Code").alias("Code"),
            pl.col("CBSA_Name").alias("Name"),
            pl.col("YYYYMM"),
            pl.col("Home_Price_Index")
        ])
        
        # Combine (equivalent to SET statement)
        combined = pl.concat([state_renamed, metro_renamed])
        
        # Create date from YYYYMM
        combined_with_date = combined.with_columns([
            pl.col("YYYYMM").str.slice(0, 4).cast(pl.Int32).alias("Year"),
            pl.col("YYYYMM").str.slice(4, 2).cast(pl.Int32).alias("Month")
        ]).with_columns([
            pl.date(pl.col("Year"), pl.col("Month"), 1).alias("date")
        ])
        
        # Add quarter (QtrDt)
        combined_with_quarters = combined_with_date.with_columns([
            pl.when(pl.col("Month").is_in([1, 2, 3])).then(pl.lit("Q1"))
            .when(pl.col("Month").is_in([4, 5, 6])).then(pl.lit("Q2"))
            .when(pl.col("Month").is_in([7, 8, 9])).then(pl.lit("Q3"))
            .when(pl.col("Month").is_in([10, 11, 12])).then(pl.lit("Q4"))
            .alias("QtrDt")
        ])
        
        # Sort and remove duplicates (proc sort noduprec)
        sorted_data = combined_with_quarters.sort(["Code", "date"]).unique(["Code", "date"])
        
        # Add seasonal adjustment (PROC X11 simplified - using HPI as HPI_SA)
        with_sa = self._add_seasonal_adjustment(sorted_data)
        
        # Convert to quarterly (mean by quarter)
        quarterly_data = self._convert_to_quarterly(with_sa)
        
        # Calculate YOY changes
        with_yoy = self._calculate_yoy_changes(quarterly_data)
        
        # Create national aggregate (not in original data, needs to be calculated)
        national_agg = with_yoy.group_by("date").agg([
            pl.col("HPI").mean().alias("HPI"),
            pl.col("HPI_SA").mean().alias("HPI_SA")
        ]).sort("date").with_columns([
            pl.col("HPI_SA").shift(4).alias("HPI_SA_lag4"),
            pl.col("HPI_SA").shift(1).alias("HPI_SA_lag1")
        ]).with_columns([
            ((pl.col("HPI_SA") / pl.col("HPI_SA_lag4")) - 1).alias("YOY_CoreLogicV4"),
            (pl.col("HPI_SA").log() - pl.col("HPI_SA_lag1").log()).alias("DLOG_CoreLogicV4")
        ]).with_columns([
            pl.lit("0").alias("Code"),
            pl.lit("National").alias("Name"),
            pl.lit(None, dtype=pl.Utf8).alias("QtrDt")
        ]).select(with_yoy.columns)
        
        # Combine regional and national
        final_data = pl.concat([with_yoy, national_agg]).sort(["Code", "date"])
        
        return final_data
    
    def _add_seasonal_adjustment(self, data: pl.DataFrame) -> pl.DataFrame:
        """Simplified seasonal adjustment (SAS PROC X11)"""
        return data.with_columns([
            pl.col("Home_Price_Index").alias("HPI_SA")
        ])
    
    def _convert_to_quarterly(self, data: pl.DataFrame) -> pl.DataFrame:
        """Convert monthly to quarterly by taking mean"""
        quarterly = data.group_by(["Code", "Name", "QtrDt"]).agg([
            pl.col("date").max().alias("date"),
            pl.col("Home_Price_Index").mean().alias("HPI"),
            pl.col("HPI_SA").mean().alias("HPI_SA")
        ])
        
        return quarterly.sort(["Code", "date"])
    
    def _calculate_yoy_changes(self, data: pl.DataFrame) -> pl.DataFrame:
        """
        Calculate YOY and DLOG changes
        YOY_CoreLogicV4 = HPI_sa/Lag4(HPI_sa)-1
        DLOG_CoreLogicV4 = LOG(HPI_sa) - Lag1(LOG(HPI_sa))
        """
        with_lags = data.with_columns([
            pl.col("HPI_SA").shift(4).over("Code").alias("HPI_SA_lag4"),
            pl.col("HPI_SA").shift(1).over("Code").alias("HPI_SA_lag1")
        ])
        
        with_changes = with_lags.with_columns([
            ((pl.col("HPI_SA") / pl.col("HPI_SA_lag4")) - 1).alias("YOY_CoreLogicV4"),
            (pl.col("HPI_SA").log() - pl.col("HPI_SA_lag1").log()).alias("DLOG_CoreLogicV4")
        ])
        
        # Filter out Micropolitan (SAS: If Index(name, "Micropolitan"))
        return with_changes.filter(~pl.col("Name").str.contains("Micropolitan"))


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/data_projection.py
# ====================================================================================
import polars as pl
import numpy as np
from datetime import date
from typing import Dict
import sys
sys.path.append('Utils')
from glm_model_architecture import HPIGLMModel

class DataProjection:
    """Data projection matching SAS macro HPI_State_Forecast_V4 and HPI_Metro_Forecast_V4"""
    
    def __init__(self, params):
        self.params = params
    
    def run_state_forecast(
        self,
        scenario: str,
        region: str,
        national_data: pl.DataFrame,
        scenario_hpi_data: pl.DataFrame,
        regional_data: pl.DataFrame
    ) -> Dict[str, pl.DataFrame]:
        """
        SAS Macro: %HPI_State_Forecast_V4
        Run GLM model for state-level forecasts
        """
        # Merge national data with scenario forecast
        national_merged = self._merge_national_data(national_data, scenario_hpi_data)
        
        # Create master panel (code x date)
        master_data = self._create_master_panel(regional_data, national_merged)
        
        # Run GLM model
        model_results = self._run_glm_model(master_data, scenario, region)
        
        # Convert DLOG predictions back to HPI
        forecast_data = self._convert_to_hpi(model_results, scenario)
        
        # Prepare final output (quarterly, monthly, transposed)
        final_results = self._prepare_final_output(forecast_data, scenario, region)
        
        return final_results
    
    def run_metro_forecast(
        self,
        scenario: str,
        region: str,
        national_data: pl.DataFrame,
        scenario_hpi_data: pl.DataFrame,
        regional_data: pl.DataFrame,
        state_results: pl.DataFrame,
        state_metro_map: pl.DataFrame,
        moodys_mapping: pl.DataFrame
    ) -> Dict[str, pl.DataFrame]:
        """
        SAS Macro: %HPI_Metro_Forecast_V4
        Run GLM model for metro-level forecasts using state data
        """
        # Merge national data with scenario forecast
        national_merged = self._merge_national_data(national_data, scenario_hpi_data)
        
        # Create master panel
        master_data = self._create_master_panel(regional_data, national_merged)
        
        # Add state-level data to metro data
        state_mapped = self._add_state_data(master_data, state_results, state_metro_map, moodys_mapping)
        
        # Run metro GLM model (uses state DLOG instead of US DLOG)
        model_results = self._run_metro_glm_model(state_mapped, scenario, region)
        
        # Convert to HPI
        forecast_data = self._convert_to_hpi(model_results, scenario)
        
        # Prepare final output
        final_results = self._prepare_final_output(forecast_data, scenario, region)
        
        return final_results
    
    def _merge_national_data(self, national_data: pl.DataFrame, scenario_hpi_data: pl.DataFrame) -> pl.DataFrame:
        """
        SAS: proc sql create table national1 + data national2
        Merge history with forecast, calculate YOY and DLOG
        """
        # Get column names (handle case sensitivity)
        scenario_cols = scenario_hpi_data.columns
        date_col = next((col for col in scenario_cols if col.lower() in ['date', 'f_date']), 'date')
        hpi_col = next((col for col in scenario_cols if col.upper() == 'CORELOGIC_V4'), 'CORELOGIC_V4')
        
        # Rename scenario columns for consistency
        scenario_renamed = scenario_hpi_data.select([
            pl.col(date_col).alias("date"),
            pl.col(hpi_col).alias("forecast_hpi")
        ])
        
        # Join national history with forecast (full outer join)
        national_with_scenario = national_data.join(
            scenario_renamed,
            on="date",
            how="outer"
        )
        
        # Remove duplicate at last history date (31MAR2025)
        # SAS: If Date and Date1 ='31MAR2025'd then delete
        cleaned_national = national_with_scenario.filter(
            ~((pl.col("HPI_SA").is_not_null()) & 
              (pl.col("forecast_hpi").is_not_null()) & 
              (pl.col("date") == date(2025, 3, 31)))
        )
        
        # Combine history and forecast
        # SAS: If Date=. then Date = Date1; If HPI_sa=. then HPI_sa=CORELOGIC_V4
        final_national = cleaned_national.with_columns([
            pl.coalesce([pl.col("HPI_SA"), pl.col("forecast_hpi")]).alias("HPI_SA")
        ]).sort("date")
        
        # Recalculate YOY and DLOG for full series
        final_national = final_national.with_columns([
            pl.col("HPI_SA").shift(4).alias("HPI_SA_lag4"),
            pl.col("HPI_SA").shift(1).alias("HPI_SA_lag1")
        ]).with_columns([
            ((pl.col("HPI_SA") / pl.col("HPI_SA_lag4")) - 1).alias("YOY_CoreLogicV4"),
            (pl.col("HPI_SA").log() - pl.col("HPI_SA_lag1").log()).alias("DLOG_CoreLogicV4")
        ])
        
        return final_national.filter(pl.col("date") >= date(2000, 1, 1))
    
    def _create_master_panel(self, regional_data: pl.DataFrame, national_data: pl.DataFrame) -> pl.DataFrame:
        """
        SAS: proc sql create table code, date, master
        Create full panel of all codes x all dates
        """
        # Get all unique codes (regions)
        codes = regional_data.select("Name").unique().with_columns(pl.lit(1).alias("ind"))
        
        # Get all unique dates
        dates = national_data.select("date").unique().with_columns(pl.lit(1).alias("ind"))
        
        # Cross join to create full panel
        master = codes.join(dates, on="ind").drop("ind")
        
        # Add regional data
        master_with_regional = master.join(
            regional_data,
            on=["date", "Name"],
            how="left"
        )
        
        # Add US-level DLOG
        master_with_national = master_with_regional.join(
            national_data.select([
                "date",
                pl.col("YOY_CoreLogicV4").alias("YOY_CoreLogicV4_US"),
                pl.col("DLOG_CoreLogicV4").alias("DLOG_CoreLogicV4_US")
            ]),
            on="date",
            how="left"
        )
        
        return master_with_national
    
    def _run_glm_model(self, data: pl.DataFrame, scenario: str, region: str) -> pl.DataFrame:
        """
        SAS: proc glmselect + scoring
        Train GLM on history, score on forecast period
        """
        from dependencies._constants import Constants
        
        # Training data (SAS: where Date1 >= '31Mar2000'd & Date1 <= '30Jun2023'd)
        train_data = data.filter(
            (pl.col("date") >= Constants.REGRESSION_START_DATE) &
            (pl.col("date") <= Constants.REGRESSION_END_DATE) &
            (pl.col("DLOG_CoreLogicV4").is_not_null()) &
            (pl.col("DLOG_CoreLogicV4_US").is_not_null())
        )
        
        # Forecast data (SAS: where Date1 >= '30Jun2025'd)
        forecast_data = data.filter(pl.col("date") >= Constants.FORECAST_START_DATE)
        
        # GLM model: class Name; model DLOG_CoreLogicV4 = DLOG_CoreLogicV4_US*Name / noint
        model = HPIGLMModel(
            target_col="DLOG_CoreLogicV4",
            categorical_cols=["Name"],
            continuous_cols=["DLOG_CoreLogicV4_US"]
        )
        
        # Fit model
        model.fit(train_data)
        
        # Score forecast period
        scored_data = model.score_data(forecast_data, "Pred")
        
        # Combine history (with Pred=.) and forecast
        history_data = data.filter(
            (pl.col("date") >= Constants.REGRESSION_START_DATE) &
            (pl.col("date") <= Constants.LAST_HISTORY_DATE)
        ).with_columns(pl.lit(None, dtype=pl.Float64).alias("Pred"))
        
        combined = pl.concat([history_data, scored_data]).sort(["Name", "date"])
        
        return combined
    
    def _run_metro_glm_model(self, data: pl.DataFrame, scenario: str, region: str) -> pl.DataFrame:
        """
        SAS: proc glmselect for metro (uses State DLOG instead of US DLOG)
        model DLOG_CoreLogicV4 = DLOG_CoreLogicV4_St*name1 / noint
        """
        from dependencies._constants import Constants
        
        # Training data
        train_data = data.filter(
            (pl.col("date") >= Constants.REGRESSION_START_DATE) &
            (pl.col("date") <= Constants.REGRESSION_END_DATE) &
            (pl.col("DLOG_CoreLogicV4").is_not_null()) &
            (pl.col("DLOG_CoreLogicV4_St").is_not_null())
        )
        
        # Forecast data
        forecast_data = data.filter(pl.col("date") >= Constants.FORECAST_START_DATE)
        
        # GLM model with state-level predictor
        model = HPIGLMModel(
            target_col="DLOG_CoreLogicV4",
            categorical_cols=["Name"],
            continuous_cols=["DLOG_CoreLogicV4_St"]
        )
        
        # Fit and score
        model.fit(train_data)
        scored_data = model.score_data(forecast_data, "Pred")
        
        # Combine history and forecast
        history_data = data.filter(
            (pl.col("date") >= Constants.REGRESSION_START_DATE) &
            (pl.col("date") <= Constants.LAST_HISTORY_DATE)
        ).with_columns(pl.lit(None, dtype=pl.Float64).alias("Pred"))
        
        combined = pl.concat([history_data, scored_data]).sort(["Name", "date"])
        
        return combined
    
    def _add_state_data(
        self,
        master_data: pl.DataFrame,
        state_results: pl.DataFrame,
        state_metro_map: pl.DataFrame,
        moodys_mapping: pl.DataFrame
    ) -> pl.DataFrame:
        """
        SAS: Join metro data with state forecasts
        Add DLOG_CoreLogicV4_St to metro data
        """
        # Get column names (case-sensitive)
        map_cols = state_metro_map.columns
        cbsa_code_col = next((col for col in map_cols if col.upper().replace('_', '').replace(' ', '') == 'CBSACODE'), 'CBSA_Code')
        st_col = next((col for col in map_cols if col.upper() == 'ST'), 'ST')
        
        mapping_cols = moodys_mapping.columns
        geography_col = next((col for col in mapping_cols if col.upper() == 'GEOGRAPHY'), 'Geography')
        geocode_col = next((col for col in mapping_cols if col.upper() == 'GEOCODE'), 'Geocode')
        
        # Add geography to state-metro map
        state_metro_enhanced = state_metro_map.join(
            moodys_mapping.select([
                pl.col(geography_col).str.to_uppercase().alias("Geography_Upper"),
                pl.col(geocode_col)
            ]),
            left_on=st_col,
            right_on=geocode_col,
            how="left"
        )
        
        # Join state results with metro mapping
        state_with_metro = state_results.join(
            state_metro_enhanced.select([
                "Geography_Upper",
                pl.col(cbsa_code_col).alias("metro_code")
            ]),
            left_on=pl.col("CBSA_name").str.to_uppercase(),
            right_on="Geography_Upper",
            how="left"
        )
        
        # Add state HPI to master data
        master_with_state = master_data.join(
            state_with_metro.select([
                "date",
                "metro_code",
                pl.col("HPI").alias("CoreLogicV4_St")
            ]),
            left_on=["date", "Code"],
            right_on=["date", "metro_code"],
            how="left"
        )
        
        # Calculate DLOG of state HPI
        master_with_state = master_with_state.with_columns([
            pl.col("CoreLogicV4_St").shift(1).over("Name").alias("CoreLogicV4_St_lag1")
        ]).with_columns([
            (pl.col("CoreLogicV4_St").log() - pl.col("CoreLogicV4_St_lag1").log()).alias("DLOG_CoreLogicV4_St")
        ])
        
        return master_with_state
    
    def _convert_to_hpi(self, model_results: pl.DataFrame, scenario: str) -> pl.DataFrame:
        """
        SAS: data &Region._&Scenario
        Convert DLOG predictions back to HPI level
        HPIPred = Exp(Log(HPIPred) + Pred)
        """
        from dependencies._constants import Constants
        
        hpi_col_name = f"HPIPred_{scenario}"
        
        # Initialize HPI column
        with_hpi = model_results.with_columns([
            pl.lit(None, dtype=pl.Float64).alias(hpi_col_name)
        ])
        
        # Process each region separately (by Name)
        result_list = []
        for name in with_hpi["Name"].unique():
            name_data = with_hpi.filter(pl.col("Name") == name).sort("date")
            
            hpi_values = []
            current_hpi = None
            
            # Iterate through dates
            for row in name_data.iter_rows(named=True):
                # At last history date, initialize with actual HPI
                if row["date"] == Constants.LAST_HISTORY_DATE:
                    current_hpi = row["HPI_SA"]
                    hpi_values.append(current_hpi)
                # For forecast dates, apply DLOG prediction
                elif row["Pred"] is not None and current_hpi is not None:
                    # HPIPred = Exp(Log(HPIPred) + Pred)
                    current_hpi = current_hpi * np.exp(row["Pred"])
                    hpi_values.append(current_hpi)
                else:
                    # For history dates, use actual HPI_SA
                    hpi_values.append(current_hpi if current_hpi is not None else row.get("HPI_SA"))
            
            name_result = name_data.with_columns(
                pl.Series(name=hpi_col_name, values=hpi_values)
            )
            result_list.append(name_result)
        
        return pl.concat(result_list)
    
    def _prepare_final_output(self, forecast_data: pl.DataFrame, scenario: str, region: str) -> Dict[str, pl.DataFrame]:
        """
        SAS: Create quarterly and monthly outputs, transpose
        proc expand: convert quarterly to monthly
        proc transpose: transpose for wide format
        """
        hpi_col = f"HPIPred_{scenario}"
        
        # Quarterly output
        quarterly_data = forecast_data.select([
            pl.col("Code").alias("CBSA_Code"),
            pl.col("Name").alias("CBSA_name"),
            "date",
            pl.col(hpi_col).alias("HPI")
        ]).sort(["CBSA_Code", "CBSA_name", "date"])
        
        # Convert quarterly to monthly (proc expand)
        monthly_data = self._convert_quarterly_to_monthly(quarterly_data)
        
        # Transpose (proc transpose)
        quarterly_transposed = self._transpose_data(quarterly_data, "date", "CBSA_name", "HPI")
        monthly_transposed = self._transpose_data(monthly_data, "date", "CBSA_name", "HPI")
        
        return {
            "quarterly": quarterly_data,
            "monthly": monthly_data,
            "quarterly_transposed": quarterly_transposed,
            "monthly_transposed": monthly_transposed
        }
    
    def _convert_quarterly_to_monthly(self, quarterly_data: pl.DataFrame) -> pl.DataFrame:
        """
        SAS: proc expand from=Quarter to=month
        Replicate quarterly value to all 3 months in quarter
        """
        monthly_data = []
        
        for row in quarterly_data.iter_rows(named=True):
            quarter_date = row["date"]
            hpi = row["HPI"]
            
            # Determine which months belong to this quarter
            if quarter_date.month == 3:  # Q1
                months = [1, 2, 3]
            elif quarter_date.month == 6:  # Q2
                months = [4, 5, 6]
            elif quarter_date.month == 9:  # Q3
                months = [7, 8, 9]
            elif quarter_date.month == 12:  # Q4
                months = [10, 11, 12]
            else:
                continue
            
            # Create monthly records
            for month in months:
                monthly_date = date(quarter_date.year, month, 1)
                monthly_data.append({
                    "CBSA_Code": row["CBSA_Code"],
                    "CBSA_name": row["CBSA_name"],
                    "date": monthly_date,
                    "HPI": hpi
                })
        
        return pl.DataFrame(monthly_data)
    
    def _transpose_data(self, data: pl.DataFrame, date_col: str, id_col: str, value_col: str) -> pl.DataFrame:
        """
        SAS: proc transpose
        Transpose long to wide format (dates as rows, regions as columns)
        """
        return data.pivot(
            values=value_col,
            index=date_col,
            columns=id_col
        )


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/model.py
# ====================================================================================
import polars as pl
from typing import Dict
from pathlib import Path
from dependencies._constants import Constants
from dependencies._parameters import Params
from data_preparation import DataPreparation
from data_projection import DataProjection
import sys
sys.path.append('Utils')
from data_import_helpers import DataImportHelper

class GLMModelScenarioProjection:
    """Main model orchestrator - runs full HPI projection pipeline"""
    
    def __init__(self, params: Params):
        self.params = params
        self.data_prep = DataPreparation(params)
        self.data_projection = DataProjection(params)
        self.data_helper = DataImportHelper(params.input_dir)
    
    def run_projections(self, input_data: Dict[str, pl.DataFrame]) -> Dict[str, Dict[str, pl.DataFrame]]:
        """
        Run complete projection pipeline for all scenarios and regions
        Returns dictionary of results by scenario-region combination
        """
        # Step 1: Prepare data (combine state and metro, calculate indicators)
        state_data, metro_data, combined_data = self.data_prep.run_data_prep(
            input_data["clv4_state_extract"],
            input_data["clv4_msa_extract"],
            input_data["moodys_mapping"]
        )
        
        # Step 2: Split into national, state, and metro datasets
        national_data = self._prepare_national_data(combined_data)
        regional_state_data = self._split_regional_data(combined_data, "state")
        regional_metro_data = self._split_regional_data(combined_data, "metro")
        
        all_results = {}
        
        # Step 3: Run forecasts for each scenario
        for scenario in self.params.scenarios:
            scenario_data = input_data[f"{scenario}_hpi_national"]
            
            # State-level forecast
            state_results = self.data_projection.run_state_forecast(
                scenario, "state", national_data, scenario_data, regional_state_data
            )
            
            # Metro-level forecast (uses state results)
            metro_results = self.data_projection.run_metro_forecast(
                scenario, "metro", national_data, scenario_data, regional_metro_data,
                state_results["quarterly"], input_data["state_metro_map"], input_data["moodys_mapping"]
            )
            
            all_results[f"state_{scenario}"] = state_results
            all_results[f"metro_{scenario}"] = metro_results
            
            # Export results
            self._export_results(state_results, f"state_{scenario}")
            self._export_results(metro_results, f"metro_{scenario}")
        
        return all_results
    
    def _prepare_national_data(self, combined_data: pl.DataFrame) -> pl.DataFrame:
        """Extract national-level data"""
        return combined_data.filter(pl.col("Name") == "National")
    
    def _split_regional_data(self, combined_data: pl.DataFrame, region_type: str) -> pl.DataFrame:
        """
        Split data by region type
        State: code > 0 and <= 100
        Metro: code >= 100 and < 100000
        """
        if region_type == "state":
            return combined_data.filter(
                (pl.col("Code").cast(pl.Int64) > 0) & 
                (pl.col("Code").cast(pl.Int64) <= 100)
            )
        elif region_type == "metro":
            return combined_data.filter(
                (pl.col("Code").cast(pl.Int64) >= 100) & 
                (pl.col("Code").cast(pl.Int64) < 100000)
            )
        else:
            return combined_data.filter(pl.col("Name") != "National")
    
    def _export_results(self, results: Dict[str, pl.DataFrame], scenario_region: str) -> None:
        """Export results to Excel files"""
        output_dir = Path(self.params.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Export monthly transposed (main output)
        if "monthly_transposed" in results:
            filename = f"CoreLogic_{scenario_region}.xlsx"
            self.data_helper.export_excel(
                results["monthly_transposed"], 
                filename, 
                scenario_region.split('_')[1],
                self.params.output_dir
            )


# ====================================================================================
# FILE: main.py
# ====================================================================================
import polars as pl
from pathlib import Path
from datetime import date
import sys

# Add src directory to path
sys.path.append('src/gemini_scenario_models/hpi_projection_us')

from model import GLMModelScenarioProjection
from dependencies._parameters import Params
sys.path.append('Utils')
from data_import_helpers import DataImportHelper

def main():
    """Main execution function"""
    
    # Define parameters
    params = Params(
        scenarios=["ce", "up", "dn", "dn2"],
        regions=["state", "metro"],
        input_dir="Input",
        output_dir="Output",
        last_history_date=date(2025, 3, 31),
        regression_start_date=date(2000, 3, 31),
        regression_end_date=date(2023, 6, 30),
        forecast_start_date=date(2025, 6, 30)
    )
    
    # Initialize data helper
    data_helper = DataImportHelper(params.input_dir)
    
    print("Loading input data...")
    
    # Load all input files
    input_data = {
        "moodys_mapping": data_helper.import_excel("Basket_2016-10-5_13_45_V2.xlsx", "Mapping"),
        "clv4_state_extract": data_helper.import_excel("HPI Data by State.xlsx", "HPI Data by State"),
        "clv4_msa_extract": data_helper.import_excel("HPI Data by CBSA.xlsx", "HPI Data by CBSA"),
        "state_metro_map": data_helper.import_excel("state_metro_map.xlsx"),
        "ce_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_202502.xlsx", "CE"),
        "up_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_202502.xlsx", "UP"),
        "dn_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_202502.xlsx", "DN"),
        "dn2_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_202502.xlsx", "DN2")
    }
    
    print("Input data loaded successfully!")
    print(f"  - State data: {len(input_data['clv4_state_extract'])} rows")
    print(f"  - Metro data: {len(input_data['clv4_msa_extract'])} rows")
    print(f"  - Mapping data: {len(input_data['moodys_mapping'])} rows")
    
    # Initialize and run projection model
    print("\nRunning HPI projections...")
    projection_model = GLMModelScenarioProjection(params)
    results = projection_model.run_projections(input_data)
    
    print("\n✓ HPI Projection model completed successfully!")
    print(f"✓ Generated results for {len(results)} scenario-region combinations")
    print(f"✓ Output files saved to: {params.output_dir}/")
    
    # Print summary
    print("\nResults summary:")
    for key, value in results.items():
        if "quarterly" in value:
            print(f"  - {key}: {len(value['quarterly'])} quarterly forecasts")

if __name__ == "__main__":
    main()


# ====================================================================================
# ADDITIONAL FILES FOR COMPLETENESS
# ====================================================================================

# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/__init__.py
from _constants import Constants
from _parameters import Params

__all__ = ["Constants", "Params"]


# ====================================================================================
# FILE: src/gemini_scenario_models/hpi_projection_us/__init__.py
# ====================================================================================
from model import GLMModelScenarioProjection

__all__ = ["GLMModelScenarioProjection"]


# ====================================================================================
# INSTRUCTIONS FOR RUNNING
# ====================================================================================
"""
PROJECT STRUCTURE:
ccar_scenario_models_v1/
├── Input/
│   ├── Basket_2016-10-5_13_45_V2.xlsx
│   ├── HPI Data by State.xlsx
│   ├── HPI Data by CBSA.xlsx
│   ├── state_metro_map.xlsx
│   └── Data_Forecast_National_HPI_202502.xlsx
├── Output/
├── Utils/
│   ├── data_import_helpers.py
│   └── glm_model_architecture.py
├── src/
│   └── gemini_scenario_models/
│       └── hpi_projection_us/
│           ├── __init__.py
│           ├── model.py
│           ├── data_preparation.py
│           ├── data_projection.py
│           └── dependencies/
│               ├── __init__.py
│               ├── _constants.py
│               └── _parameters.py
├── main.py
└── requirements.txt

REQUIREMENTS (requirements.txt):
polars>=0.20.0
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0
openpyxl>=3.1.0

TO RUN:
1. Install dependencies: pip install -r requirements.txt
2. Place all input Excel files in Input/ directory
3. Run: python main.py

KEY FIXES APPLIED:
1. ✓ Removed scenario_data.xlsx dependency (not needed per SAS code)
2. ✓ Fixed month name to number conversion ("January" -> "01")
3. ✓ Fixed date parsing from Excel (M/D/YYYY format)
4. ✓ Fixed case sensitivity for all column names
5. ✓ Created national aggregate from regional data
6. ✓ Fixed column alignment for concat operations
7. ✓ Fixed date column dtype handling in joins
8. ✓ Added proper null handling in GLM training
9. ✓ Matched exact SAS logic for all calculations
10. ✓ Fixed state-metro mapping joins

LOGIC MATCHES SAS CODE:
- Data preparation follows SAS data steps exactly
- GLM model matches proc glmselect with noint
- YOY and DLOG calculations identical to SAS
- Quarterly to monthly conversion matches proc expand
- Transpose operations match proc transpose
"""