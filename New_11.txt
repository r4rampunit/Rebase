import sqlite3
import pandas as pd
from pathlib import Path
import re
import logging
from datetime import datetime
from collections import Counter
from typing import Dict, Set, List

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'db_transformation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)

def get_dynamic_priority_terms(descriptions: List[str]) -> Set[str]:

    skip_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 
        'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 
        'over', 'after', 'fully', 'consolidated', 'non', 'us', 'domicile'
    }
    
    word_freq = Counter()
    end_word_freq = Counter()
    
    for desc in descriptions:
        desc = re.sub(r'\([^)]*\)', '', desc.lower())
        words = re.sub(r'[^a-z0-9\s]', ' ', desc).split()
        
        words = [w for w in words 
                if w not in skip_words 
                and not any(char.isdigit() for char in w)]
        
        word_freq.update(words)
        
        if words:
            end_word_freq.update([words[-1]])
    
    total_freq = Counter()
    for word, freq in word_freq.items():
        total_freq[word] = freq
        if word in end_word_freq:
            total_freq[word] += end_word_freq[word] * 2
    
    avg_freq = sum(total_freq.values()) / len(total_freq)
    priority_terms = {word for word, freq in total_freq.items() 
                     if freq > avg_freq}
    
    return priority_terms

def clean_description_to_column_name(description: str, priority_terms: Set[str]) -> str:

    name = description.lower()
    
    name = re.sub(r'\([^)]*\)', '', name)
    
    parts = name.split('-')
    
    last_part = parts[-1].strip() if parts else ''
    main_desc = ' '.join(parts[:-1]).strip() if len(parts) > 1 else name
    
    skip_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 
        'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 
        'over', 'after', 'fully', 'consolidated', 'non', 'us', 'domicile'
    }
    
    def process_text(text):
        words = re.sub(r'[^a-z0-9\s]', ' ', text.lower()).split()
        seen = set()
        return [w for w in words 
                if not any(char.isdigit() for char in w)
                and w not in skip_words
                and not (w in seen or seen.add(w))]
    
    main_words = process_text(main_desc)
    last_words = process_text(last_part)
    
    all_words = []
    
    priority_matches = [w for w in main_words + last_words if w in priority_terms]
    all_words.extend(priority_matches[:2])
    
    if last_words and last_words[-1] not in all_words:
        all_words.append(last_words[-1])
    
    while len(all_words) < 3 and main_words:
        word = main_words.pop(0)
        if word not in all_words:
            all_words.append(word)
    
    column_name = '_'.join(all_words)
    column_name = re.sub(r'[^a-z0-9_]', '', column_name)
    
    return column_name if column_name[0].isalpha() else f'col_{column_name}'



def validate_mapping_file(mapping_df):

    required_columns = {'MDRM', 'Line Item Description'}
    missing_columns = required_columns - set(mapping_df.columns)
    
    if missing_columns:
        raise ValueError(f"Missing required columns in mapping file: {missing_columns}")
    
    if mapping_df.empty:
        raise ValueError("Mapping file is empty")
    
    return True



def create_column_mapping(mapping_df: pd.DataFrame) -> Dict[str, str]:

    all_descriptions = mapping_df['Line Item Description'].dropna().tolist()
    priority_terms = get_dynamic_priority_terms(all_descriptions)
    
    logging.info(f"Identified {len(priority_terms)} priority terms from {len(all_descriptions)} descriptions")
    
    mapping_dict = {}
    duplicates = set()
    
    for _, row in mapping_df.iterrows():
        mdrm = row['MDRM']
        description = row['Line Item Description']
        
        if pd.isna(mdrm) or pd.isna(description):
            logging.warning(f"Skipping row with missing data: MDRM={mdrm}, Description={description}")
            continue
            
        new_name = clean_description_to_column_name(description, priority_terms)
        
        base_name = new_name
        counter = 1
        while new_name in mapping_dict.values():
            new_name = f"{base_name}_{counter}"
            counter += 1
            duplicates.add(base_name)
        
        mapping_dict[mdrm] = new_name
    
    if duplicates:
        logging.info(f"Handled duplicate names for: {duplicates}")
    
    return mapping_dict




def process_database(source_db_path, target_db_path, mapping_dict):

    with sqlite3.connect(source_db_path) as source_conn, \
         sqlite3.connect(target_db_path) as target_conn:
        
        cursor = source_conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = [row[0] for row in cursor.fetchall()]
        
        for table_name in tables:
            try:
                cursor.execute(f"PRAGMA table_info({table_name});")
                columns = [row[1] for row in cursor.fetchall()]
                
                new_columns = {col: mapping_dict.get(col, col) for col in columns}
                
                df = pd.read_sql_query(f"SELECT * FROM {table_name}", source_conn)
                df = df.rename(columns=new_columns)
                
                df.to_sql(table_name, target_conn, index=False, if_exists='replace')
                
                changes = {col: new_col for col, new_col in new_columns.items() if col != new_col}
                if changes:
                    logging.info(f"Table {table_name} - Renamed columns: {changes}")
                else:
                    logging.info(f"Table {table_name} - No columns renamed")
                
            except Exception as e:
                logging.error(f"Error processing table {table_name}: {str(e)}")
                raise

def main():
    try:
        source_db = Path("self_loc/raw.sqlite")
        target_db = Path("self_loc/mirror.sqlite")
        mapping_file = Path("path_to_your_excel_file.xlsx")
        if not source_db.exists():
            raise FileNotFoundError(f"Source database not found: {source_db}")
        
        if not mapping_file.exists():
            raise FileNotFoundError(f"Mapping file not found: {mapping_file}")
        
        logging.info("Reading mapping file...")
        mapping_df = pd.read_excel(mapping_file)
        validate_mapping_file(mapping_df)
        
        logging.info("Creating column mapping...")
        mapping_dict = create_column_mapping(mapping_df)
        
        logging.info("Starting database transformation...")
        process_database(str(source_db), str(target_db), mapping_dict)
        
        logging.info("Database transformation completed successfully!")
        
    except Exception as e:
        logging.error(f"Error during database transformation: {str(e)}")
        raise

if __name__ == "__main__":
    main()