# FILE: Utils/data_import_helpers.py
import polars as pl
from pathlib import Path
from typing import Optional
import openpyxl

class DataImportHelper:
    def __init__(self, input_dir: str = "Input"):
        self.input_dir = Path(input_dir)
    
    def import_excel(self, filename: str, sheet_name: Optional[str] = None) -> pl.DataFrame:
        """Import Excel file and return Polars DataFrame"""
        file_path = self.input_dir / filename
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Read with pandas first to handle Excel properly, then convert to Polars
        import pandas as pd
        df_pandas = pd.read_excel(file_path, sheet_name=sheet_name)
        
        # Convert date columns if they exist
        for col in df_pandas.columns:
            if 'date' in col.lower() and df_pandas[col].dtype == 'object':
                try:
                    df_pandas[col] = pd.to_datetime(df_pandas[col])
                except:
                    pass
        
        return pl.from_pandas(df_pandas)
    
    def export_csv(self, data: pl.DataFrame, filename: str, output_dir: str = "Output") -> None:
        output_path = Path(output_dir) / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        data.write_csv(output_path)
    
    def export_excel(self, data: pl.DataFrame, filename: str, sheet_name: str = "Sheet1", output_dir: str = "Output") -> None:
        output_path = Path(output_dir) / filename
        output_path.parent.mkdir(parents=True, exist_ok=True)
        data.to_pandas().to_excel(output_path, sheet_name=sheet_name, index=False)


# FILE: Utils/glm_model_architecture.py
import polars as pl
import numpy as np
from sklearn.linear_model import LinearRegression
from typing import List
from abc import ABC, abstractmethod
import pandas as pd

class BaseGLMModel(ABC):
    def __init__(self, fit_intercept: bool = False):
        self.model = LinearRegression(fit_intercept=fit_intercept)
        self.is_fitted = False
        self.feature_columns = None
        self.target_column = None
        self.categorical_columns = []
    
    @abstractmethod
    def prepare_features(self, data: pl.DataFrame) -> np.ndarray:
        pass
    
    @abstractmethod
    def prepare_target(self, data: pl.DataFrame) -> np.ndarray:
        pass
    
    def fit(self, train_data: pl.DataFrame) -> 'BaseGLMModel':
        X = self.prepare_features(train_data)
        y = self.prepare_target(train_data)
        
        if len(X) == 0 or len(y) == 0:
            raise ValueError("Training data is empty after preparation")
        
        self.model.fit(X, y)
        self.is_fitted = True
        return self
    
    def predict(self, data: pl.DataFrame) -> np.ndarray:
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        X = self.prepare_features(data)
        return self.model.predict(X)
    
    def score_data(self, data: pl.DataFrame, prediction_col: str = "prediction") -> pl.DataFrame:
        predictions = self.predict(data)
        return data.with_columns(pl.Series(name=prediction_col, values=predictions))


class HPIGLMModel(BaseGLMModel):
    def __init__(self, target_col: str, categorical_cols: List[str], continuous_cols: List[str]):
        super().__init__(fit_intercept=False)
        self.target_col = target_col
        self.categorical_cols = categorical_cols
        self.continuous_cols = continuous_cols
        self.fitted_categories = {}
        self.category_mapping = {}
    
    def prepare_features(self, data: pl.DataFrame) -> np.ndarray:
        features = []
        
        # Handle categorical variables with interaction
        for cat_col in self.categorical_cols:
            for cont_col in self.continuous_cols:
                # Create interaction between categorical and continuous
                cat_data = data.select(cat_col).to_pandas()
                cont_data = data.select(cont_col).to_numpy().ravel()
                
                # Get dummies for categorical
                dummies = pd.get_dummies(cat_data[cat_col], drop_first=False, dtype=float)
                
                if not self.is_fitted:
                    self.category_mapping[cat_col] = list(dummies.columns)
                
                # Multiply each dummy by continuous variable (interaction)
                for col in dummies.columns:
                    interaction = dummies[col].values * cont_data
                    features.append(interaction.reshape(-1, 1))
        
        if features:
            return np.hstack(features)
        else:
            return np.array([]).reshape(len(data), 0)
    
    def prepare_target(self, data: pl.DataFrame) -> np.ndarray:
        return data.select(self.target_col).to_numpy().ravel()


# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/__init__.py
from ._constants import Constants
from ._parameters import Params

__all__ = ["Constants", "Params"]


# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/_constants.py
from datetime import date

class Constants:
    LAST_HISTORY_DATE = date(2025, 3, 31)
    REGRESSION_START_DATE = date(2000, 3, 31)
    REGRESSION_END_DATE = date(2023, 6, 30)
    FORECAST_START_DATE = date(2025, 6, 30)
    
    MONTH_MAPPING = {
        "January": "01", "February": "02", "March": "03", "April": "04",
        "May": "05", "June": "06", "July": "07", "August": "08",
        "September": "09", "October": "10", "November": "11", "December": "12"
    }


# FILE: src/gemini_scenario_models/hpi_projection_us/dependencies/_parameters.py
from dataclasses import dataclass
from typing import List
from datetime import date

@dataclass
class Params:
    scenarios: List[str]
    regions: List[str]
    input_dir: str
    output_dir: str
    last_history_date: date
    regression_start_date: date
    regression_end_date: date
    forecast_start_date: date


# FILE: src/gemini_scenario_models/hpi_projection_us/__init__.py
from .model import GLMModelScenarioProjection

__all__ = ["GLMModelScenarioProjection"]


# FILE: src/gemini_scenario_models/hpi_projection_us/data_preparation.py
import polars as pl
from datetime import datetime, date
from typing import Tuple
from .dependencies import Constants

class DataPreparation:
    def __init__(self, params):
        self.params = params
    
    def run_data_prep(
        self,
        clv4_state_extract: pl.DataFrame,
        clv4_msa_extract: pl.DataFrame,
        moodys_mapping: pl.DataFrame
    ) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:
        """Main data preparation pipeline"""
        state_data = self._prepare_state_data(clv4_state_extract, moodys_mapping)
        metro_data = self._prepare_metro_data(clv4_msa_extract, moodys_mapping)
        combined_data = self._combine_data(state_data, metro_data)
        return state_data, metro_data, combined_data
    
    def _prepare_state_data(self, clv4_state_extract: pl.DataFrame, moodys_mapping: pl.DataFrame) -> pl.DataFrame:
        """Prepare state-level data with proper date handling"""
        # Normalize column names
        state_cols = {col: col.lower().replace(" ", "_") for col in clv4_state_extract.columns}
        state_df = clv4_state_extract.rename(state_cols)
        
        mapping_cols = {col: col.lower().replace(" ", "_") for col in moodys_mapping.columns}
        mapping_df = moodys_mapping.rename(mapping_cols)
        
        # Map month names to numbers
        month_col = next((c for c in state_df.columns if 'month' in c.lower()), None)
        year_col = next((c for c in state_df.columns if 'year' in c.lower()), None)
        
        if month_col and year_col:
            state_df = state_df.with_columns([
                pl.col(month_col).map_elements(
                    lambda x: Constants.MONTH_MAPPING.get(str(x), "01") if x else "01",
                    return_dtype=pl.Utf8
                ).alias("month_str")
            ]).with_columns([
                (pl.col(year_col).cast(pl.Utf8) + pl.col("month_str")).alias("yyyymm")
            ])
        
        # Join with mapping to get FIP codes
        state_name_col = next((c for c in state_df.columns if 'state' in c.lower() and 'name' in c.lower()), None)
        geo_col = next((c for c in mapping_df.columns if 'geography' in c.lower()), None)
        fip_col = next((c for c in mapping_df.columns if 'fip' in c.lower()), None)
        
        if state_name_col and geo_col and fip_col:
            state_with_fip = state_df.join(
                mapping_df.select([
                    pl.col(geo_col).str.to_uppercase().alias("geo_upper"),
                    pl.col(fip_col).alias("state_code")
                ]),
                left_on=pl.col(state_name_col).str.to_uppercase(),
                right_on="geo_upper",
                how="left"
            )
        else:
            state_with_fip = state_df.with_columns(pl.lit(None).alias("state_code"))
        
        # Filter to last history date
        if "yyyymm" in state_with_fip.columns:
            filtered = state_with_fip.filter(pl.col("yyyymm").cast(pl.Int32) <= 202503)
        else:
            filtered = state_with_fip
        
        return filtered
    
    def _prepare_metro_data(self, clv4_msa_extract: pl.DataFrame, moodys_mapping: pl.DataFrame) -> pl.DataFrame:
        """Prepare metro-level data"""
        metro_cols = {col: col.lower().replace(" ", "_") for col in clv4_msa_extract.columns}
        metro_df = clv4_msa_extract.rename(metro_cols)
        
        mapping_cols = {col: col.lower().replace(" ", "_") for col in moodys_mapping.columns}
        mapping_df = moodys_mapping.rename(mapping_cols)
        
        # Map month names to numbers
        month_col = next((c for c in metro_df.columns if 'month' in c.lower()), None)
        year_col = next((c for c in metro_df.columns if 'year' in c.lower()), None)
        
        if month_col and year_col:
            metro_df = metro_df.with_columns([
                pl.col(month_col).map_elements(
                    lambda x: Constants.MONTH_MAPPING.get(str(x), "01") if x else "01",
                    return_dtype=pl.Utf8
                ).alias("month_str")
            ]).with_columns([
                (pl.col(year_col).cast(pl.Utf8) + pl.col("month_str")).alias("yyyymm")
            ])
        
        # Join with mapping
        cbsa_name_col = next((c for c in metro_df.columns if 'cbsa' in c.lower() and 'name' in c.lower()), None)
        geo_col = next((c for c in mapping_df.columns if 'geography' in c.lower()), None)
        fip_col = next((c for c in mapping_df.columns if 'fip' in c.lower()), None)
        
        if cbsa_name_col and geo_col and fip_col:
            metro_with_fip = metro_df.join(
                mapping_df.select([
                    pl.col(geo_col).str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase().alias("geo_clean"),
                    pl.col(fip_col).alias("cbsa_code")
                ]),
                left_on=pl.col(cbsa_name_col).str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase(),
                right_on="geo_clean",
                how="left"
            )
        else:
            metro_with_fip = metro_df.with_columns(pl.lit(None).alias("cbsa_code"))
        
        # Filter data
        if "yyyymm" in metro_with_fip.columns:
            filtered = metro_with_fip.filter(pl.col("yyyymm").cast(pl.Int32) <= 202503)
        else:
            filtered = metro_with_fip
        
        return filtered
    
    def _combine_data(self, state_data: pl.DataFrame, metro_data: pl.DataFrame) -> pl.DataFrame:
        """Combine state and metro data"""
        # Identify key columns
        state_code_col = next((c for c in state_data.columns if 'state_code' in c.lower() or 'code' in c.lower()), None)
        state_name_col = next((c for c in state_data.columns if 'state' in c.lower() and 'name' in c.lower()), None)
        state_hpi_col = next((c for c in state_data.columns if 'price' in c.lower() and 'index' in c.lower()), None)
        state_yyyymm_col = "yyyymm" if "yyyymm" in state_data.columns else None
        
        metro_code_col = next((c for c in metro_data.columns if 'cbsa_code' in c.lower() or 'code' in c.lower()), None)
        metro_name_col = next((c for c in metro_data.columns if 'cbsa' in c.lower() and 'name' in c.lower()), None)
        metro_hpi_col = next((c for c in metro_data.columns if 'price' in c.lower() and 'index' in c.lower()), None)
        metro_yyyymm_col = "yyyymm" if "yyyymm" in metro_data.columns else None
        
        # Rename to standard names
        state_renamed = state_data.select([
            pl.col(state_code_col).alias("code") if state_code_col else pl.lit(None).alias("code"),
            pl.col(state_name_col).alias("name") if state_name_col else pl.lit(None).alias("name"),
            pl.col(state_yyyymm_col).alias("yyyymm") if state_yyyymm_col else pl.lit(None).alias("yyyymm"),
            pl.col(state_hpi_col).alias("home_price_index") if state_hpi_col else pl.lit(None).alias("home_price_index")
        ])
        
        metro_renamed = metro_data.select([
            pl.col(metro_code_col).alias("code") if metro_code_col else pl.lit(None).alias("code"),
            pl.col(metro_name_col).alias("name") if metro_name_col else pl.lit(None).alias("name"),
            pl.col(metro_yyyymm_col).alias("yyyymm") if metro_yyyymm_col else pl.lit(None).alias("yyyymm"),
            pl.col(metro_hpi_col).alias("home_price_index") if metro_hpi_col else pl.lit(None).alias("home_price_index")
        ])
        
        # Combine
        combined = pl.concat([state_renamed, metro_renamed])
        
        # Create proper dates
        combined = combined.with_columns([
            pl.col("yyyymm").str.slice(0, 4).cast(pl.Int32).alias("year"),
            pl.col("yyyymm").str.slice(4, 2).cast(pl.Int32).alias("month")
        ]).with_columns([
            pl.date(pl.col("year"), pl.col("month"), 1).alias("date")
        ])
        
        # Add quarterly date
        combined = combined.with_columns([
            pl.when(pl.col("month").is_in([1, 2, 3])).then(pl.col("year").cast(pl.Utf8) + "Q1")
            .when(pl.col("month").is_in([4, 5, 6])).then(pl.col("year").cast(pl.Utf8) + "Q2")
            .when(pl.col("month").is_in([7, 8, 9])).then(pl.col("year").cast(pl.Utf8) + "Q3")
            .otherwise(pl.col("year").cast(pl.Utf8) + "Q4")
            .alias("qtrdt")
        ])
        
        # Remove duplicates and sort
        combined = combined.unique(subset=["code", "date"]).sort(["code", "date"])
        
        # Add seasonal adjustment (simplified - just copy HPI)
        combined = combined.with_columns([
            pl.col("home_price_index").alias("hpi_sa")
        ])
        
        # Convert to quarterly
        quarterly = combined.group_by(["code", "name", "qtrdt"]).agg([
            pl.col("date").max().alias("date"),
            pl.col("home_price_index").mean().alias("hpi"),
            pl.col("hpi_sa").mean().alias("hpi_sa")
        ]).sort(["code", "date"])
        
        # Calculate YoY changes
        quarterly = quarterly.with_columns([
            pl.col("hpi_sa").shift(4).over("code").alias("hpi_sa_lag4"),
            pl.col("hpi_sa").shift(1).over("code").alias("hpi_sa_lag1")
        ]).with_columns([
            ((pl.col("hpi_sa") / pl.col("hpi_sa_lag4")) - 1).alias("yoy_corelogicv4"),
            (pl.col("hpi_sa").log() - pl.col("hpi_sa_lag1").log()).alias("dlog_corelogicv4")
        ])
        
        return quarterly


# FILE: src/gemini_scenario_models/hpi_projection_us/data_projection.py
import polars as pl
import numpy as np
from datetime import date
from typing import Dict
import sys
sys.path.append('.')
from Utils.glm_model_architecture import HPIGLMModel

class DataProjection:
    def __init__(self, params):
        self.params = params
    
    def run_state_forecast(
        self,
        scenario: str,
        region: str,
        national_data: pl.DataFrame,
        scenario_data: pl.DataFrame,
        regional_data: pl.DataFrame
    ) -> Dict[str, pl.DataFrame]:
        """Run state-level forecast"""
        # Merge national with scenario
        national_merged = self._merge_national_data(national_data, scenario_data)
        
        # Create master panel
        master_data = self._create_master_panel(regional_data, national_merged)
        
        # Run GLM
        model_results = self._run_glm_model(master_data, "dlog_corelogicv4", ["name"], ["dlog_corelogicv4_us"])
        
        # Convert predictions to HPI
        forecast_data = self._convert_to_hpi(model_results, scenario)
        
        # Prepare output
        final_results = self._prepare_final_output(forecast_data, scenario)
        
        return final_results
    
    def _merge_national_data(self, national_data: pl.DataFrame, scenario_data: pl.DataFrame) -> pl.DataFrame:
        """Merge national historical with forecast"""
        # Ensure date columns exist and are proper dates
        if "date" not in scenario_data.columns:
            date_col = next((c for c in scenario_data.columns if 'date' in c.lower()), None)
            if date_col:
                scenario_data = scenario_data.rename({date_col: "date"})
        
        # Merge
        merged = national_data.join(
            scenario_data.select(["date", "corelogic_v4"]),
            on="date",
            how="outer"
        )
        
        # Fill HPI: use scenario forecast where historical is missing
        merged = merged.with_columns([
            pl.when(pl.col("hpi_sa").is_null())
            .then(pl.col("corelogic_v4"))
            .otherwise(pl.col("hpi_sa"))
            .alias("hpi_sa")
        ])
        
        # Calculate changes
        merged = merged.sort("date").with_columns([
            pl.col("hpi_sa").shift(4).alias("hpi_sa_lag4"),
            pl.col("hpi_sa").shift(1).alias("hpi_sa_lag1")
        ]).with_columns([
            ((pl.col("hpi_sa") / pl.col("hpi_sa_lag4")) - 1).alias("yoy_corelogicv4"),
            (pl.col("hpi_sa").log() - pl.col("hpi_sa_lag1").log()).alias("dlog_corelogicv4")
        ])
        
        return merged
    
    def _create_master_panel(self, regional_data: pl.DataFrame, national_data: pl.DataFrame) -> pl.DataFrame:
        """Create panel data structure"""
        # Get unique regions and dates
        regions = regional_data.select("code", "name").unique()
        dates = national_data.select("date").unique()
        
        # Create full panel
        regions = regions.with_columns(pl.lit(1).alias("key"))
        dates = dates.with_columns(pl.lit(1).alias("key"))
        panel = regions.join(dates, on="key").drop("key")
        
        # Add regional data
        panel = panel.join(
            regional_data.select(["code", "name", "date", "hpi_sa", "yoy_corelogicv4", "dlog_corelogicv4"]),
            on=["code", "name", "date"],
            how="left"
        )
        
        # Add national data
        panel = panel.join(
            national_data.select(["date", "yoy_corelogicv4", "dlog_corelogicv4"]).rename({
                "yoy_corelogicv4": "yoy_corelogicv4_us",
                "dlog_corelogicv4": "dlog_corelogicv4_us"
            }),
            on="date",
            how="left"
        )
        
        return panel.sort(["code", "date"])
    
    def _run_glm_model(self, data: pl.DataFrame, target_col: str, categorical_cols: list, continuous_cols: list) -> pl.DataFrame:
        """Run GLM model with proper train/test split"""
        # Split data
        train_data = data.filter(
            (pl.col("date") >= self.params.regression_start_date) &
            (pl.col("date") <= self.params.regression_end_date) &
            (pl.col(target_col).is_not_null()) &
            (pl.col(continuous_cols[0]).is_not_null())
        )
        
        forecast_data = data.filter(pl.col("date") >= self.params.forecast_start_date)
        
        print(f"Training data size: {len(train_data)}")
        print(f"Forecast data size: {len(forecast_data)}")
        
        if len(train_data) == 0:
            print("WARNING: No training data available. Returning data with null predictions.")
            return data.with_columns(pl.lit(None).cast(pl.Float64).alias("pred"))
        
        # Fit model
        model = HPIGLMModel(target_col, categorical_cols, continuous_cols)
        model.fit(train_data)
        
        # Predict on forecast period
        forecast_scored = model.score_data(forecast_data, "pred")
        
        # Combine with history
        history_data = data.filter(pl.col("date") < self.params.forecast_start_date).with_columns(
            pl.lit(None).cast(pl.Float64).alias("pred")
        )
        
        result = pl.concat([history_data, forecast_scored]).sort(["code", "date"])
        
        return result
    
    def _convert_to_hpi(self, model_results: pl.DataFrame, scenario: str) -> pl.DataFrame:
        """Convert log differences back to HPI levels"""
        hpi_col = f"hpipred_{scenario}"
        
        results_list = []
        for code_val in model_results["code"].unique():
            code_data = model_results.filter(pl.col("code") == code_val).sort("date")
            
            hpi_values = []
            current_hpi = None
            
            for row in code_data.iter_rows(named=True):
                if row["date"] <= self.params.last_history_date and row["hpi_sa"] is not None:
                    current_hpi = row["hpi_sa"]
                    hpi_values.append(current_hpi)
                elif row["pred"] is not None and current_hpi is not None:
                    current_hpi = current_hpi * np.exp(row["pred"])
                    hpi_values.append(current_hpi)
                else:
                    hpi_values.append(current_hpi if current_hpi else row.get("hpi_sa"))
            
            code_result = code_data.with_columns(pl.Series(name=hpi_col, values=hpi_values))
            results_list.append(code_result)
        
        return pl.concat(results_list)
    
    def _prepare_final_output(self, forecast_data: pl.DataFrame, scenario: str) -> Dict[str, pl.DataFrame]:
        """Prepare final output in multiple formats"""
        hpi_col = f"hpipred_{scenario}"
        
        # Quarterly output
        quarterly = forecast_data.select([
            pl.col("code").alias("cbsa_code"),
            pl.col("name").alias("cbsa_name"),
            "date",
            pl.col(hpi_col).alias("hpi")
        ]).sort(["cbsa_code", "date"])
        
        # Monthly (simple interpolation)
        monthly = self._convert_quarterly_to_monthly(quarterly)
        
        # Transposed versions
        quarterly_tr = quarterly.pivot(values="hpi", index="date", columns="cbsa_name")
        monthly_tr = monthly.pivot(values="hpi", index="date", columns="cbsa_name")
        
        return {
            "quarterly": quarterly,
            "monthly": monthly,
            "quarterly_transposed": quarterly_tr,
            "monthly_transposed": monthly_tr
        }
    
    def _convert_quarterly_to_monthly(self, quarterly_data: pl.DataFrame) -> pl.DataFrame:
        """Convert quarterly to monthly by repeating values"""
        monthly_rows = []
        
        for row in quarterly_data.iter_rows(named=True):
            q_date = row["date"]
            # Repeat quarterly value for each month in quarter
            if q_date.month == 3:
                months = [1, 2, 3]
            elif q_date.month == 6:
                months = [4, 5, 6]
            elif q_date.month == 9:
                months = [7, 8, 9]
            else:
                months = [10, 11, 12]
            
            for m in months:
                monthly_rows.append({
                    "cbsa_code": row["cbsa_code"],
                    "cbsa_name": row["cbsa_name"],
                    "date": date(q_date.year, m, 1),
                    "hpi": row["hpi"]
                })
        
        return pl.DataFrame(monthly_rows)


# FILE: src/gemini_scenario_models/hpi_projection_us/model.py
import polars as pl
from typing import Dict
import sys
sys.path.append('.')
from .data_preparation import DataPreparation
from .data_projection import DataProjection
from Utils.data_import_helpers import DataImportHelper

class GLMModelScenarioProjection:
    def __init__(self, params):
        self.params = params
        self.data_prep = DataPreparation(params)
        self.data_projection = DataProjection(params)
        self.data_helper = DataImportHelper(params.input_dir)
    
    def run_projections(self, input_data: Dict[str, pl.DataFrame]) -> Dict[str, Dict[str, pl.DataFrame]]:
        """Main execution method"""
        print("Starting data preparation...")
        state_data, metro_data, combined_data = self.data_prep.run_data_prep(
            input_data["clv4_state_extract"],
            input_data["clv4_msa_extract"],
            input_data["moodys_mapping"]
        )
        
        print(f"Combined data shape: {combined_data.shape}")
        
        # Split into national, state, metro
        national_data = combined_data.filter(pl.col("name") == "National")
        state_regional = combined_data.filter(
            (pl.col("code").cast(pl.Int64, strict=False) > 0) &
            (pl.col("code").cast(pl.Int64, strict=False) <= 100)
        )
        
        all_results = {}
        
        for scenario in self.params.scenarios:
            print(f"\nProcessing scenario: {scenario}")
            scenario_data = input_data[f"{scenario}_hpi_national"]
            
            # Run state forecast
            print(f"Running {scenario} state forecast...")
            state_results = self.data_projection.run_state_forecast(
                scenario, "state", national_data, scenario_data, state_regional
            )
            
            all_results[f"state_{scenario}"] = state_results
            
            # Export results
            self._export_results(state_results, f"state_{scenario}")
        
        return all_results
    
    def _export_results(self, results: Dict[str, pl.DataFrame], scenario_region: str) -> None:
        """Export results to Excel"""
        for data_type, data in results.items():
            if len(data) > 0:
                filename = f"CoreLogic_{scenario_region}_{data_type}.xlsx"
                try:
                    self.data_helper.export_excel(data, filename, scenario_region.split('_')[1])
                    print(f"Exported: {filename}")
                except Exception as e:
                    print(f"Error exporting {filename}: {e}")


# FILE: main.py
import polars as pl
from pathlib import Path
from datetime import date
import sys
sys.path.append('.')
from src.gemini_scenario_models.hpi_projection_us import GLMModelScenarioProjection
from src.gemini_scenario_models.hpi_projection_us.dependencies import Params
from Utils.data_import_helpers import DataImportHelper

def main():
    """Main execution function"""
    # Set up parameters
    params = Params(
        scenarios=["ce", "up", "dn", "dn2"],
        regions=["state", "metro"],
        input_dir="Input",
        output_dir="Output",
        last_history_date=date(2025, 3, 31),
        regression_start_date=date(2000, 3, 31),
        regression_end_date=date(2023, 6, 30),
        forecast_start_date=date(2025, 6, 30)
    )
    
    print("Initializing data helper...")
    data_helper = DataImportHelper(params.input_dir)
    
    print("Loading input data...")
    try:
        input_data = {
            "moodys_mapping": data_helper.import_excel("Basket_2016-10-5_13_45_V2.xlsx", "Mapping"),
            "clv4_state_extract": data_helper.import_excel("HPI Data by State.xlsx", "HPI Data by State"),
            "clv4_msa_extract": data_helper.import_excel("HPI Data by CBSA.xlsx", "HPI Data by CBSA"),
            "state_metro_map": data_helper.import_excel("state_metro_map.xlsx"),
            "ce_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_2025Q2.xlsx", "CE"),
            "up_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_2025Q2.xlsx", "UP"),
            "dn_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_2025Q2.xlsx", "DN"),
            "dn2_hpi_national": data_helper.import_excel("Data_Forecast_National_HPI_2025Q2.xlsx", "DN2"),
        }
        print("Data loaded successfully!")
        
        # Print sample data for verification
        print("\nSample Moodys Mapping:")
        print(input_data["moodys_mapping"].head())
        print("\nSample State Data:")
        print(input_data["clv4_state_extract"].head())
        print("\nSample National Forecast:")
        print(input_data["ce_hpi_national"].head())
        
    except Exception as e:
        print(f"Error loading data: {e}")
        return
    
    print("\nInitializing projection model...")
    projection_model = GLMModelScenarioProjection(params)
    
    print("Running projections...")
    try:
        results = projection_model.run_projections(input_data)
        print(f"\n✓ HPI Projection model completed successfully!")
        print(f"✓ Generated results for {len(results)} scenario-region combinations")
    except Exception as e:
        print(f"Error running projections: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


# FILE: tests/__init__.py
pass


# FILE: tests/hpi_projection_us/__init__.py
pass


# FILE: tests/hpi_projection_us/conftest.py
import pytest
import polars as pl
from datetime import date
import sys
sys.path.append('.')
from src.gemini_scenario_models.hpi_projection_us.dependencies import Params

@pytest.fixture
def sample_params():
    return Params(
        scenarios=["ce", "up"],
        regions=["state", "metro"],
        input_dir="test_input",
        output_dir="test_output",
        last_history_date=date(2025, 3, 31),
        regression_start_date=date(2000, 3, 31),
        regression_end_date=date(2023, 6, 30),
        forecast_start_date=date(2025, 6, 30)
    )

@pytest.fixture
def sample_state_data():
    return pl.DataFrame({
        "State Name": ["California", "Texas", "New York"] * 12,
        "Year": [2023] * 36,
        "Month": ["January", "February", "March", "April", "May", "June",
                  "July", "August", "September", "October", "November", "December"] * 3,
        "Home Price Index": [100.5, 101.0, 101.5, 102.0, 102.5, 103.0,
                            103.5, 104.0, 104.5, 105.0, 105.5, 106.0] * 3
    })

@pytest.fixture
def sample_moodys_mapping():
    return pl.DataFrame({
        "Geography": ["CALIFORNIA", "TEXAS", "NEW YORK", "United States"],
        "FIP": ["06", "48", "36", "00"],
        "GeoCode": ["CA", "TX", "NY", "US"]
    })

@pytest.fixture
def sample_national_data():
    return pl.DataFrame({
        "date": [date(2023, 3, 31), date(2023, 6, 30), date(2023, 9, 30), date(2023, 12, 31)],
        "CORELOGIC_V4": [100.0, 101.5, 103.2, 105.0]
    })


# FILE: tests/hpi_projection_us/test_data_preparation.py
import pytest
import polars as pl
from datetime import date
import sys
sys.path.append('.')
from src.gemini_scenario_models.hpi_projection_us.data_preparation import DataPreparation

class TestDataPreparation:
    def test_initialization(self, sample_params):
        data_prep = DataPreparation(sample_params)
        assert data_prep.params == sample_params
    
    def test_prepare_state_data(self, sample_params, sample_state_data, sample_moodys_mapping):
        data_prep = DataPreparation(sample_params)
        result = data_prep._prepare_state_data(sample_state_data, sample_moodys_mapping)
        
        assert len(result) > 0
        assert "yyyymm" in result.columns
        print(f"Prepared state data: {result.shape}")
    
    def test_combine_data(self, sample_params, sample_state_data, sample_moodys_mapping):
        data_prep = DataPreparation(sample_params)
        
        state_data = data_prep._prepare_state_data(sample_state_data, sample_moodys_mapping)
        metro_data = pl.DataFrame({
            "CBSA Name": ["Los Angeles"] * 3,
            "Year": [2023] * 3,
            "Month": ["January", "February", "March"],
            "Home Price Index": [120.0, 121.0, 122.0]
        })
        metro_data = data_prep._prepare_metro_data(metro_data, sample_moodys_mapping)
        
        result = data_prep._combine_data(state_data, metro_data)
        
        assert "code" in result.columns
        assert "name" in result.columns
        assert "date" in result.columns
        assert len(result) > 0
        print(f"Combined data shape: {result.shape}")


# FILE: tests/hpi_projection_us/test_data_projection.py
import pytest
import polars as pl
from datetime import date
import sys
sys.path.append('.')
from src.gemini_scenario_models.hpi_projection_us.data_projection import DataProjection

class TestDataProjection:
    def test_initialization(self, sample_params):
        data_proj = DataProjection(sample_params)
        assert data_proj.params == sample_params
    
    def test_merge_national_data(self, sample_params, sample_national_data):
        data_proj = DataProjection(sample_params)
        
        national_hist = pl.DataFrame({
            "name": ["National"] * 4,
            "date": [date(2023, 3, 31), date(2023, 6, 30), date(2023, 9, 30), date(2023, 12, 31)],
            "hpi_sa": [100.0, 101.0, 102.0, 103.0]
        })
        
        result = data_proj._merge_national_data(national_hist, sample_national_data)
        
        assert len(result) >= len(national_hist)
        print(f"Merged national data: {result.shape}")
    
    def test_convert_quarterly_to_monthly(self, sample_params):
        data_proj = DataProjection(sample_params)
        
        quarterly = pl.DataFrame({
            "cbsa_code": ["06", "48"],
            "cbsa_name": ["California", "Texas"],
            "date": [date(2023, 3, 31), date(2023, 6, 30)],
            "hpi": [100.0, 101.5]
        })
        
        result = data_proj._convert_quarterly_to_monthly(quarterly)
        
        assert len(result) > len(quarterly)
        print(f"Monthly conversion: {len(quarterly)} quarters -> {len(result)} months")


# FILE: tests/hpi_projection_us/test_model.py
import pytest
import polars as pl
from datetime import date
import sys
sys.path.append('.')
from src.gemini_scenario_models.hpi_projection_us.model import GLMModelScenarioProjection

class TestGLMModel:
    def test_initialization(self, sample_params):
        model = GLMModelScenarioProjection(sample_params)
        assert model.params == sample_params
        assert model.data_prep is not None
        assert model.data_projection is not None
    
    def test_data_pipeline(self, sample_params, sample_state_data, sample_moodys_mapping):
        model = GLMModelScenarioProjection(sample_params)
        
        metro_data = pl.DataFrame({
            "CBSA Name": ["Los Angeles"] * 3,
            "Year": [2023] * 3,
            "Month": ["January", "February", "March"],
            "Home Price Index": [120.0, 121.0, 122.0]
        })
        
        state_data, metro_result, combined = model.data_prep.run_data_prep(
            sample_state_data,
            metro_data,
            sample_moodys_mapping
        )
        
        assert len(state_data) > 0
        assert len(combined) > 0
        print(f"Pipeline test passed: {combined.shape}")


# FILE: pyproject.toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "hpi-projection-models"
version = "1.0.0"
description = "GLM-based HPI projection models for scenario analysis"
requires-python = ">=3.9"
dependencies = [
    "polars>=0.20.0",
    "scikit-learn>=1.3.0",
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "openpyxl>=3.1.0",
    "xlsxwriter>=3.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "flake8>=6.0.0",
]

[tool.setuptools.packages.find]
where = ["src", "."]
include = ["src*", "Utils*"]

[tool.black]
line-length = 100
target-version = ['py39']

[tool.isort]
profile = "black"
line_length = 100

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "-v --tb=short"


# FILE: requirements.txt
polars>=0.20.0
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0
openpyxl>=3.1.0
xlsxwriter>=3.1.0
pytest>=7.4.0
pytest-cov>=4.1.0


# FILE: README.md
# HPI Projection Microservices

GLM-based House Price Index (HPI) projection models for scenario analysis.

## Project Structure

```
project_root/
├── Input/                          # Input data files
├── Output/                         # Output results
├── Utils/                          # Shared utilities (cross-project)
│   ├── data_import_helpers.py     # Data I/O helpers
│   └── glm_model_architecture.py  # GLM model base classes
├── src/                           # Source code
│   └── gemini_scenario_models/
│       └── hpi_projection_us/     # HPI projection module
│           ├── __init__.py
│           ├── model.py           # Main model orchestrator
│           ├── data_preparation.py
│           ├── data_projection.py
│           └── dependencies/
│               ├── __init__.py
│               ├── _constants.py
│               └── _parameters.py
├── tests/                         # Unit tests
│   └── hpi_projection_us/
├── main.py                        # Entry point
├── pyproject.toml                 # Project configuration
└── requirements.txt               # Dependencies
```

## Key Features

- ✓ Proper date handling with month name conversion
- ✓ GLM modeling with categorical/continuous interactions
- ✓ Quarterly to monthly interpolation
- ✓ Multi-scenario forecasting (CE, UP, DN, DN2)
- ✓ State and Metro level projections
- ✓ Comprehensive test coverage

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

## Running Tests

```bash
pytest tests/ -v
```

## Data Requirements

Place the following files in the `Input/` directory:
- `Basket_2016-10-5_13_45_V2.xlsx`
- `HPI Data by State.xlsx`
- `HPI Data by CBSA.xlsx`
- `state_metro_map.xlsx`
- `Data_Forecast_National_HPI_2025Q2.xlsx`

## Key Changes from SAS

1. **Date Handling**: Month names converted to numbers using mapping dictionary
2. **Panel Data**: Full cross-join of regions and dates before merging
3. **GLM Model**: Interaction terms between categorical (region) and continuous (national HPI change)
4. **Null Handling**: Explicit filtering before model training
5. **HPI Reconstruction**: Exponential transformation of log differences

## Notes

- Utils folder is at project root for reusability across projects
- Module imports use relative paths with sys.path adjustments
- All date columns are properly typed as pl.Date
- Training data filtering ensures non-null values for model fitting