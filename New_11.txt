import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import make_pipeline
import seaborn as sns
from scipy import stats

np.random.seed(42)

def generate_data(n=200, noise_level=0.3):
    X = np.random.uniform(-2, 2, n)
    y = 1.5 * X**3 - 2 * X**2 + 0.5 * X + 1 + np.random.normal(0, noise_level, n)
    return X.reshape(-1, 1), y

def bias_variance_decomposition(X_train, y_train, X_test, y_test, degree, n_trials=100):
    predictions = []
    
    for _ in range(n_trials):
        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)
        X_bootstrap = X_train[indices]
        y_bootstrap = y_train[indices]
        
        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
        model.fit(X_bootstrap, y_bootstrap)
        pred = model.predict(X_test)
        predictions.append(pred)
    
    predictions = np.array(predictions)
    mean_prediction = np.mean(predictions, axis=0)
    
    bias_squared = np.mean((mean_prediction - y_test) ** 2)
    variance = np.mean(np.var(predictions, axis=0))
    noise = np.var(y_test - np.mean(y_test))
    
    return bias_squared, variance, noise

def evaluate_polynomial_degrees():
    X, y = generate_data(n=300, noise_level=0.4)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    degrees = [2, 3, 4, 5, 6]
    results = {}
    models = {}
    
    plt.figure(figsize=(20, 24))
    
    for i, degree in enumerate(degrees):
        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
        model.fit(X_train, y_train)
        
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_mae = mean_absolute_error(y_train, y_train_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)
        
        bias_sq, variance, noise = bias_variance_decomposition(X_train, y_train, X_test, y_test, degree)
        
        results[degree] = {
            'train_mse': train_mse, 'test_mse': test_mse,
            'train_r2': train_r2, 'test_r2': test_r2,
            'train_mae': train_mae, 'test_mae': test_mae,
            'bias_squared': bias_sq, 'variance': variance, 'noise': noise,
            'total_error': bias_sq + variance + noise
        }
        models[degree] = model
        
        plt.subplot(6, 3, i*3 + 1)
        X_plot = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
        y_plot = model.predict(X_plot)
        
        plt.scatter(X_train, y_train, alpha=0.5, color='blue', s=30, label='Train')
        plt.scatter(X_test, y_test, alpha=0.7, color='red', s=30, label='Test')
        plt.plot(X_plot, y_plot, color='green', linewidth=2, label=f'Degree {degree}')
        plt.title(f'Polynomial Degree {degree}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(6, 3, i*3 + 2)
        residuals_train = y_train - y_train_pred
        residuals_test = y_test - y_test_pred
        plt.scatter(y_train_pred, residuals_train, alpha=0.6, color='blue', label='Train')
        plt.scatter(y_test_pred, residuals_test, alpha=0.6, color='red', label='Test')
        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        plt.title(f'Residuals - Degree {degree}')
        plt.xlabel('Predicted')
        plt.ylabel('Residuals')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(6, 3, i*3 + 3)
        plt.bar(['Bias²', 'Variance', 'Noise'], [bias_sq, variance, noise], 
                color=['orange', 'purple', 'gray'], alpha=0.7)
        plt.title(f'Bias-Variance Decomp. (Degree {degree})')
        plt.ylabel('Error Components')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    plt.figure(figsize=(18, 12))
    
    plt.subplot(2, 3, 1)
    train_mses = [results[d]['train_mse'] for d in degrees]
    test_mses = [results[d]['test_mse'] for d in degrees]
    plt.plot(degrees, train_mses, 'bo-', label='Training MSE', linewidth=2, markersize=8)
    plt.plot(degrees, test_mses, 'ro-', label='Testing MSE', linewidth=2, markersize=8)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Mean Squared Error')
    plt.title('Training vs Testing MSE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.subplot(2, 3, 2)
    train_r2s = [results[d]['train_r2'] for d in degrees]
    test_r2s = [results[d]['test_r2'] for d in degrees]
    plt.plot(degrees, train_r2s, 'bo-', label='Training R²', linewidth=2, markersize=8)
    plt.plot(degrees, test_r2s, 'ro-', label='Testing R²', linewidth=2, markersize=8)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('R² Score')
    plt.title('Training vs Testing R²')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.subplot(2, 3, 3)
    biases = [results[d]['bias_squared'] for d in degrees]
    variances = [results[d]['variance'] for d in degrees]
    total_errors = [results[d]['total_error'] for d in degrees]
    plt.plot(degrees, biases, 'go-', label='Bias²', linewidth=2, markersize=8)
    plt.plot(degrees, variances, 'mo-', label='Variance', linewidth=2, markersize=8)
    plt.plot(degrees, total_errors, 'ko-', label='Total Error', linewidth=2, markersize=8)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Error')
    plt.title('Bias-Variance Trade-off')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.subplot(2, 3, 4)
    overfitting_gap = np.array(test_mses) - np.array(train_mses)
    plt.bar(degrees, overfitting_gap, color='coral', alpha=0.7)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Test MSE - Train MSE')
    plt.title('Overfitting Gap')
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.subplot(2, 3, 5)
    train_maes = [results[d]['train_mae'] for d in degrees]
    test_maes = [results[d]['test_mae'] for d in degrees]
    plt.plot(degrees, train_maes, 'co-', label='Training MAE', linewidth=2, markersize=8)
    plt.plot(degrees, test_maes, 'yo-', label='Testing MAE', linewidth=2, markersize=8)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Mean Absolute Error')
    plt.title('Training vs Testing MAE')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.subplot(2, 3, 6)
    complexity_penalty = np.array(test_mses) + 0.1 * np.array(variances)
    plt.bar(degrees, complexity_penalty, color='lightblue', alpha=0.7)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Penalized Error')
    plt.title('Complexity-Penalized Error')
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.tight_layout()
    plt.show()
    
    results_df = []
    for degree in degrees:
        r = results[degree]
        results_df.append([
            degree, r['train_mse'], r['test_mse'], r['train_r2'], r['test_r2'],
            r['train_mae'], r['test_mae'], r['bias_squared'], r['variance'], r['total_error']
        ])
    
    import pandas as pd
    df = pd.DataFrame(results_df, columns=[
        'Degree', 'Train_MSE', 'Test_MSE', 'Train_R2', 'Test_R2', 
        'Train_MAE', 'Test_MAE', 'Bias²', 'Variance', 'Total_Error'
    ])
    
    print("=== POLYNOMIAL COMPARISON RESULTS ===")
    print(df.round(4))
    
    best_test_mse = df.loc[df['Test_MSE'].idxmin()]
    best_r2 = df.loc[df['Test_R2'].idxmax()]
    best_bias_variance = df.loc[df['Total_Error'].idxmin()]
    
    print(f"\nBest by Test MSE: Degree {int(best_test_mse['Degree'])} (MSE: {best_test_mse['Test_MSE']:.4f})")
    print(f"Best by Test R²: Degree {int(best_r2['Degree'])} (R²: {best_r2['Test_R2']:.4f})")
    print(f"Best by Bias-Variance: Degree {int(best_bias_variance['Degree'])} (Total Error: {best_bias_variance['Total_Error']:.4f})")
    
    learning_curves(X, y, degrees)
    
    return results, models

def learning_curves(X, y, degrees):
    plt.figure(figsize=(15, 10))
    
    train_sizes = np.linspace(0.1, 1.0, 10)
    
    for i, degree in enumerate(degrees):
        plt.subplot(2, 3, i+1)
        
        train_scores = []
        test_scores = []
        
        for train_size in train_sizes:
            n_samples = int(train_size * len(X))
            scores_train = []
            scores_test = []
            
            for _ in range(20):
                X_temp, _, y_temp, _ = train_test_split(X, y, train_size=train_size, random_state=np.random.randint(1000))
                X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=np.random.randint(1000))
                
                model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
                model.fit(X_train, y_train)
                
                scores_train.append(r2_score(y_train, model.predict(X_train)))
                scores_test.append(r2_score(y_test, model.predict(X_test)))
            
            train_scores.append(np.mean(scores_train))
            test_scores.append(np.mean(scores_test))
        
        sample_counts = [int(size * len(X)) for size in train_sizes]
        plt.plot(sample_counts, train_scores, 'bo-', label='Training Score', markersize=6)
        plt.plot(sample_counts, test_scores, 'ro-', label='Validation Score', markersize=6)
        plt.title(f'Learning Curve - Degree {degree}')
        plt.xlabel('Training Set Size')
        plt.ylabel('R² Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def cross_validation_analysis(X, y, degrees, cv_folds=5):
    from sklearn.model_selection import cross_val_score
    
    cv_results = {}
    
    for degree in degrees:
        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
        scores = cross_val_score(model, X, y, cv=cv_folds, scoring='neg_mean_squared_error')
        cv_results[degree] = -scores
    
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    box_data = [cv_results[degree] for degree in degrees]
    plt.boxplot(box_data, labels=degrees)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Cross-Validation MSE')
    plt.title('Cross-Validation MSE Distribution')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    cv_means = [np.mean(cv_results[degree]) for degree in degrees]
    cv_stds = [np.std(cv_results[degree]) for degree in degrees]
    plt.errorbar(degrees, cv_means, yerr=cv_stds, marker='o', capsize=5, capthick=2, linewidth=2, markersize=8)
    plt.xlabel('Polynomial Degree')
    plt.ylabel('Cross-Validation MSE')
    plt.title('Cross-Validation MSE with Error Bars')
    plt.grid(True, alpha=0.3)
    plt.xticks(degrees)
    
    plt.tight_layout()
    plt.show()
    
    print("\n=== CROSS-VALIDATION RESULTS ===")
    for degree in degrees:
        mean_score = np.mean(cv_results[degree])
        std_score = np.std(cv_results[degree])
        print(f"Degree {degree}: CV MSE = {mean_score:.4f} ± {std_score:.4f}")

if __name__ == "__main__":
    print("Starting Polynomial Comparison Analysis...")
    X, y = generate_data(n=300, noise_level=0.4)
    results, models = evaluate_polynomial_degrees()
    cross_validation_analysis(X, y, [2, 3, 4, 5, 6])