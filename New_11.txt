# utils/__init__.py
from .excel_utils import ExcelReader, ExcelWriter
from .data_utils import DataProcessor

__all__ = ["ExcelReader", "ExcelWriter", "DataProcessor"]


# utils/excel_utils.py
import polars as pl
from openpyxl import load_workbook, Workbook
from openpyxl.styles import Font
from datetime import datetime
from typing import Dict, List, Tuple


class ExcelReader:
    
    @staticmethod
    def read_sheet(file_path: str, sheet_name: str, header_row: int = 2) -> Tuple[pl.DataFrame, List[str]]:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        headers = []
        date_columns = []
        date_indices = []
        
        header_row_data = list(ws[header_row])
        
        for idx, cell in enumerate(header_row_data):
            if cell.value:
                if isinstance(cell.value, datetime):
                    col_name = cell.value.strftime('%d-%b-%y')
                    date_columns.append(col_name)
                    date_indices.append(idx)
                    headers.append(col_name)
                elif isinstance(cell.value, str) and any(char.isdigit() for char in str(cell.value)):
                    try:
                        col_name = str(cell.value).strip()
                        date_columns.append(col_name)
                        date_indices.append(idx)
                        headers.append(col_name)
                    except:
                        col_name = str(cell.value).replace(' ', '_').replace('.', '')
                        headers.append(col_name)
                else:
                    col_name = str(cell.value).replace(' ', '_').replace('.', '')
                    headers.append(col_name)
        
        data = []
        for row in ws.iter_rows(min_row=header_row + 1, values_only=True):
            if row and row[0] is not None:
                row_dict = {}
                for col_idx in range(min(len(headers), len(row))):
                    header = headers[col_idx]
                    value = row[col_idx]
                    
                    if value is None:
                        value = 0.0 if col_idx in date_indices else ""
                    elif col_idx in date_indices:
                        if isinstance(value, (int, float)):
                            value = float(value)
                        else:
                            try:
                                value = float(str(value).replace(',', ''))
                            except:
                                value = 0.0
                    else:
                        value = str(value) if value is not None else ""
                    row_dict[header] = value
                data.append(row_dict)
        
        wb.close()
        
        if not data:
            return pl.DataFrame(), date_columns
            
        df = pl.DataFrame(data)
        
        for date_col in date_columns:
            if date_col in df.columns:
                df = df.with_columns(pl.col(date_col).cast(pl.Float64))
        
        return df, date_columns
    
    @staticmethod
    def read_assumptions(file_path: str, sheet_name: str) -> pl.DataFrame:
        wb = load_workbook(file_path, data_only=True)
        ws = wb[sheet_name]
        
        data = []
        for row in ws.iter_rows(min_row=3, values_only=True):
            if row[1] is not None:
                desc = str(row[1]).strip()
                us_ilm = row[2] if len(row) > 2 and row[2] else 0
                ilm = row[3] if len(row) > 3 and row[3] else 0
                
                if isinstance(us_ilm, str):
                    us_ilm = float(us_ilm.rstrip('%')) / 100.0 if '%' in us_ilm else float(us_ilm)
                else:
                    us_ilm = float(us_ilm) if us_ilm else 0.0
                
                if isinstance(ilm, str):
                    ilm = float(ilm.rstrip('%')) / 100.0 if '%' in ilm else float(ilm)
                else:
                    ilm = float(ilm) if ilm else 0.0
                
                data.append({
                    'Description': desc,
                    'US_ILM': us_ilm,
                    'ILM': ilm
                })
        
        wb.close()
        return pl.DataFrame(data)


class ExcelWriter:
    
    @staticmethod
    def write_output(file_path: str, sheet_name: str, data_dict: Dict[str, pl.DataFrame], 
                     date_columns: List[str], bold_rows: List[str] = None):
        wb = load_workbook(file_path)
        
        if sheet_name in wb.sheetnames:
            del wb[sheet_name]
        
        ws = wb.create_sheet(sheet_name)
        
        for i, col in enumerate(date_columns):
            cell = ws.cell(row=2, column=i+3)
            cell.value = col
            cell.font = Font(bold=True)
        
        row_num = 3
        
        for section_name, section_df in data_dict.items():
            for category in section_df.columns:
                if category != 'Date':
                    ws.cell(row=row_num, column=2, value=category)
                    
                    for col_idx, date_col in enumerate(date_columns):
                        value = 0.0
                        try:
                            filtered = section_df.filter(pl.col('Date') == date_col)
                            if len(filtered) > 0:
                                value = filtered[category][0]
                        except:
                            value = 0.0
                        
                        ws.cell(row=row_num, column=col_idx+3, value=value)
                    
                    if bold_rows and category in bold_rows:
                        for c in range(2, len(date_columns)+3):
                            ws.cell(row=row_num, column=c).font = Font(bold=True)
                    
                    row_num += 1
            
            if section_name != list(data_dict.keys())[-1]:
                row_num += 1
        
        wb.save(file_path)


# utils/data_utils.py
import polars as pl
from typing import List


class DataProcessor:
    
    @staticmethod
    def get_value_by_key(df: pl.DataFrame, key_column: str, key_value: str, value_column: str) -> float:
        try:
            key_value = key_value.strip()
            result = df.filter(pl.col(key_column).str.strip_chars() == key_value)
            if len(result) > 0:
                return float(result[value_column][0])
            return 0.0
        except:
            return 0.0
    
    @staticmethod
    def calculate_row_range_diff(df: pl.DataFrame, row_indices: List[int], 
                                 date_col: str, base_col: str) -> float:
        try:
            if not row_indices:
                return 0.0
            total_date = 0.0
            total_base = 0.0
            for idx in row_indices:
                if idx < len(df):
                    row = df.row(idx, named=True)
                    if date_col in row:
                        total_date += float(row[date_col] or 0)
                    if base_col in row:
                        total_base += float(row[base_col] or 0)
            return total_date - total_base
        except:
            return 0.0
    
    @staticmethod
    def calculate_sumifs(df: pl.DataFrame, date_col: str, base_col: str,
                        filter_column: str, filter_value: str) -> float:
        try:
            filtered = df.filter(pl.col(filter_column).str.strip_chars() == filter_value.strip())
            if len(filtered) > 0:
                date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
                base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
                return float(date_sum or 0) - float(base_sum or 0)
            return 0.0
        except:
            return 0.0
    
    @staticmethod
    def calculate_sumifs_multi(df: pl.DataFrame, date_col: str, base_col: str,
                              filter_column1: str, filter_value1: str,
                              filter_column2: str, filter_value2: str) -> float:
        try:
            filtered = df.filter(
                (pl.col(filter_column1).str.strip_chars() == filter_value1.strip()) &
                (pl.col(filter_column2).str.strip_chars() == filter_value2.strip())
            )
            if len(filtered) > 0:
                date_sum = filtered.select(pl.col(date_col).sum())[0, 0] if date_col in filtered.columns else 0
                base_sum = filtered.select(pl.col(base_col).sum())[0, 0] if base_col in filtered.columns else 0
                return float(date_sum or 0) - float(base_sum or 0)
            return 0.0
        except:
            return 0.0


# src/gemini_liquidity_models/__init__.py
from .liquidity_impact_calculation import LiquidityImpactCalculation

__all__ = ["LiquidityImpactCalculation"]


# src/gemini_liquidity_models/liquidity_impact_calculation/__init__.py
from .model import LiquidityImpactCalculation
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig

__all__ = ["LiquidityImpactCalculation", "LiquidityCalculator", "CalculationConfig"]


# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_config.py
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class CalculationConfig:
    asset_row_mappings: Dict[str, List[int]]
    liability_row_mappings: Dict[str, List[int]]
    facility_row_mappings: Dict[str, List[int]]
    assumption_mappings: Dict[str, str]
    
    @classmethod
    def get_default_config(cls):
        return cls(
            asset_row_mappings={
                'IWPB Premier': list(range(3, 19)),
                'IWPB Private Banking': list(range(19, 23)),
                'CIB Loans': list(range(23, 45)),
                'UST HTM': [52],
                'Level 1 MBS HTM': [51],
                'Level 1 Other HTM': [53],
                'Level 2A MBS - HTM': [55],
                'Level 2A Other HTM': [56],
                'UST AFS': [61],
                'Level 1 MBS AFS': [60],
                'Level 1 Other - AFS': [62],
                'Level 2A- MBS - AFS': [64],
                'Level 2A- Other AFS': [65],
                'Liquid Equities': [84],
                'Illiquid Trading Assets': [88],
                'MSS - Loans': [92]
            },
            liability_row_mappings={
                'IWPB - Premier': [98],
                'PB - Personal': [106],
                'PB Commercial - Financial': [111],
                'PB Commercial - Non Financial': [116],
                'PB - Other': [121],
                'SME': [200],
                'Other (GPS)': [206],
                'Brokered Committed': [211],
                'Brokered Uncommitted': [212],
                'ISV': [213],
                'Innovation Banking': [216],
                'Other (CIB)': [219],
                'Structured CDs': [128],
                'Wholesale CDs': [137],
                'Equity': [223]
            },
            facility_row_mappings={
                'Mortgage commitments': [248],
                'Retail commitments': [251]
            },
            assumption_mappings={
                'UST HTM': 'UST HTM',
                'Level 1 MBS HTM': 'Level 1 MBS HTM',
                'Level 1 Other HTM': 'Level 1 Other HTM',
                'Level 2A MBS - HTM': 'Level 2A MBS HTM',
                'Level 2A Other HTM': 'Level 2A Other HTM',
                'UST AFS': 'UST AFS',
                'Level 1 MBS AFS': 'Level 1 MBS AFS',
                'Level 1 Other - AFS': 'Level 1 Other AFS',
                'Level 2A- MBS - AFS': 'Level 2A MBS AFS',
                'Level 2A- Other AFS': 'Level 2A Other AFS',
                'Liquid Equities': 'Equities',
                'IWPB - Premier': 'IWPB Premier',
                'PB - Personal': 'PB Personal',
                'PB Commercial - Financial': 'PB Commercial Financial',
                'PB Commercial - Non Financial': 'PB Commercial Non Financial',
                'PB - Other': 'PB Other',
                'SME': 'SME',
                'Other (GPS)': 'Other (GPS)',
                'Brokered Committed': 'Brokered Committed',
                'Brokered Uncommitted': 'Brokered Uncommitted',
                'ISV': 'ISV',
                'Innovation Banking': 'Innovation Banking',
                'Other (CIB)': 'Other (CIB)',
                'Corp Operational': 'Corp Operational',
                'Corp Non Operational': 'Corp Non Operational',
                'NBFI Operational': 'NBFI Operational',
                'NBFI Non Operational': 'NBFI Non Operational',
                'Banks Operational': 'Banks Operational',
                'Banks Non Operational': 'Banks Non Operational',
                'Credit': 'Credit',
                'Liquidity': 'Liquidity',
                'Mortgage commitments': 'Mortgage commitments',
                'Retail commitments': 'Retail commitments'
            }
        )


# src/gemini_liquidity_models/liquidity_impact_calculation/calculation_engine.py
import polars as pl
from typing import Dict, List
from utils.data_utils import DataProcessor


class LiquidityCalculator:
    
    def __init__(self, config: 'CalculationConfig'):
        self.config = config
        self.processor = DataProcessor()
    
    def calculate_impacts(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                         date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        asset_impact = self._calculate_asset_impact(liquidity_df, assumptions_df, date_columns, base_date)
        liability_impact = self._calculate_liability_impact(liquidity_df, assumptions_df, date_columns, base_date)
        facility_impact = self._calculate_facility_impact(liquidity_df, assumptions_df, date_columns, base_date)
        us_ilm_summary = self._calculate_us_ilm_summary(asset_impact, liability_impact, facility_impact, date_columns)
        
        return {
            'asset': asset_impact,
            'liability': liability_impact,
            'facility': facility_impact,
            'summary': us_ilm_summary
        }
    
    def _get_assumption(self, assumptions_df: pl.DataFrame, key: str) -> float:
        mapped_key = self.config.assumption_mappings.get(key, key)
        return self.processor.get_value_by_key(assumptions_df, 'Description', mapped_key, 'US_ILM')
    
    def _calculate_asset_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.asset_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['IWPB Premier', 'IWPB Private Banking', 'CIB Loans', 
                                'Illiquid Trading Assets', 'MSS - Loans']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = diff * -assumption_val
            
            row_data['Illiquid'] = 0.0
            row_data['Liquid Trading Assets'] = 0.0
            
            asset_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Asset Impact'] = asset_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_liability_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            for item_name, row_indices in self.config.liability_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                
                if item_name in ['Structured CDs', 'Wholesale CDs', 'Equity', 
                                'Brokered Uncommitted', 'ISV', 'Innovation Banking', 'Other (CIB)']:
                    row_data[item_name] = -diff
                else:
                    assumption_val = self._get_assumption(assumptions_df, item_name)
                    row_data[item_name] = -diff * (1 - assumption_val)
            
            row_data['Affiliate'] = 0.0
            row_data['Corp'] = 0.0
            
            row_data['Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp Operational'))
            
            row_data['Non-Operational'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Corp', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Corp Non Operational'))
            
            row_data['NBFI'] = 0.0
            
            row_data['Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI Operational'))
            
            row_data['Non-Operational_NBFI'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'NBFI', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'NBFI Non Operational'))
            
            row_data['Banks'] = 0.0
            
            row_data['Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks Operational'))
            
            row_data['Non-Operational_Banks'] = self.processor.calculate_sumifs_multi(
                liquidity_df, date_col, base_date, 'Business', 'Banks', 'Level_3', 'Non-Operational'
            ) * -(1 - self._get_assumption(assumptions_df, 'Banks Non Operational'))
            
            liability_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Liability Impact'] = liability_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_facility_impact(self, liquidity_df: pl.DataFrame, assumptions_df: pl.DataFrame,
                                   date_columns: List[str], base_date: str) -> pl.DataFrame:
        results = []
        
        for date_col in date_columns:
            row_data = {'Date': date_col}
            
            credit_assumption = self._get_assumption(assumptions_df, 'Credit')
            liquidity_assumption = self._get_assumption(assumptions_df, 'Liquidity')
            
            row_data['Credit'] = self.processor.calculate_sumifs(
                liquidity_df, date_col, base_date, 'Level_2', 'Credit'
            ) * -credit_assumption
            
            row_data['Liquidity'] = self.processor.calculate_sumifs(
                liquidity_df, date_col, base_date, 'Level_2', 'Liquidity'
            ) * -liquidity_assumption
            
            for item_name, row_indices in self.config.facility_row_mappings.items():
                diff = self.processor.calculate_row_range_diff(liquidity_df, row_indices, date_col, base_date)
                assumption_val = self._get_assumption(assumptions_df, item_name)
                row_data[item_name] = diff * -assumption_val
            
            facility_total = sum(v for k, v in row_data.items() if k != 'Date')
            row_data['Comitted Facility Impact'] = facility_total
            results.append(row_data)
        
        return pl.DataFrame(results)
    
    def _calculate_us_ilm_summary(self, asset_df: pl.DataFrame, liability_df: pl.DataFrame,
                                  facility_df: pl.DataFrame, date_columns: List[str]) -> pl.DataFrame:
        results = []
        cumulative = 0.0
        
        for date_col in date_columns:
            asset_val = asset_df.filter(pl.col('Date') == date_col).select('Asset Impact')[0, 0]
            liability_val = liability_df.filter(pl.col('Date') == date_col).select('Liability Impact')[0, 0]
            facility_val = facility_df.filter(pl.col('Date') == date_col).select('Comitted Facility Impact')[0, 0]
            
            cumulative += asset_val + liability_val + facility_val
            results.append({'Date': date_col, 'US ILM December': cumulative})
        
        return pl.DataFrame(results)


# src/gemini_liquidity_models/liquidity_impact_calculation/model.py
import polars as pl
from typing import Dict, List, Optional
from .calculation_engine import LiquidityCalculator
from .calculation_config import CalculationConfig


class LiquidityImpactCalculation:
    
    def __init__(self, config: Optional[CalculationConfig] = None):
        self.config = config or CalculationConfig.get_default_config()
        self.calculator = LiquidityCalculator(self.config)
    
    def calculate(self, liquidity_data: pl.DataFrame, assumptions_data: pl.DataFrame,
                 date_columns: List[str], base_date: str) -> Dict[str, pl.DataFrame]:
        
        return self.calculator.calculate_impacts(
            liquidity_data, assumptions_data, date_columns, base_date
        )
    
    def get_formatted_output(self, calculation_results: Dict[str, pl.DataFrame]) -> Dict[str, pl.DataFrame]:
        return {
            'Assets': calculation_results['asset'],
            'Liabilities': calculation_results['liability'],
            'Committed Facilities': calculation_results['facility'],
            'US ILM Summary': calculation_results['summary']
        }


# main.py
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from src.gemini_liquidity_models.liquidity_impact_calculation import LiquidityImpactCalculation
from utils import ExcelReader, ExcelWriter


def main():
    loc_link = r"path/to/your/excel/file.xlsx"
    
    reader = ExcelReader()
    
    liquidity_data, date_columns = reader.read_sheet(
        file_path=loc_link,
        sheet_name="Liquidity Input",
        header_row=2
    )
    
    assumptions_data = reader.read_assumptions(
        file_path=loc_link,
        sheet_name="US ILM ILM Assumptions"
    )
    
    print(f"Loaded {len(liquidity_data)} rows from Liquidity Input")
    print(f"Found {len(date_columns)} date columns: {date_columns[:5]}...")
    print(f"Loaded {len(assumptions_data)} assumptions")
    
    model = LiquidityImpactCalculation()
    
    results = model.calculate(
        liquidity_data=liquidity_data,
        assumptions_data=assumptions_data,
        date_columns=date_columns,
        base_date='31-Dec-24'
    )
    
    formatted_results = model.get_formatted_output(results)
    
    writer = ExcelWriter()
    writer.write_output(
        file_path=loc_link,
        sheet_name="US ILM + ILM",
        data_dict=formatted_results,
        date_columns=date_columns,
        bold_rows=['Asset Impact', 'Liability Impact', 'Comitted Facility Impact', 'US ILM December']
    )
    
    print(f"\nCalculation completed. Results saved to sheet 'US ILM + ILM'")


if __name__ == "__main__":
    main()