# ==== pyproject.toml ====
[build-system]
requires = ["setuptools>=45", "wheel", "setuptools_scm[toml]>=6.2"]
build-backend = "setuptools.build_meta"

[project]
name = "hpi-projection-us"
version = "0.1.0"
description = "HPI Forecasting Model"
requires-python = ">=3.9"
dependencies = [
    "polars>=0.20.0",
    "openpyxl>=3.1.0",
    "scikit-learn>=1.3.0",
    "numpy>=1.24.0",
    "pandera>=0.17.0",
    "statsmodels>=0.14.0"
]

[project.optional-dependencies]
dev = ["pytest>=7.0", "black", "flake8"]

# ==== src/gemini_scenario_models/hpi_projection_us/dependencies/__init__.py ====
from .model import GLMModelScenarioProjection
from .data_preparation import *
from .data_projection import *
from .parameters import *

__all__ = ["GLMModelScenarioProjection"]

# ==== src/gemini_scenario_models/hpi_projection_us/dependencies/parameters.py ====
import polars as pl
from datetime import datetime
from dataclasses import dataclass
from typing import Optional

@dataclass
class Constants:
    LAST_HISTORY_DATE: str = "2025-03-31"
    FORECAST_START_DATE: str = "2025-06-30"
    MODEL_START_DATE: str = "2000-03-31"
    MODEL_END_DATE: str = "2023-06-30"

@dataclass
class Params:
    root_dir: str
    results_dir: str
    last_history_date: str = "2025-03-31"
    scenarios: list = None
    
    def __post_init__(self):
        if self.scenarios is None:
            self.scenarios = ['ce', 'up', 'dn', 'dn2']

class CLV4Combined(pl.DataFrame):
    pass

class NationalHPI(pl.DataFrame):
    pass

class StateMetroMap(pl.DataFrame):
    pass

class MoodysMapping(pl.DataFrame):
    pass

# ==== src/gemini_scenario_models/hpi_projection_us/dependencies/data_preparation.py ====
import polars as pl
from datetime import datetime
from typing import Tuple
from .parameters import CLV4Combined, MoodysMapping

def run_data_prep(
    params,
    clv4_state_extract: CLV4Combined,
    clv4_msa_extract: CLV4Combined,
    moodys_mapping: MoodysMapping
) -> Tuple[CLV4Combined, CLV4Combined, CLV4Combined]:
    state_data = prepare_state_data(clv4_state_extract, moodys_mapping)
    metro_data = prepare_metro_data(clv4_msa_extract, moodys_mapping)
    combined_data = combine_data(state_data, metro_data)
    return state_data, metro_data, combined_data

def prepare_state_data(
    clv4_state_extract: CLV4Combined,
    moodys_mapping: MoodysMapping
) -> CLV4Combined:
    state_with_fip = clv4_state_extract.join(
        moodys_mapping.select([
            pl.col("geography").str.to_uppercase().alias("state_name_upper"),
            pl.col("fip").alias("state_code")
        ]),
        left_on=pl.col("state_name").str.to_uppercase(),
        right_on="state_name_upper",
        how="left"
    )
    
    state_with_yyyymm = state_with_fip.with_columns([
        pl.when(pl.col("d") < 10)
        .then(pl.col("year").cast(pl.Utf8) + "0" + pl.col("d").cast(pl.Utf8))
        .otherwise(pl.col("year").cast(pl.Utf8) + pl.col("d").cast(pl.Utf8))
        .alias("yyyymm")
    ])
    
    return state_with_yyyymm.filter(pl.col("yyyymm").cast(pl.Int32) <= 202503)

def prepare_metro_data(
    clv4_msa_extract: CLV4Combined,
    moodys_mapping: MoodysMapping
) -> CLV4Combined:
    metro_with_cbsa = clv4_msa_extract.join(
        moodys_mapping.select([
            pl.col("geography").str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase().alias("cbsa_name_clean"),
            pl.col("fip").alias("cbsa_code")
        ]),
        left_on=pl.col("cbsa_name").str.replace_all(r"[^a-zA-Z0-9\s]", "").str.to_uppercase(),
        right_on="cbsa_name_clean",
        how="left"
    )
    
    metro_with_yyyymm = metro_with_cbsa.with_columns([
        pl.when(pl.col("d") < 10)
        .then(pl.col("year").cast(pl.Utf8) + "0" + pl.col("d").cast(pl.Utf8))
        .otherwise(pl.col("year").cast(pl.Utf8) + pl.col("d").cast(pl.Utf8))
        .alias("yyyymm")
    ])
    
    return metro_with_yyyymm.filter(pl.col("yyyymm").cast(pl.Int32) <= 202503)

def combine_data(state_data: CLV4Combined, metro_data: CLV4Combined) -> CLV4Combined:
    state_renamed = state_data.select([
        pl.col("state_code").alias("code"),
        pl.col("state_name").alias("name"),
        pl.col("yyyymm"),
        pl.col("home_price_index")
    ])
    
    metro_renamed = metro_data.select([
        pl.col("cbsa_code").alias("code"),
        pl.col("cbsa_name").alias("name"),
        pl.col("yyyymm"),
        pl.col("home_price_index")
    ])
    
    combined = pl.concat([state_renamed, metro_renamed])
    
    combined_with_date = combined.with_columns([
        pl.col("yyyymm").str.slice(0, 4).alias("year"),
        pl.col("yyyymm").str.slice(4, 2).alias("month")
    ]).with_columns([
        pl.date(pl.col("year").cast(pl.Int32), pl.col("month").cast(pl.Int32), 1).alias("date")
    ])
    
    combined_with_qtr = combined_with_date.with_columns([
        pl.when(pl.col("month").is_in(["01", "02", "03"]))
        .then(pl.col("year") + "Q1")
        .when(pl.col("month").is_in(["04", "05", "06"]))
        .then(pl.col("year") + "Q2")
        .when(pl.col("month").is_in(["07", "08", "09"]))
        .then(pl.col("year") + "Q3")
        .otherwise(pl.col("year") + "Q4")
        .alias("qtrdt")
    ])
    
    return combined_with_qtr.sort(["code", "date"]).unique(["code", "date"])

def create_seasonally_adjusted_hpi(combined_data: CLV4Combined) -> CLV4Combined:
    from statsmodels.tsa.x13 import x13_arima_analysis
    
    result_list = []
    for code in combined_data["code"].unique():
        code_data = combined_data.filter(pl.col("code") == code).sort("date")
        ts_data = code_data["home_price_index"].to_pandas()
        
        try:
            sa_result = x13_arima_analysis(ts_data)
            sa_values = sa_result.seasadj
            code_data_pd = code_data.to_pandas()
            code_data_pd["hpi_sa"] = sa_values
            result_list.append(pl.from_pandas(code_data_pd))
        except:
            code_data = code_data.with_columns(pl.col("home_price_index").alias("hpi_sa"))
            result_list.append(code_data)
    
    return pl.concat(result_list)

def calculate_quarterly_metrics(data: CLV4Combined) -> CLV4Combined:
    quarterly_data = data.group_by(["code", "name", "qtrdt"]).agg([
        pl.col("date").max().alias("date"),
        pl.col("home_price_index").mean().alias("hpi"),
        pl.col("hpi_sa").mean().alias("hpi_sa")
    ])
    
    quarterly_with_yoy = quarterly_data.sort(["code", "date"]).with_columns([
        (pl.col("hpi_sa") / pl.col("hpi_sa").shift(4, fill_value=None) - 1).alias("yoy_corelogicv4"),
        (pl.col("hpi_sa").log() - pl.col("hpi_sa").shift(1, fill_value=None).log()).alias("dlog_corelogicv4")
    ]).filter(~pl.col("name").str.contains("Micropolitan"))
    
    return quarterly_with_yoy

# ==== src/gemini_scenario_models/hpi_projection_us/dependencies/data_projection.py ====
import polars as pl
import numpy as np
from typing import Dict, List, Tuple
from sklearn.linear_model import LinearRegression
from datetime import datetime, timedelta
from .parameters import Constants

class DataProjector:
    def __init__(self, constants: Constants):
        self.constants = constants
        
    def create_national_forecast(self, national_data: pl.DataFrame, scenario_data: pl.DataFrame, scenario: str) -> pl.DataFrame:
        national_with_forecast = national_data.join(
            scenario_data.select([
                pl.col("f_date").alias("date1"),
                pl.col("corelogic_v4")
            ]),
            left_on="date",
            right_on="date1",
            how="full"
        )
        
        national_cleaned = national_with_forecast.with_columns([
            pl.when(pl.col("date").is_null())
            .then(pl.col("date1"))
            .otherwise(pl.col("date"))
            .alias("date"),
            
            pl.when(pl.col("hpi_sa").is_null())
            .then(pl.col("corelogic_v4"))
            .otherwise(pl.col("hpi_sa"))
            .alias("hpi_sa")
        ]).filter(pl.col("date") != pl.date(2025, 3, 31))
        
        national_with_changes = national_cleaned.sort("date").with_columns([
            (pl.col("hpi_sa") / pl.col("hpi_sa").shift(4, fill_value=None) - 1).alias("yoy_corelogicv4"),
            (pl.col("hpi_sa").log() - pl.col("hpi_sa").shift(1, fill_value=None).log()).alias("dlog_corelogicv4")
        ])
        
        return national_with_changes.filter(pl.col("date") >= pl.date(2000, 1, 1))
    
    def create_panel_data(self, region_data: pl.DataFrame, national_data: pl.DataFrame) -> pl.DataFrame:
        code_table = region_data.select(pl.col("name").unique().alias("code1")).with_columns(pl.lit(1).alias("ind"))
        date_table = national_data.select(pl.col("date").unique().alias("date1")).with_columns(pl.lit(1).alias("ind"))
        
        master = code_table.join(date_table, on="ind", how="outer").drop("ind")
        
        merged = master.join(
            region_data,
            left_on=["date1", "code1"],
            right_on=["date", "name"],
            how="left"
        )
        
        merged_with_us = merged.join(
            national_data.select([
                "date",
                pl.col("yoy_corelogicv4").alias("yoy_corelogicv4_us"),
                pl.col("dlog_corelogicv4").alias("dlog_corelogicv4_us")
            ]),
            left_on="date1",
            right_on="date",
            how="left"
        )
        
        return merged_with_us
    
    def run_glm_model(self, panel_data: pl.DataFrame) -> Tuple[pl.DataFrame, LinearRegression]:
        train_data = panel_data.filter(
            (pl.col("date1") >= pl.date(2000, 3, 31)) &
            (pl.col("date1") <= pl.date(2023, 6, 30))
        ).drop_nulls(["dlog_corelogicv4", "dlog_corelogicv4_us"])
        
        forecast_data = panel_data.filter(pl.col("date1") >= pl.date(2025, 6, 30))
        
        codes = train_data["code1"].unique().to_list()
        code_dummies = np.zeros((len(train_data), len(codes)))
        
        for i, code in enumerate(codes):
            code_dummies[:, i] = (train_data["code1"] == code).to_numpy().astype(int)
        
        X_train = np.column_stack([
            train_data["dlog_corelogicv4_us"].to_numpy().reshape(-1, 1) * code_dummies
        ]).reshape(len(train_data), -1)
        
        y_train = train_data["dlog_corelogicv4"].to_numpy()
        
        model = LinearRegression(fit_intercept=False)
        model.fit(X_train, y_train)
        
        forecast_codes = forecast_data["code1"].unique().to_list()
        forecast_dummies = np.zeros((len(forecast_data), len(codes)))
        
        for i, code in enumerate(codes):
            if code in forecast_codes:
                forecast_dummies[:, i] = (forecast_data["code1"] == code).to_numpy().astype(int)
        
        X_forecast = np.column_stack([
            forecast_data["dlog_corelogicv4_us"].to_numpy().reshape(-1, 1) * forecast_dummies
        ]).reshape(len(forecast_data), -1)
        
        predictions = model.predict(X_forecast)
        
        forecast_with_pred = forecast_data.with_columns(
            pl.Series("pred", predictions)
        )
        
        return forecast_with_pred, model

# ==== src/gemini_scenario_models/hpi_projection_us/dependencies/model.py ====
import polars as pl
from typing import Dict, List
from .data_preparation import *
from .data_projection import DataProjector
from .parameters import Constants, Params

class GLMModelScenarioProjection:
    def __init__(self, params: Params):
        self.params = params
        self.constants = Constants()
        self.projector = DataProjector(self.constants)
        
    def run_state_forecast(self, scenario: str, region_data: pl.DataFrame, national_data: pl.DataFrame, scenario_data: pl.DataFrame) -> Dict:
        national_forecast = self.projector.create_national_forecast(national_data, scenario_data, scenario)
        panel_data = self.projector.create_panel_data(region_data, national_forecast)
        forecast_results, model = self.projector.run_glm_model(panel_data)
        
        history_data = panel_data.filter(
            (pl.col("date1") >= pl.date(2000, 3, 31)) &
            (pl.col("date1") <= pl.date(2025, 3, 31))
        ).with_columns(pl.lit(None).alias("pred"))
        
        pred_table = pl.concat([history_data, forecast_results]).sort(["code1", "date1"])
        
        hpi_forecast = self._convert_to_hpi(pred_table, scenario)
        monthly_forecast = self._convert_to_monthly(hpi_forecast, scenario)
        transposed_data = self._transpose_data(monthly_forecast)
        
        return {
            "pred_table": pred_table,
            "hpi_forecast": hpi_forecast,
            "monthly_forecast": monthly_forecast,
            "transposed": transposed_data,
            "model": model
        }
    
    def run_metro_forecast(self, scenario: str, region_data: pl.DataFrame, national_data: pl.DataFrame, 
                          scenario_data: pl.DataFrame, state_forecast: pl.DataFrame, 
                          state_metro_map: pl.DataFrame) -> Dict:
        national_forecast = self.projector.create_national_forecast(national_data, scenario_data, scenario)
        panel_data = self.projector.create_panel_data(region_data, national_forecast)
        
        state_with_metro = state_forecast.join(
            state_metro_map,
            left_on="cbsa_name",
            right_on="geography",
            how="left"
        )
        
        panel_with_state = panel_data.join(
            state_with_metro.select(["date", "hpi", "cbsa_code"]),
            left_on=["date1", "code1"],
            right_on=["date", "cbsa_code"],
            how="left"
        ).with_columns([
            pl.col("hpi").alias("corelogicv4_st"),
            (pl.col("hpi").log() - pl.col("hpi").shift(1).log()).alias("dlog_corelogicv4_st")
        ])
        
        train_data = panel_with_state.filter(
            (pl.col("date1") >= pl.date(2000, 3, 31)) &
            (pl.col("date1") <= pl.date(2023, 6, 30))
        ).drop_nulls(["dlog_corelogicv4", "dlog_corelogicv4_st"])
        
        forecast_data = panel_with_state.filter(pl.col("date1") >= pl.date(2025, 6, 30))
        
        X_train = train_data.select(["dlog_corelogicv4_st"]).to_numpy()
        y_train = train_data["dlog_corelogicv4"].to_numpy()
        
        from sklearn.linear_model import LinearRegression
        model = LinearRegression(fit_intercept=False)
        model.fit(X_train, y_train)
        
        X_forecast = forecast_data.select(["dlog_corelogicv4_st"]).to_numpy()
        predictions = model.predict(X_forecast)
        
        forecast_results = forecast_data.with_columns(
            pl.Series("pred", predictions)
        )
        
        history_data = panel_with_state.filter(
            (pl.col("date1") >= pl.date(2000, 3, 31)) &
            (pl.col("date1") <= pl.date(2025, 3, 31))
        ).with_columns(pl.lit(None).alias("pred"))
        
        pred_table = pl.concat([history_data, forecast_results]).sort(["name1", "date1"])
        
        hpi_forecast = self._convert_to_hpi_metro(pred_table, scenario)
        monthly_forecast = self._convert_to_monthly_metro(hpi_forecast, scenario)
        transposed_data = self._transpose_data_metro(monthly_forecast)
        
        return {
            "pred_table": pred_table,
            "hpi_forecast": hpi_forecast,
            "monthly_forecast": monthly_forecast,
            "transposed": transposed_data,
            "model": model
        }
    
    def _convert_to_hpi(self, pred_table: pl.DataFrame, scenario: str) -> pl.DataFrame:
        sorted_data = pred_table.sort(["code1", "date1"])
        
        result_list = []
        for code in sorted_data["code1"].unique():
            code_data = sorted_data.filter(pl.col("code1") == code).sort("date1")
            
            hpi_pred = []
            last_hpi = None
            
            for row in code_data.iter_rows(named=True):
                if row["date1"] <= pl.date(2025, 3, 31):
                    hpi_val = row["hpi_sa"]
                    last_hpi = hpi_val
                else:
                    if last_hpi is not None and row["pred"] is not None:
                        hpi_val = last_hpi * np.exp(row["pred"])
                        last_hpi = hpi_val
                    else:
                        hpi_val = None
                
                hpi_pred.append(hpi_val)
            
            code_result = code_data.with_columns(
                pl.Series(f"hpipred_{scenario}", hpi_pred)
            )
            result_list.append(code_result)
        
        return pl.concat(result_list)
    
    def _convert_to_hpi_metro(self, pred_table: pl.DataFrame, scenario: str) -> pl.DataFrame:
        return self._convert_to_hpi(pred_table.rename({"name1": "code1"}), scenario)
    
    def _convert_to_monthly(self, data: pl.DataFrame, scenario: str) -> pl.DataFrame:
        return data.group_by("date1").agg([
            pl.col(f"hpipred_{scenario}").mean().alias("hpi")
        ])
    
    def _convert_to_monthly_metro(self, data: pl.DataFrame, scenario: str) -> pl.DataFrame:
        return self._convert_to_monthly(data, scenario)
    
    def _transpose_data(self, data: pl.DataFrame) -> pl.DataFrame:
        return data.pivot(index="date1", columns="code1", values="hpi")
    
    def _transpose_data_metro(self, data: pl.DataFrame) -> pl.DataFrame:
        return self._transpose_data(data)

# ==== Utils/data_import_helpers.py ====
import polars as pl
from pathlib import Path
from typing import Dict, Any

class DataImporter:
    def __init__(self, root_dir: str):
        self.root_dir = Path(root_dir)
        self.input_dir = self.root_dir / "Input"
    
    def import_excel(self, filename: str, sheet_name: str) -> pl.DataFrame:
        file_path = self.input_dir / filename
        return pl.read_excel(file_path, sheet_name=sheet_name)
    
    def import_all_data(self) -> Dict[str, pl.DataFrame]:
        data_files = {
            "moodys_mapping": ("Basket_2016-10-5_13_45_V2.xlsx", "Mapping"),
            "clv4_state": ("HPI Data by State.xlsx", "HPI Data by State"),
            "clv4_msa": ("HPI Data by CBSA.xlsx", "HPI Data by CBSA"),
            "state_metro_map": ("state_metro_map.xlsx", "Sheet1"),
            "scenario_ce": ("scenario_data.xlsx", "ce"),
            "scenario_up": ("scenario_data.xlsx", "up"),
            "scenario_dn": ("scenario_data.xlsx", "dn"),
            "scenario_dn2": ("scenario_data.xlsx", "dn2"),
        }
        
        imported_data = {}
        for key, (filename, sheet) in data_files.items():
            try:
                imported_data[key] = self.import_excel(filename, sheet)
            except Exception as e:
                print(f"Error importing {key}: {e}")
                imported_data[key] = pl.DataFrame()
        
        return imported_data

# ==== Utils/glm_model_architecture.py ====
import polars as pl
from sklearn.linear_model import LinearRegression
from typing import List, Dict, Optional, Tuple, Any
from abc import ABC, abstractmethod
from datetime import datetime, date
import numpy as np

class BaseGLMModel(ABC):
    def __init__(self, fit_intercept: bool = False):
        self.model = LinearRegression(fit_intercept=fit_intercept)
        self.is_fitted = False
        self.feature_columns = None
        self.target_column = None
    
    @abstractmethod
    def prepare_features(self, data: pl.DataFrame) -> np.ndarray:
        pass
    
    @abstractmethod
    def prepare_target(self, data: pl.DataFrame) -> np.ndarray:
        pass
    
    def fit(self, data: pl.DataFrame) -> 'BaseGLMModel':
        X = self.prepare_features(data)
        y = self.prepare_target(data)
        
        self.model.fit(X, y)
        self.is_fitted = True
        return self
    
    def predict(self, data: pl.DataFrame) -> np.ndarray:
        if not self.is_fitted:
            raise ValueError("Model must be fitted before prediction")
        
        X = self.prepare_features(data)
        return self.model.predict(X)
    
    def score(self, data: pl.DataFrame) -> float:
        if not self.is_fitted:
            raise ValueError("Model must be fitted before scoring")
        
        X = self.prepare_features(data)
        y = self.prepare_target(data)
        return self.model.score(X, y)

class HPIGLMModel(BaseGLMModel):
    def __init__(self, code_columns: List[str], interaction_column: str, fit_intercept: bool = False):
        super().__init__(fit_intercept)
        self.code_columns = code_columns
        self.interaction_column = interaction_column
    
    def prepare_features(self, data: pl.DataFrame) -> np.ndarray:
        codes = data[self.code_columns[0]].unique().to_list()
        interaction_values = data[self.interaction_column].to_numpy()
        
        features = []
        for code in codes:
            code_mask = (data[self.code_columns[0]] == code).to_numpy().astype(float)
            feature_col = interaction_values * code_mask
            features.append(feature_col)
        
        return np.column_stack(features)
    
    def prepare_target(self, data: pl.DataFrame) -> np.ndarray:
        return data[self.target_column].to_numpy()
    
    def set_target_column(self, target_col: str):
        self.target_column = target_col

# ==== Utils/create_state_metro_map.py ====
import polars as pl
from pathlib import Path

class StateMetroMapCreator:
    def __init__(self, input_dir: str):
        self.input_dir = Path(input_dir)
    
    def create_map(self) -> pl.DataFrame:
        try:
            return pl.read_excel(self.input_dir / "state_metro_map.xlsx")
        except Exception as e:
            print(f"Error reading state metro map: {e}")
            return pl.DataFrame({
                "cbsa_code": [],
                "cbsa_name": [],
                "st": []
            })
    
    def validate_map(self, df: pl.DataFrame) -> bool:
        required_columns = ["cbsa_code", "cbsa_name", "st"]
        return all(col in df.columns for col in required_columns)

# ==== main.py ====
import polars as pl
from pathlib import Path
from src.gemini_scenario_models.hpi_projection_us.dependencies import GLMModelScenarioProjection
from src.gemini_scenario_models.hpi_projection_us.dependencies.parameters import Params
from src.gemini_scenario_models.hpi_projection_us.dependencies.data_preparation import *
from Utils.data_import_helpers import DataImporter
from Utils.create_state_metro_map import StateMetroMapCreator

class HPIForecastRunner:
    def __init__(self, root_dir: str):
        self.root_dir = root_dir
        self.results_dir = Path(root_dir) / "Output"
        self.results_dir.mkdir(exist_ok=True)
        
        self.params = Params(
            root_dir=root_dir,
            results_dir=str(self.results_dir)
        )
        
        self.importer = DataImporter(root_dir)
        self.model = GLMModelScenarioProjection(self.params)
    
    def load_data(self):
        self.data = self.importer.import_all_data()
        
        state_metro_creator = StateMetroMapCreator(Path(self.root_dir) / "Input")
        self.data["state_metro_map"] = state_metro_creator.create_map()
        
        return self.data
    
    def prepare_data(self):
        state_data, metro_data, combined_data = run_data_prep(
            self.params,
            self.data["clv4_state"],
            self.data["clv4_msa"],
            self.data["moodys_mapping"]
        )
        
        combined_sa = create_seasonally_adjusted_hpi(combined_data)
        quarterly_data = calculate_quarterly_metrics(combined_sa)
        
        self.national_data = quarterly_data.filter(pl.col("name") == "National")
        self.state_data = quarterly_data.filter(
            (pl.col("code") > 0) & (pl.col("code") <= 100)
        )
        self.metro_data = quarterly_data.filter(
            (pl.col("code") >= 100) & (pl.col("code") < 100000)
        )
        
        return self.national_data, self.state_data, self.metro_data
    
    def run_all_forecasts(self):
        self.load_data()
        national_data, state_data, metro_data = self.prepare_data()
        
        results = {}
        
        for scenario in self.params.scenarios:
            scenario_data = self.data[f"scenario_{scenario}"]
            
            print(f"Running state forecast for scenario: {scenario}")
            state_results = self.model.run_state_forecast(
                scenario, state_data, national_data, scenario_data
            )
            
            print(f"Running metro forecast for scenario: {scenario}")
            metro_results = self.model.run_metro_forecast(
                scenario, metro_data, national_data, scenario_data,
                state_results["hpi_forecast"], self.data["state_metro_map"]
            )
            
            results[scenario] = {
                "state": state_results,
                "metro": metro_results
            }
            
            self.export_results(scenario, state_results, metro_results)
        
        return results
    
    def export_results(self, scenario: str, state_results: dict, metro_results: dict):
        state_file = self.results_dir / f"CoreLogic_State_CECL24_V3_{scenario}.xlsx"
        metro_file = self.results_dir / f"CoreLogic_Metro_CECL24_V3_{scenario}.xlsx"
        
        with pl.Config(fmt_str_lengths=1000):
            try:
                state_results["transposed"].write_excel(state_file, worksheet=scenario)
                metro_results["transposed"].write_excel(metro_file, worksheet=scenario)
                print(f"Exported results for scenario {scenario}")
            except Exception as e:
                print(f"Error exporting {scenario}: {e}")

if __name__ == "__main__":
    root_directory = "/path/to/your/project"
    
    runner = HPIForecastRunner(root_directory)
    results = runner.run_all_forecasts()
    
    print("All forecasts completed successfully!")

# ==== tests/hpi_projection_us/__init__.py ====

# ==== tests/hpi_projection_us/conftest.py ====
import pytest
import polars as pl
from pathlib import Path
import tempfile
import shutil

@pytest.fixture
def sample_data():
    return {
        "state_data": pl.DataFrame({
            "state_name": ["California", "Texas", "Florida"],
            "year": [2023, 2023, 2023],
            "d": [1, 1, 1],
            "home_price_index": [100.0, 90.0, 85.0]
        }),
        "metro_data": pl.DataFrame({
            "cbsa_name": ["Los Angeles-Long Beach-Anaheim, CA", "Dallas-Fort Worth-Arlington, TX"],
            "year": [2023, 2023],
            "d": [1, 1],
            "home_price_index": [105.0, 92.0]
        }),
        "moodys_mapping": pl.DataFrame({
            "geography": ["CALIFORNIA", "TEXAS", "FLORIDA"],
            "fip": [6, 48, 12]
        })
    }

@pytest.fixture
def temp_dir():
    temp_path = tempfile.mkdtemp()
    yield temp_path
    shutil.rmtree(temp_path)

# ==== tests/hpi_projection_us/test_container.py ====
import pytest
import polars as pl
from src.gemini_scenario_models.hpi_projection_us.dependencies import GLMModelScenarioProjection
from src.gemini_scenario_models.hpi_projection_us.dependencies.parameters import Params

class TestContainer:
    def test_glm_model_initialization(self, temp_dir):
        params = Params(root_dir=temp_dir, results_dir=temp_dir)
        model = GLMModelScenarioProjection(params)
        assert model.params == params
        assert model.constants is not None
        assert model.projector is not None

# ==== tests/hpi_projection_us/test_data_preparation.py ====
import pytest
import polars as pl
from src.gemini_scenario_models.hpi_projection_us.dependencies.data_preparation import *

class TestDataPreparation:
    def test_prepare_state_data(self, sample_data):
        result = prepare_state_data(
            sample_data["state_data"],
            sample_data["moodys_mapping"]
        )
        
        assert "state_code" in result.columns
        assert "yyyymm" in result.columns
        assert len(result) <= len(sample_data["state_data"])
    
    def test_prepare_metro_data(self, sample_data):
        result = prepare_metro_data(
            sample_data["metro_data"],
            sample_data["moodys_mapping"]
        )
        
        assert "yyyymm" in result.columns
        assert len(result) <= len(sample_data["metro_data"])
    
    def test_combine_data(self, sample_data):
        state_prepared = prepare_state_data(
            sample_data["state_data"],
            sample_data["moodys_mapping"]
        )
        metro_prepared = prepare_metro_data(
            sample_data["metro_data"],
            sample_data["moodys_mapping"]
        )
        
        result = combine_data(state_prepared, metro_prepared)
        
        assert "code" in result.columns
        assert "name" in result.columns
        assert "date" in result.columns
        assert "qtrdt" in result.columns

# ==== tests/hpi_projection_us/test_data_projection.py ====
import pytest
import polars as pl
import numpy as np
from src.gemini_scenario_models.hpi_projection_us.dependencies.data_projection import DataProjector
from src.gemini_scenario_models.hpi_projection_us.dependencies.parameters import Constants

class TestDataProjection:
    def test_data_projector_initialization(self):
        constants = Constants()
        projector = DataProjector(constants)
        assert projector.constants == constants
    
    def test_create_national_forecast(self):
        constants = Constants()
        projector = DataProjector(constants)
        
        national_data = pl.DataFrame({
            "date": [pl.date(2023, 3, 31), pl.date(2023, 6, 30)],
            "hpi_sa": [100.0, 102.0],
            "name": ["National", "National"]
        })
        
        scenario_data = pl.DataFrame({
            "f_date": [pl.date(2023, 9, 30), pl.date(2023, 12, 31)],
            "corelogic_v4": [104.0, 106.0]
        })
        
        result = projector.create_national_forecast(national_data, scenario_data, "ce")
        
        assert "yoy_corelogicv4" in result.columns
        assert "dlog_corelogicv4" in result.columns
        assert len(result) >= len(national_data)

# ==== tests/hpi_projection_us/test_model.py ====
import pytest
import polars as pl
from src.gemini_scenario_models.hpi_projection_us.dependencies.model import GLMModelScenarioProjection
from src.gemini_scenario_models.hpi_projection_us.dependencies.parameters import Params, Constants

class TestModel:
    def test_convert_to_hpi(self, temp_dir):
        params = Params(root_dir=temp_dir, results_dir=temp_dir)
        model = GLMModelScenarioProjection(params)
        
        pred_table = pl.DataFrame({
            "code1": ["CA", "TX", "CA", "TX"],
            "date1": [pl.date(2025, 3, 31), pl.date(2025, 3, 31), 
                     pl.date(2025, 6, 30), pl.date(2025, 6, 30)],
            "hpi_sa": [100.0, 90.0, None, None],
            "pred": [None, None, 0.01, 0.02]
        })
        
        result = model._convert_to_hpi(pred_table, "ce")
        
        assert "hpipred_ce" in result.columns
        assert len(result) == len(pred_table)

# ==== tests/hpi_projection_us/test_parameters.py ====
import pytest
from src.gemini_scenario_models.hpi_projection_us.dependencies.parameters import Constants, Params

class TestParameters:
    def test_constants_initialization(self):
        constants = Constants()
        assert constants.LAST_HISTORY_DATE == "2025-03-31"
        assert constants.FORECAST_START_DATE == "2025-06-30"
        assert constants.MODEL_START_DATE == "2000-03-31"
        assert constants.MODEL_END_DATE == "2023-06-30"
    
    def test_params_initialization(self, temp_dir):
        params = Params(root_dir=temp_dir, results_dir=temp_dir)
        assert params.root_dir == temp_dir
        assert params.results_dir == temp_dir
        assert params.scenarios == ['ce', 'up', 'dn', 'dn2']
    
    def test_params_custom_scenarios(self, temp_dir):
        custom_scenarios = ['scenario1', 'scenario2']
        params = Params(root_dir=temp_dir, results_dir=temp_dir, scenarios=custom_scenarios)
        assert params.scenarios == custom_scenarios